commit 31c949580cdfd395af3625e6472cf2b042163495
Author: Lucas Newman <lucas@future.fit>
Date:   Tue Feb 24 09:50:59 2026 -0800

    Add a CLI option to emit timestamps for speech <-> text alignment. (#74)

diff --git a/Sources/Tools/mlx-audio-swift-tts/App.swift b/Sources/Tools/mlx-audio-swift-tts/App.swift
index 6dafd02..b80a182 100644
--- a/Sources/Tools/mlx-audio-swift-tts/App.swift
+++ b/Sources/Tools/mlx-audio-swift-tts/App.swift
@@ -2,6 +2,7 @@ import AVFoundation
 import Foundation
 @preconcurrency import MLX
 import MLXAudioCore
+import MLXAudioSTT
 import MLXAudioTTS
 import MLXLMCommon
 
@@ -31,6 +32,8 @@ enum AppError: Error, LocalizedError, CustomStringConvertible {
 
 @main
 enum App {
+    private static let forcedAlignerRepo = "mlx-community/Qwen3-ForcedAligner-0.6B-4bit"
+
     static func main() async {
         do {
             let args = try CLI.parse()
@@ -43,7 +46,8 @@ enum App {
                 refText: args.refText,
                 maxTokens: args.maxTokens,
                 temperature: args.temperature,
-                topP: args.topP
+                topP: args.topP,
+                timestamps: args.timestamps
             )
         } catch {
             fputs("Error: \(error)\n", stderr)
@@ -62,6 +66,7 @@ enum App {
         maxTokens: Int?,
         temperature: Float?,
         topP: Float?,
+        timestamps: Bool,
         hfToken: String? = nil
     ) async throws {
         Memory.cacheLimit = 100 * 1024 * 1024
@@ -119,6 +124,28 @@ enum App {
         try writeWavFile(samples: audioData, sampleRate: sampleRate, outputURL: outputURL)
         print("Wrote WAV to \(outputURL.path)")
 
+        if timestamps {
+            print("Loading forced aligner (\(forcedAlignerRepo))")
+            let forcedAligner = try await Qwen3ForcedAlignerModel.fromPretrained(forcedAlignerRepo)
+            let alignmentAudio = try prepareAudioForForcedAlignment(
+                samples: audioData,
+                sampleRate: Int(loadedModel.sampleRate)
+            )
+            let aligned = forcedAligner.generate(audio: alignmentAudio, text: text, language: "English")
+
+            print("Timestamps:")
+            for item in aligned.items {
+                print(
+                    String(
+                        format: "  [%.3fs - %.3fs] %@",
+                        item.startTime,
+                        item.endTime,
+                        item.text
+                    )
+                )
+            }
+        }
+
         print(String(format: "Finished generation in %0.2fs", CFAbsoluteTimeGetCurrent() - started))
         print("Memory usage:\n\(Memory.snapshot())")
 
@@ -159,6 +186,14 @@ enum App {
         let audioFile = try AVAudioFile(forWriting: outputURL, settings: format.settings)
         try audioFile.write(from: buffer)
     }
+
+    private static func prepareAudioForForcedAlignment(samples: [Float], sampleRate: Int) throws -> MLXArray {
+        guard sampleRate != 16000 else {
+            return MLXArray(samples)
+        }
+        let resampled = try resampleAudio(samples, from: sampleRate, to: 16000)
+        return MLXArray(resampled)
+    }
 }
 
 // MARK: -
@@ -187,6 +222,7 @@ struct CLI {
     let maxTokens: Int?
     let temperature: Float?
     let topP: Float?
+    let timestamps: Bool
 
     static func parse() throws -> CLI {
         var text: String?
@@ -198,6 +234,7 @@ struct CLI {
         var maxTokens: Int? = nil
         var temperature: Float? = nil
         var topP: Float? = nil
+        var timestamps = false
 
         var it = CommandLine.arguments.dropFirst().makeIterator()
         while let arg = it.next() {
@@ -232,6 +269,8 @@ struct CLI {
                 guard let v = it.next() else { throw CLIError.missingValue(arg) }
                 guard let value = Float(v) else { throw CLIError.invalidValue(arg, v) }
                 topP = value
+            case "--timestamps":
+                timestamps = true
             case "--help", "-h":
                 printUsage()
                 exit(0)
@@ -257,7 +296,8 @@ struct CLI {
             refText: refText,
             maxTokens: maxTokens,
             temperature: temperature,
-            topP: topP
+            topP: topP,
+            timestamps: timestamps
         )
     }
 
@@ -265,7 +305,7 @@ struct CLI {
         let exe = (CommandLine.arguments.first as NSString?)?.lastPathComponent ?? "marvis-tts-cli"
         print("""
         Usage:
-          \(exe) --text "Hello world" [--voice conversational_b] [--model <hf-repo>] [--output <path>] [--ref_audio <path>] [--ref_text <string>] [--max_tokens <int>] [--temperature <float>] [--top_p <float>]
+          \(exe) --text "Hello world" [--voice conversational_b] [--model <hf-repo>] [--output <path>] [--ref_audio <path>] [--ref_text <string>] [--max_tokens <int>] [--temperature <float>] [--top_p <float>] [--timestamps]
 
         Options:
           -t, --text <string>           Text to synthesize (required if not passed as trailing arg)
@@ -277,6 +317,7 @@ struct CLI {
               --max_tokens <int>       Maximum number of tokens to generate (overrides model default)
               --temperature <float>    Sampling temperature (overrides model default)
               --top_p <float>          Top-p sampling (overrides model default)
+              --timestamps             Emit word timestamps using mlx-community/Qwen3-ForcedAligner-0.6B-4bit
           -h, --help                    Show this help
         """)
     }
diff --git a/Sources/Tools/mlx-audio-swift-tts/README.md b/Sources/Tools/mlx-audio-swift-tts/README.md
index 78fabc0..e8327c9 100644
--- a/Sources/Tools/mlx-audio-swift-tts/README.md
+++ b/Sources/Tools/mlx-audio-swift-tts/README.md
@@ -15,6 +15,7 @@ swift run mlx-audio-swift-tts \
   --model mlx-community/VyvoTTS-EN-Beta-4bit \
   --text "Hello from MLX Audio" \
   --voice en-us-1 \
+  --timestamps \
   --output /tmp/tts.wav
 ```
 
@@ -40,4 +41,5 @@ swift run mlx-audio-swift-tts \
 - `--max_tokens`: Override generation max tokens
 - `--temperature`: Override sampling temperature
 - `--top_p`: Override top-p
+- `--timestamps`: Emit word timestamps using `mlx-community/Qwen3-ForcedAligner-0.6B-4bit` (default: false)
 - `--help`, `-h`: Show help

commit 1524d12ee4ebf2fd1b33efd1218608f64c0b8929
Author: Prince Canuma <prince.gdt@gmail.com>
Date:   Tue Feb 24 01:10:16 2026 +0100

    Update GitHub folder

diff --git a/.github/FUNDING.yml b/.github/FUNDING.yml
new file mode 100644
index 0000000..b3db39c
--- /dev/null
+++ b/.github/FUNDING.yml
@@ -0,0 +1,15 @@
+# These are supported funding model platforms
+
+github: Blaizzy # Replace with up to 4 GitHub Sponsors-enabled usernames e.g., [user1, user2]
+patreon: # Replace with a single Patreon username
+open_collective: # Replace with a single Open Collective username
+ko_fi: # Replace with a single Ko-fi username
+tidelift: # Replace with a single Tidelift platform-name/package-name e.g., npm/babel
+community_bridge: # Replace with a single Community Bridge project-name e.g., cloud-foundry
+liberapay: # Replace with a single Liberapay username
+issuehunt: # Replace with a single IssueHunt username
+lfx_crowdfunding: # Replace with a single LFX Crowdfunding project-name e.g., cloud-foundry
+polar: # Replace with a single Polar username
+buy_me_a_coffee: # Replace with a single Buy Me a Coffee username
+thanks_dev: # Replace with a single thanks.dev username
+custom: # Replace with up to 4 custom sponsorship URLs e.g., ['link1', 'link2']

commit bc38d3714b3668939a73bc6422068aa7d3b28f7a
Author: Prince Canuma <prince.gdt@gmail.com>
Date:   Mon Feb 23 20:33:22 2026 +0100

    Update README.md to reflect new audio models and enhancements (#72)

diff --git a/README.md b/README.md
index 63e484c..0ebe9d3 100644
--- a/README.md
+++ b/README.md
@@ -11,11 +11,11 @@ A modular Swift SDK for audio processing with MLX on Apple Silicon
 MLXAudio follows a modular design allowing you to import only what you need:
 
 - **MLXAudioCore**: Base types, protocols, and utilities
-- **MLXAudioCodecs**: Audio codec implementations (SNAC, Vocos, Mimi)
-- **MLXAudioTTS**: Text-to-Speech models (Soprano, VyvoTTS, Orpheus, Marvis TTS, Pocket TTS)
-- **MLXAudioSTT**: Speech-to-Text models (GLMASR)
-- **MLXAudioVAD**: Voice Activity Detection & Speaker Diarization (Sortformer)
-- **MLXAudioSTS**: Speech-to-Speech models (LFM2.5-Audio)
+- **MLXAudioCodecs**: Audio codec implementations (SNAC, Encodec, Vocos, Mimi, DACVAE)
+- **MLXAudioTTS**: Text-to-Speech models (Qwen3-TTS, Soprano, VyvoTTS, Orpheus, Marvis TTS, Pocket TTS)
+- **MLXAudioSTT**: Speech-to-Text models (Qwen3-ASR, Voxtral Realtime, Parakeet, GLMASR)
+- **MLXAudioVAD**: Voice Activity Detection & Speaker Diarization (Sortformer, SmartTurn)
+- **MLXAudioSTS**: Speech-to-Speech models (LFM2.5-Audio, SAM-Audio, MossFormer2-SE)
 - **MLXAudioUI**: SwiftUI components for audio interfaces
 
 ## Installation
@@ -116,6 +116,7 @@ for try await event in model.generateStream(text: text, parameters: parameters)
 
 | Model | Model README | HuggingFace Repo |
 |-------|--------------|------------------|
+| Qwen3-TTS | — | [mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit](https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit) |
 | Soprano | [Soprano README](Sources/MLXAudioTTS/Models/Soprano/README.md) | [mlx-community/Soprano-80M-bf16](https://huggingface.co/mlx-community/Soprano-80M-bf16) |
 | VyvoTTS | [VyvoTTS README](Sources/MLXAudioTTS/Models/Qwen3/README.md) | [mlx-community/VyvoTTS-EN-Beta-4bit](https://huggingface.co/mlx-community/VyvoTTS-EN-Beta-4bit) |
 | Orpheus | [Orpheus README](Sources/MLXAudioTTS/Models/Llama/README.md) | [mlx-community/orpheus-3b-0.1-ft-bf16](https://huggingface.co/mlx-community/orpheus-3b-0.1-ft-bf16) |
@@ -126,6 +127,10 @@ for try await event in model.generateStream(text: text, parameters: parameters)
 
 | Model | Model README | HuggingFace Repo |
 |-------|--------------|------------------|
+| Qwen3-ASR | [Qwen3-ASR README](Sources/MLXAudioSTT/Models/Qwen3ASR/README.md) | [mlx-community/Qwen3-ASR-1.7B-bf16](https://huggingface.co/mlx-community/Qwen3-ASR-1.7B-bf16) |
+| Qwen3-ForcedAligner | [Qwen3-ASR README](Sources/MLXAudioSTT/Models/Qwen3ASR/README.md) | [mlx-community/Qwen3-ForcedAligner-0.6B-bf16](https://huggingface.co/mlx-community/Qwen3-ForcedAligner-0.6B-bf16) |
+| Voxtral Realtime | [Voxtral README](Sources/MLXAudioSTT/Models/VoxtralRealtime/README.md) | [mlx-community/Voxtral-Mini-4B-Realtime-2602-fp16](https://huggingface.co/mlx-community/Voxtral-Mini-4B-Realtime-2602-fp16) |
+| Parakeet | [Parakeet README](Sources/MLXAudioSTT/Models/Parakeet/README.md) | [mlx-community/parakeet-tdt-0.6b-v3](https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v3) |
 | GLMASR | [GLMASR README](Sources/MLXAudioSTT/Models/GLMASR/README.md) | [mlx-community/GLM-ASR-Nano-2512-4bit](https://huggingface.co/mlx-community/GLM-ASR-Nano-2512-4bit) |
 
 ### STS Models
@@ -133,12 +138,15 @@ for try await event in model.generateStream(text: text, parameters: parameters)
 | Model | Model README | HuggingFace Repo |
 |-------|--------------|------------------|
 | LFM2.5-Audio | [LFM Audio README](Sources/MLXAudioSTS/Models/LFMAudio/README.md) | [mlx-community/LFM2.5-Audio-1.5B-6bit](https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-6bit) |
+| SAM-Audio | [SAM Audio README](Sources/MLXAudioSTS/Models/SAMAudio/README.md) | [mlx-community/sam-audio-large-fp16](https://huggingface.co/mlx-community/sam-audio-large-fp16) |
+| MossFormer2-SE | — | [starkdmi/MossFormer2-SE-fp16](https://huggingface.co/starkdmi/MossFormer2-SE-fp16) |
 
 ### VAD / Speaker Diarization Models
 
 | Model | Model README | HuggingFace Repo |
 |-------|--------------|------------------|
 | Sortformer | [Sortformer README](Sources/MLXAudioVAD/Models/Sortformer/README.md) | [mlx-community/diar_streaming_sortformer_4spk-v2.1-fp16](https://huggingface.co/mlx-community/diar_streaming_sortformer_4spk-v2.1-fp16) |
+| SmartTurn | [SmartTurn README](Sources/MLXAudioVAD/Models/SmartTurn/README.md) | [mlx-community/smart-turn-v3](https://huggingface.co/mlx-community/smart-turn-v3) |
 
 ## Features
 

commit 712b27dc159653b45eec52dee0ca5f17e37f5c67
Author: Lucas Newman <lucas@future.fit>
Date:   Mon Feb 23 11:11:12 2026 -0800

    Use an OSS model instead of the on-device foundation model, which has region restrictions. (#71)

diff --git a/Examples/SimpleChat/SimpleChat/ConversationController.swift b/Examples/SimpleChat/SimpleChat/ConversationController.swift
index 3274305..ea8d64d 100644
--- a/Examples/SimpleChat/SimpleChat/ConversationController.swift
+++ b/Examples/SimpleChat/SimpleChat/ConversationController.swift
@@ -1,9 +1,8 @@
 import AVFoundation
-import FoundationModels
 import MLX
 import MLXAudioCore
 import MLXAudioTTS
-import MLXLMCommon
+@preconcurrency import MLXLMCommon
 
 @MainActor
 protocol ConversationControllerDelegate: AnyObject {
@@ -83,6 +82,11 @@ final class ConversationController {
     - A complete request with missing details is still COMPLETE. Ask clarifying questions in your ✓ response.
       Example: "Can you tell me what the weather is today?" -> `✓ Sure. What city are you in?`
 
+    ANTI-ECHO RULE (CRITICAL):
+    - If you choose COMPLETE (✓), you must answer the user's intent.
+    - Do NOT repeat, restate, or lightly paraphrase the user transcript as your full response.
+    - Never output `✓` followed by the same question the user just asked.
+
     RESPOND in one of these three formats:
     1. If COMPLETE: `✓` followed by a space and your full substantive response
     2. If INCOMPLETE SHORT: ONLY the character `○` (user will continue in a few seconds)
@@ -90,7 +94,7 @@ final class ConversationController {
 
     FORMAT REQUIREMENTS:
     - ALWAYS use single-character indicators: `✓` (complete), `○` (short wait), or `◐` (long wait)
-    - For COMPLETE: `✓` followed by a space and your full response
+    - For COMPLETE: `✓` followed by a space and your full response to the user's intent
     - For INCOMPLETE: ONLY the single character (`○` or `◐`) with absolutely nothing else
     - Your turn indicator must be the very first character in your response
     """
@@ -98,7 +102,7 @@ final class ConversationController {
     private nonisolated static let defaultIncompleteShortPrompt = """
     The user paused briefly. Generate a brief, natural prompt to encourage them to continue.
 
-    IMPORTANT: You MUST respond with ✓ followed by your message. Do NOT output ○ or ◐ - the user has already been given time to continue.
+    IMPORTANT: You MUST respond with ✓ followed by your full response. Do NOT output ○ or ◐ - the user has already been given time to continue.
 
     Your response should:
     - Be contextually relevant to what was just discussed
@@ -110,7 +114,7 @@ final class ConversationController {
     private nonisolated static let defaultIncompleteLongPrompt = """
     The user has been quiet for a while. Generate a friendly check-in message.
 
-    IMPORTANT: You MUST respond with ✓ followed by your message. Do NOT output ○ or ◐ - the user has already been given plenty of time.
+    IMPORTANT: You MUST respond with ✓ followed by your full response. Do NOT output ○ or ◐ - the user has already been given plenty of time.
 
     Your response should:
     - Acknowledge they might be thinking or busy
@@ -142,7 +146,7 @@ final class ConversationController {
     @ObservationIgnored
     private var captureTask: Task<Void, Never>?
     @ObservationIgnored
-    private var languageSession: LanguageModelSession?
+    private var languageSession: ChatSession?
     @ObservationIgnored
     private var incompleteTimeoutTask: Task<Void, Never>?
     @ObservationIgnored
@@ -180,7 +184,7 @@ final class ConversationController {
         try session.setActive(true)
 #endif
 
-        resetLanguageSession()
+        try await resetLanguageSession()
         try await ensureEngineStarted()
         startCaptureLoopIfNeeded()
         isActive = true
@@ -239,9 +243,10 @@ final class ConversationController {
         audioEngine.speak(buffersStream: audioStream)
     }
 
-    private func resetLanguageSession() {
+    private func resetLanguageSession() async throws {
+        let model = try await loadModelContainer(id: "mlx-community/Qwen3-1.7B-4bit")
         let instructions = Self.baseInstructions + "\n\n" + turnCompletionConfig.completionInstructions
-        languageSession = LanguageModelSession(instructions: instructions)
+        languageSession = ChatSession(model, instructions: instructions, generateParameters: GenerateParameters(maxTokens: 4096, temperature: 0.6, topP: 0.95))
     }
 
     private func ensureEngineStarted() async throws {
@@ -372,9 +377,13 @@ final class ConversationController {
         do {
             print("Using LLM prompt:\n\(prompt)")
             let response = try await session.respond(to: prompt)
-            print("LLM turn response [\(source)]: \(response.content)")
+            print("LLM turn response [\(source)]: \(response)")
+
+            let text = stripThinkContent(from: response)
+            print("Cleaned LLM turn response [\(source)]: \(text)")
+
             try await handleTurnResponse(
-                response.content,
+                text,
                 source: source,
                 originalTranscript: originalTranscript
             )
@@ -432,7 +441,7 @@ final class ConversationController {
         User transcript:
         \(transcript)
 
-        IMPORTANT: Your response MUST start with ✓ followed by your response text.
+        IMPORTANT: Your response MUST start with ✓ followed by your full response.
         Do NOT output ○ or ◐.
         """
         await requestTurnAwareResponse(
@@ -452,6 +461,15 @@ final class ConversationController {
         return false
     }
 
+    private func stripThinkContent(from text: String) -> String {
+        let withoutThinkBlocks = text.replacingOccurrences(
+            of: #"(?is)<think\b[^>]*>.*?</think>"#,
+            with: "",
+            options: .regularExpression
+        )
+        return withoutThinkBlocks.trimmingCharacters(in: .whitespacesAndNewlines)
+    }
+
     private func parseTurnResponse(_ text: String) -> (TurnMarker, String)? {
         let matches: [(marker: TurnMarker, index: String.Index)] = [
             (.complete, text.firstIndex(of: "✓")),
@@ -480,15 +498,25 @@ final class ConversationController {
     private func handleCompletedUserTranscript(_ transcription: String) {
         guard !isSpeaking, transcription.count > 1 else { return }
         let prompt = """
-        Determine whether the user has completed their turn, then respond in the required marker format.
+        Determine whether the user has completed their turn, then produce the final response in marker format.
 
         User transcript:
         \(transcription)
 
-        IMPORTANT:
-        - Your response MUST begin with exactly one turn marker: ✓, ○, or ◐.
-        - If COMPLETE, respond with ✓ followed by a space and your response text.
-        - If INCOMPLETE, respond with ONLY ○ or ONLY ◐ and no additional text.
+        Follow these steps:
+        1. Decide turn status:
+           - COMPLETE: user finished speaking and you can respond now.
+           - INCOMPLETE SHORT: user was cut off and will continue in a few seconds.
+           - INCOMPLETE LONG: user is still thinking and needs more time.
+        2. Format output exactly:
+           - If COMPLETE: start with `✓ `, then give a direct helpful answer to the user's intent.
+           - If INCOMPLETE SHORT: output only `○`.
+           - If INCOMPLETE LONG: output only `◐`.
+
+        Critical constraints:
+        - Do not repeat the user's transcript as your answer.
+        - Do not output explanations of your reasoning.
+        - Output exactly one response in one of the formats above.
         """
         startLLMTurnTask(
             prompt: prompt,
diff --git a/Examples/SimpleChat/SimpleChat/SemanticVAD.swift b/Examples/SimpleChat/SimpleChat/SemanticVAD.swift
index 8f63842..178f059 100644
--- a/Examples/SimpleChat/SimpleChat/SemanticVAD.swift
+++ b/Examples/SimpleChat/SimpleChat/SemanticVAD.swift
@@ -40,7 +40,7 @@ actor SemanticVAD {
     private let transcriptState = TranscriptState()
 
     init(
-        hangTime: TimeInterval = 0.8,
+        hangTime: TimeInterval = 0.9,
         detectionRMSThreshold: Float = 0.01,
         smartTurnRepoID: String = "mlx-community/smart-turn-v3",
         smartTurnThreshold: Float? = nil

commit 9b2b522256f136681332a6b93f8eccfb0ef237cc
Author: Prince Canuma <prince.gdt@gmail.com>
Date:   Mon Feb 23 19:08:09 2026 +0100

    Add streaming audio chunk support in Qwen3TTS model (#70)
    
    * Add streaming audio chunk support in Qwen3TTS model
    
    - Implemented a new method to decode audio chunks during speech generation.
    - Updated `generateVoiceDesign` to yield audio chunks in a streaming manner.
    - Added a helper function `decodeChunk` for processing codec codes into audio waveform.
    - Enhanced audio generation flow to handle streaming efficiently, improving responsiveness and memory usage.
    
    * Refactor TTSViewModel for improved streaming audio chunk handling
    
    - Simplified streaming playback logic to allow audio chunks to be processed progressively.
    - Removed unnecessary variable for audio data and streamlined chunk processing.
    - Updated SettingsView to maintain UI consistency for streaming audio toggle based on chunking settings.

diff --git a/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift b/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
index be0605b..010427a 100644
--- a/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
+++ b/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
@@ -302,8 +302,8 @@ class TTSViewModel {
 
             var totalTokenCount = 0
 
-            // Start streaming playback if enabled and we have multiple chunks
-            let useStreaming = streamingPlayback && chunks.count > 1
+            // Streaming playback — model yields audio chunks progressively
+            let useStreaming = streamingPlayback
             if useStreaming {
                 audioPlayer.startStreaming(sampleRate: sampleRate)
             }
@@ -317,7 +317,6 @@ class TTSViewModel {
                 }
 
                 var chunkTokenCount = 0
-                var audio: MLXArray?
 
                 // Set cache limit for this chunk
                 Memory.cacheLimit = 512 * 1024 * 1024 // 512MB cache limit
@@ -329,6 +328,7 @@ class TTSViewModel {
                     : voice?.name
 
                 // Each chunk needs a fresh generation
+                // Audio chunks arrive progressively during streaming
                 for try await event in model.generateStream(
                     text: chunk,
                     voice: voiceParam,
@@ -354,25 +354,17 @@ class TTSViewModel {
                     case .info(let info):
                         tokensPerSecond = info.tokensPerSecond
                     case .audio(let audioData):
-                        audio = audioData
-                    }
-                }
+                        autoreleasepool {
+                            let samples = audioData.asArray(Float.self)
 
-                // Convert to CPU samples and write directly to file
-                if let audioData = audio {
-                    autoreleasepool {
-                        let samples = audioData.asArray(Float.self)
+                            if useStreaming {
+                                audioPlayer.scheduleAudioChunk(samples, withCrossfade: true)
+                            }
 
-                        // Stream playback immediately as chunks are ready
-                        if useStreaming {
-                            audioPlayer.scheduleAudioChunk(samples, withCrossfade: true)
+                            try? wavWriter.writeChunk(samples)
                         }
-
-                        // Write directly to file - no memory accumulation
-                        try? wavWriter.writeChunk(samples)
                     }
                 }
-                audio = nil
 
                 // Clear GPU cache after each chunk
                 Memory.clearCache()
diff --git a/Examples/VoicesApp/VoicesApp/Views/SettingsView.swift b/Examples/VoicesApp/VoicesApp/Views/SettingsView.swift
index 29177ae..3500355 100644
--- a/Examples/VoicesApp/VoicesApp/Views/SettingsView.swift
+++ b/Examples/VoicesApp/VoicesApp/Views/SettingsView.swift
@@ -376,14 +376,14 @@ struct SettingsView: View {
                     .padding(.top, 4)
                 #endif
 
-                if viewModel.enableChunking {
-                    #if os(iOS)
-                    CompactToggle(label: "Stream audio", isOn: $viewModel.streamingPlayback, font: textFont)
-                    #else
-                    Toggle("Stream audio", isOn: $viewModel.streamingPlayback)
-                        .font(textFont)
-                    #endif
+                #if os(iOS)
+                CompactToggle(label: "Stream audio", isOn: $viewModel.streamingPlayback, font: textFont)
+                #else
+                Toggle("Stream audio", isOn: $viewModel.streamingPlayback)
+                    .font(textFont)
+                #endif
 
+                if viewModel.enableChunking {
                     HStack {
                         Text("Max chunk length")
                             .font(textFont)
diff --git a/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift b/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
index 7ef7f96..3992c26 100644
--- a/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
+++ b/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
@@ -101,7 +101,7 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
                 let repPenalty = generationParameters.repetitionPenalty ?? 1.05
                 let maxTokens = generationParameters.maxTokens ?? 4096
 
-                let audio = generateVoiceDesign(
+                _ = generateVoiceDesign(
                     text: text,
                     instruct: instruct,
                     language: lang,
@@ -118,9 +118,11 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
                     },
                     onInfo: { info in
                         continuation.yield(.info(info))
+                    },
+                    onAudioChunk: { chunk in
+                        continuation.yield(.audio(chunk))
                     }
                 )
-                continuation.yield(.audio(audio))
                 continuation.finish()
             } catch {
                 continuation.finish(throwing: error)
@@ -129,6 +131,32 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
         return stream
     }
 
+    // MARK: - Decode chunk helper
+
+    /// Decode a chunk of codec codes to audio waveform.
+    /// - Parameters:
+    ///   - codes: Codec codes [1, time, numCodeGroups]
+    ///   - chunkTokens: Tokens per decode chunk (controls decode granularity)
+    /// - Returns: Decoded audio waveform (1D)
+    private func decodeChunk(_ codes: MLXArray, chunkTokens: Int = 100) -> MLXArray {
+        guard let speechTokenizer else { return MLXArray.zeros([1]) }
+
+        var audioChunks = [MLXArray]()
+        for chunk in speechTokenizer.streamingDecode(codes, chunkTokens: chunkTokens) {
+            audioChunks.append(chunk)
+        }
+        var audio = concatenated(audioChunks, axis: -1)[0]
+
+        let validLen = Int((codes[0..., 0..., 0] .> 0).sum().item(Int32.self))
+            * speechTokenizer.decodeUpsampleRate
+        if validLen > 0, validLen < audio.dim(0) {
+            audio = audio[..<validLen]
+        }
+
+        eval(audio)
+        return audio
+    }
+
     // MARK: - VoiceDesign generation
 
     func generateVoiceDesign(
@@ -144,7 +172,8 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
         minP: Float,
         maxTokens: Int,
         onToken: ((Int) -> Void)? = nil,
-        onInfo: ((AudioGenerationInfo) -> Void)? = nil
+        onInfo: ((AudioGenerationInfo) -> Void)? = nil,
+        onAudioChunk: ((MLXArray) -> Void)? = nil
     ) -> MLXArray {
         guard let speechTokenizer, let tokenizer else {
             return MLXArray.zeros([1])
@@ -196,6 +225,11 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
         let suppressTokens = (talkerConfig.vocabSize - 1024 ..< talkerConfig.vocabSize)
             .filter { $0 != eosTokenId }
 
+        // Streaming decode state
+        let streamingChunkSize = 25  // ~2s at 12.5Hz codec rate
+        let contextSize = 25        // Overlap tokens for smooth audio transitions
+        var decodedTokens = 0
+
         var trailingIdx = 0
         var inputEmbeds = inputEmbedsInit
 
@@ -257,6 +291,30 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
             codeCache = nil
             Memory.clearCache()
 
+            // Streaming: decode and yield audio chunks during generation
+            if let onAudioChunk {
+                let newTokens = generatedCodes.count - decodedTokens
+                if newTokens >= streamingChunkSize {
+                    let startIdx = max(0, decodedTokens - contextSize)
+                    let codesChunk = stacked(Array(generatedCodes[startIdx...]), axis: 1)
+                    eval(codesChunk)
+
+                    var audioChunk = decodeChunk(codesChunk, chunkTokens: streamingChunkSize)
+
+                    // Trim overlap audio from previous chunk
+                    if decodedTokens > 0, startIdx < decodedTokens {
+                        let contextTokens = decodedTokens - startIdx
+                        let trimSamples = contextTokens * speechTokenizer.decodeUpsampleRate
+                        if trimSamples < audioChunk.dim(0) {
+                            audioChunk = audioChunk[trimSamples ..< audioChunk.dim(0)]
+                        }
+                    }
+
+                    decodedTokens = generatedCodes.count
+                    onAudioChunk(audioChunk)
+                }
+            }
+
             // Prepare next input
             let textEmbed: MLXArray
             if trailingIdx < trailingTextHidden.dim(1) {
@@ -297,28 +355,40 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
         )
         onInfo?(info)
 
-        // Stack and decode
+        // Streaming path: yield remaining tokens and return early
+        if let onAudioChunk {
+            if generatedCodes.count > decodedTokens {
+                let startIdx = max(0, decodedTokens - contextSize)
+                let codesChunk = stacked(Array(generatedCodes[startIdx...]), axis: 1)
+                eval(codesChunk)
+
+                var audioChunk = decodeChunk(codesChunk, chunkTokens: streamingChunkSize)
+
+                // Trim overlap audio from previous chunk
+                if decodedTokens > 0, startIdx < decodedTokens {
+                    let contextTokens = decodedTokens - startIdx
+                    let trimSamples = contextTokens * speechTokenizer.decodeUpsampleRate
+                    if trimSamples < audioChunk.dim(0) {
+                        audioChunk = audioChunk[trimSamples ..< audioChunk.dim(0)]
+                    }
+                }
+
+                onAudioChunk(audioChunk)
+            }
+            // Streaming chunks already yielded; return empty (caller uses chunks)
+            return MLXArray.zeros([1])
+        }
+
+        // Non-streaming path: full decode (existing behavior)
         let codes = stacked(generatedCodes, axis: 1) // [1, seq_len, num_code_groups]
 
-        // Streaming decode for memory efficiency
         var decodeCodes = codes
         if let refCodes {
             let refCodesT = refCodes.transposed(0, 2, 1)
             decodeCodes = concatenated([refCodesT, codes], axis: 1)
         }
 
-        var audioChunks = [MLXArray]()
-        for chunk in speechTokenizer.streamingDecode(decodeCodes, chunkTokens: 100) {
-            audioChunks.append(chunk)
-        }
-        var audio = concatenated(audioChunks, axis: -1)[0] // Remove batch dim
-
-        // Trim to valid length
-        let validLen = Int((decodeCodes[0..., 0..., 0] .> 0).sum().item(Int32.self))
-            * speechTokenizer.decodeUpsampleRate
-        if validLen > 0, validLen < audio.dim(0) {
-            audio = audio[..<validLen]
-        }
+        var audio = decodeChunk(decodeCodes, chunkTokens: 100)
 
         if let refCodes {
             let refLen = refCodes.dim(2)

commit 6d956a6edabc83ba8d72dab7e4cbead92e26a464
Author: Prince Canuma <prince.gdt@gmail.com>
Date:   Mon Feb 23 17:43:51 2026 +0100

    Fix audio entitlements for MacOS and file validation in resolveDownload (#69)
    
    * add entitlements for audio input permissions on MacOS
    
    * Fix  model validation by checking file size for required files in cache. Clear cache if incomplete.

diff --git a/Examples/VoicesApp/VoicesApp.xcodeproj/project.pbxproj b/Examples/VoicesApp/VoicesApp.xcodeproj/project.pbxproj
index d352164..32ee63d 100644
--- a/Examples/VoicesApp/VoicesApp.xcodeproj/project.pbxproj
+++ b/Examples/VoicesApp/VoicesApp.xcodeproj/project.pbxproj
@@ -376,7 +376,7 @@
 			baseConfigurationReference = CONF002 /* Debug.xcconfig */;
 			buildSettings = {
 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
-				CODE_SIGN_ENTITLEMENTS = "";
+				"CODE_SIGN_ENTITLEMENTS[sdk=macosx*]" = VoicesApp/VoicesApp.entitlements;
 				COMBINE_HIDPI_IMAGES = YES;
 				CURRENT_PROJECT_VERSION = 1;
 				DEAD_CODE_STRIPPING = YES;
@@ -410,7 +410,7 @@
 			baseConfigurationReference = CONF003 /* Release.xcconfig */;
 			buildSettings = {
 				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
-				CODE_SIGN_ENTITLEMENTS = "";
+				"CODE_SIGN_ENTITLEMENTS[sdk=macosx*]" = VoicesApp/VoicesApp.entitlements;
 				COMBINE_HIDPI_IMAGES = YES;
 				CURRENT_PROJECT_VERSION = 1;
 				DEAD_CODE_STRIPPING = YES;
diff --git a/Examples/VoicesApp/VoicesApp/Helpers/AudioRecorderManager.swift b/Examples/VoicesApp/VoicesApp/Helpers/AudioRecorderManager.swift
index 113a838..1d7cb10 100644
--- a/Examples/VoicesApp/VoicesApp/Helpers/AudioRecorderManager.swift
+++ b/Examples/VoicesApp/VoicesApp/Helpers/AudioRecorderManager.swift
@@ -18,7 +18,11 @@ class AudioRecorderManager: NSObject {
     /// The underlying capture engine (non-isolated, runs on GCD main queue)
     private let capture = AudioCaptureEngine()
 
-    func startRecording() throws {
+    func startRecording() async throws {
+        #if os(macOS)
+        try await Self.requestMicrophoneAccess()
+        #endif
+
         #if os(iOS)
         let session = AVAudioSession.sharedInstance()
         try session.setCategory(.playAndRecord, mode: .measurement, options: [.defaultToSpeaker])
@@ -83,6 +87,33 @@ class AudioRecorderManager: NSObject {
         capture.stop()
         capture.reset()
     }
+
+    #if os(macOS)
+    private static func requestMicrophoneAccess() async throws {
+        let status = AVCaptureDevice.authorizationStatus(for: .audio)
+        switch status {
+        case .authorized:
+            return
+        case .notDetermined:
+            let granted = await AVCaptureDevice.requestAccess(for: .audio)
+            if !granted {
+                throw MicrophonePermissionError.denied
+            }
+        case .denied, .restricted:
+            throw MicrophonePermissionError.denied
+        @unknown default:
+            break
+        }
+    }
+    #endif
+}
+
+enum MicrophonePermissionError: LocalizedError {
+    case denied
+
+    var errorDescription: String? {
+        "Microphone access is required for recording. Please grant access in System Settings > Privacy & Security > Microphone."
+    }
 }
 
 // MARK: - Audio Capture Engine
diff --git a/Examples/VoicesApp/VoicesApp/STTView.swift b/Examples/VoicesApp/VoicesApp/STTView.swift
index a04cb33..cfe0979 100644
--- a/Examples/VoicesApp/VoicesApp/STTView.swift
+++ b/Examples/VoicesApp/VoicesApp/STTView.swift
@@ -153,7 +153,7 @@ struct STTView: View {
                     if viewModel.isRecording {
                         viewModel.stopRecording()
                     } else {
-                        viewModel.startRecording()
+                        Task { await viewModel.startRecording() }
                     }
                 }) {
                     ViewThatFits(in: .horizontal) {
diff --git a/Examples/VoicesApp/VoicesApp/ViewModels/STTViewModel.swift b/Examples/VoicesApp/VoicesApp/ViewModels/STTViewModel.swift
index e2304c5..8f1b2e3 100644
--- a/Examples/VoicesApp/VoicesApp/ViewModels/STTViewModel.swift
+++ b/Examples/VoicesApp/VoicesApp/ViewModels/STTViewModel.swift
@@ -193,7 +193,7 @@ class STTViewModel {
     private var streamingSession: StreamingInferenceSession?
     private var lastReadPos: Int = 0
 
-    func startRecording() {
+    func startRecording() async {
         guard let model = model else {
             errorMessage = "Model not loaded"
             return
@@ -206,7 +206,7 @@ class STTViewModel {
         lastReadPos = 0
 
         do {
-            try recorder.startRecording()
+            try await recorder.startRecording()
         } catch {
             errorMessage = error.localizedDescription
             return
diff --git a/Examples/VoicesApp/VoicesApp/VoicesApp.entitlements b/Examples/VoicesApp/VoicesApp/VoicesApp.entitlements
new file mode 100644
index 0000000..b572d9c
--- /dev/null
+++ b/Examples/VoicesApp/VoicesApp/VoicesApp.entitlements
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
+<plist version="1.0">
+<dict>
+	<key>com.apple.security.device.audio-input</key>
+	<true/>
+</dict>
+</plist>
diff --git a/Sources/MLXAudioCore/ModelUtils.swift b/Sources/MLXAudioCore/ModelUtils.swift
index 76b9033..a39086e 100644
--- a/Sources/MLXAudioCore/ModelUtils.swift
+++ b/Sources/MLXAudioCore/ModelUtils.swift
@@ -74,10 +74,14 @@ public enum ModelUtils {
 
         // Check if model already exists with required files
         if FileManager.default.fileExists(atPath: modelDir.path) {
-            let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-            let hasRequiredFiles = files?.contains { $0.pathExtension == normalizedRequiredExtension } ?? false
+            let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: [.fileSizeKey])
+            let hasRequiredFile = files?.contains { file in
+                guard file.pathExtension == normalizedRequiredExtension else { return false }
+                let size = (try? file.resourceValues(forKeys: [.fileSizeKey]))?.fileSize ?? 0
+                return size > 0
+            } ?? false
 
-            if hasRequiredFiles {
+            if hasRequiredFile {
                 // Validate that config.json is valid JSON
                 let configPath = modelDir.appendingPathComponent("config.json")
                 if FileManager.default.fileExists(atPath: configPath.path) {
@@ -90,6 +94,9 @@ public enum ModelUtils {
                         try? FileManager.default.removeItem(at: modelDir)
                     }
                 }
+            } else {
+                print("Cached model appears incomplete, clearing cache...")
+                try? FileManager.default.removeItem(at: modelDir)
             }
         }
 

commit f6b872b477ef142f3a0efe39c149a6085d58d9a0
Author: Lucas Newman <lucas@future.fit>
Date:   Sun Feb 22 16:00:09 2026 -0800

    Add stock voices and clean up UI in the Voices app. (#68)

diff --git a/Examples/VoicesApp/VoicesApp.xcodeproj/project.pbxproj b/Examples/VoicesApp/VoicesApp.xcodeproj/project.pbxproj
index a6e956b..d352164 100644
--- a/Examples/VoicesApp/VoicesApp.xcodeproj/project.pbxproj
+++ b/Examples/VoicesApp/VoicesApp.xcodeproj/project.pbxproj
@@ -9,6 +9,10 @@
 /* Begin PBXBuildFile section */
 		3322ACB42F2FC458008246F3 /* MLXAudioCore in Frameworks */ = {isa = PBXBuildFile; productRef = 33B1215C2F2EA41E006A1A8A /* MLXAudioCore */; };
 		3322ACB52F2FC458008246F3 /* MLXAudioTTS in Frameworks */ = {isa = PBXBuildFile; productRef = 33B1215E2F2EA41E006A1A8A /* MLXAudioTTS */; };
+		3322ACB62F2FC458008246F3 /* MLXAudioSTT in Frameworks */ = {isa = PBXBuildFile; productRef = 33B121602F2EA41E006A1A8A /* MLXAudioSTT */; };
+		332425792F4BBFB6002EBAE4 /* Lily.wav in Resources */ = {isa = PBXBuildFile; fileRef = 332425772F4BBFB6002EBAE4 /* Lily.wav */; };
+		3324257B2F4BC0D5002EBAE4 /* James.wav in Resources */ = {isa = PBXBuildFile; fileRef = 3324257A2F4BC0D5002EBAE4 /* James.wav */; };
+		3324257D2F4BC1A9002EBAE4 /* Sophie.wav in Resources */ = {isa = PBXBuildFile; fileRef = 3324257C2F4BC1A9002EBAE4 /* Sophie.wav */; };
 		APP001 /* VoicesApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = SRC001 /* VoicesApp.swift */; };
 		APP002 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = SRC002 /* ContentView.swift */; };
 		APP003 /* Voice.swift in Sources */ = {isa = PBXBuildFile; fileRef = SRC003 /* Voice.swift */; };
@@ -22,10 +26,12 @@
 		APP011 /* STTViewModel.swift in Sources */ = {isa = PBXBuildFile; fileRef = SRC011 /* STTViewModel.swift */; };
 		APP012 /* STTSettingsView.swift in Sources */ = {isa = PBXBuildFile; fileRef = SRC012 /* STTSettingsView.swift */; };
 		APP013 /* AudioRecorderManager.swift in Sources */ = {isa = PBXBuildFile; fileRef = SRC013 /* AudioRecorderManager.swift */; };
-		3322ACB62F2FC458008246F3 /* MLXAudioSTT in Frameworks */ = {isa = PBXBuildFile; productRef = 33B121602F2EA41E006A1A8A /* MLXAudioSTT */; };
 /* End PBXBuildFile section */
 
 /* Begin PBXFileReference section */
+		332425772F4BBFB6002EBAE4 /* Lily.wav */ = {isa = PBXFileReference; lastKnownFileType = audio.wav; path = Lily.wav; sourceTree = "<group>"; };
+		3324257A2F4BC0D5002EBAE4 /* James.wav */ = {isa = PBXFileReference; lastKnownFileType = audio.wav; path = James.wav; sourceTree = "<group>"; };
+		3324257C2F4BC1A9002EBAE4 /* Sophie.wav */ = {isa = PBXFileReference; lastKnownFileType = audio.wav; path = Sophie.wav; sourceTree = "<group>"; };
 		CONF001 /* Base.xcconfig */ = {isa = PBXFileReference; lastKnownFileType = text.xcconfig; path = Base.xcconfig; sourceTree = "<group>"; };
 		CONF002 /* Debug.xcconfig */ = {isa = PBXFileReference; lastKnownFileType = text.xcconfig; path = Debug.xcconfig; sourceTree = "<group>"; };
 		CONF003 /* Release.xcconfig */ = {isa = PBXFileReference; lastKnownFileType = text.xcconfig; path = Release.xcconfig; sourceTree = "<group>"; };
@@ -40,10 +46,10 @@
 		SRC007 /* VoiceRow.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = VoiceRow.swift; sourceTree = "<group>"; };
 		SRC008 /* VoiceCollectionCard.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = VoiceCollectionCard.swift; sourceTree = "<group>"; };
 		SRC009 /* SettingsView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = SettingsView.swift; sourceTree = "<group>"; };
-		SRC0A0 /* STTView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = STTView.swift; sourceTree = "<group>"; };
 		SRC011 /* STTViewModel.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = STTViewModel.swift; sourceTree = "<group>"; };
 		SRC012 /* STTSettingsView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = STTSettingsView.swift; sourceTree = "<group>"; };
 		SRC013 /* AudioRecorderManager.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = AudioRecorderManager.swift; sourceTree = "<group>"; };
+		SRC0A0 /* STTView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = STTView.swift; sourceTree = "<group>"; };
 /* End PBXFileReference section */
 
 /* Begin PBXFrameworksBuildPhase section */
@@ -60,6 +66,16 @@
 /* End PBXFrameworksBuildPhase section */
 
 /* Begin PBXGroup section */
+		332425782F4BBFB6002EBAE4 /* Voices */ = {
+			isa = PBXGroup;
+			children = (
+				3324257A2F4BC0D5002EBAE4 /* James.wav */,
+				332425772F4BBFB6002EBAE4 /* Lily.wav */,
+				3324257C2F4BC1A9002EBAE4 /* Sophie.wav */,
+			);
+			path = Voices;
+			sourceTree = "<group>";
+		};
 		GRP001 = {
 			isa = PBXGroup;
 			children = (
@@ -72,12 +88,13 @@
 		GRP002 /* VoicesApp */ = {
 			isa = PBXGroup;
 			children = (
-				SRC001 /* VoicesApp.swift */,
-				SRC0A0 /* STTView.swift */,
+				GRP009 /* Helpers */,
 				GRP003 /* Models */,
+				SRC0A0 /* STTView.swift */,
 				GRP004 /* ViewModels */,
 				GRP005 /* Views */,
-				GRP009 /* Helpers */,
+				332425782F4BBFB6002EBAE4 /* Voices */,
+				SRC001 /* VoicesApp.swift */,
 			);
 			path = VoicesApp;
 			sourceTree = "<group>";
@@ -214,6 +231,9 @@
 			isa = PBXResourcesBuildPhase;
 			buildActionMask = 2147483647;
 			files = (
+				3324257D2F4BC1A9002EBAE4 /* Sophie.wav in Resources */,
+				332425792F4BBFB6002EBAE4 /* Lily.wav in Resources */,
+				3324257B2F4BC0D5002EBAE4 /* James.wav in Resources */,
 			);
 			runOnlyForDeploymentPostprocessing = 0;
 		};
diff --git a/Examples/VoicesApp/VoicesApp.xcodeproj/project.xcworkspace/xcshareddata/swiftpm/Package.resolved b/Examples/VoicesApp/VoicesApp.xcodeproj/project.xcworkspace/xcshareddata/swiftpm/Package.resolved
index 2dbd863..24335ff 100644
--- a/Examples/VoicesApp/VoicesApp.xcodeproj/project.xcworkspace/xcshareddata/swiftpm/Package.resolved
+++ b/Examples/VoicesApp/VoicesApp.xcodeproj/project.xcworkspace/xcshareddata/swiftpm/Package.resolved
@@ -1,5 +1,5 @@
 {
-  "originHash" : "95e623d72168a36f3c5ba247f185f9aadd411f123248ac48c44f3f002da16cdf",
+  "originHash" : "94a1789fc6ddc95bd6beb6edb357d31f927b1c4250220e547a92d2ea8da4a460",
   "pins" : [
     {
       "identity" : "async-http-client",
@@ -24,8 +24,8 @@
       "kind" : "remoteSourceControl",
       "location" : "https://github.com/ml-explore/mlx-swift.git",
       "state" : {
-        "revision" : "4dccaeda1d83cf8697f235d2786c2d72ad4bb925",
-        "version" : "0.30.3"
+        "revision" : "6ba4827fb82c97d012eec9ab4b2de21f85c3b33d",
+        "version" : "0.30.6"
       }
     },
     {
diff --git a/Examples/VoicesApp/VoicesApp/Helpers/AudioRecorderManager.swift b/Examples/VoicesApp/VoicesApp/Helpers/AudioRecorderManager.swift
index e945c9a..113a838 100644
--- a/Examples/VoicesApp/VoicesApp/Helpers/AudioRecorderManager.swift
+++ b/Examples/VoicesApp/VoicesApp/Helpers/AudioRecorderManager.swift
@@ -1,5 +1,5 @@
 import Foundation
-import AVFoundation
+@preconcurrency import AVFoundation
 import MLX
 
 /// Manages continuous audio capture via AVAudioEngine with a ring buffer.
diff --git a/Examples/VoicesApp/VoicesApp/Models/Voice.swift b/Examples/VoicesApp/VoicesApp/Models/Voice.swift
index 077d5fe..26b7fd0 100644
--- a/Examples/VoicesApp/VoicesApp/Models/Voice.swift
+++ b/Examples/VoicesApp/VoicesApp/Models/Voice.swift
@@ -76,21 +76,27 @@ extension Voice {
             description: "Velvety Actress",
             language: "English",
             color: .purple.opacity(0.3),
-            lastUsed: Date()
+            lastUsed: Date(),
+            audioFileURL: Bundle.main.url(forResource: "Lily", withExtension: "wav"),
+            transcription: "Judge Victor quickly mixed a beige waxy potion for the zesty, grumpy elf who yawns."
         ),
         Voice(
             name: "James",
             description: "Professional Narrator",
             language: "English",
             color: .blue.opacity(0.3),
-            lastUsed: Date().addingTimeInterval(-3600)
+            lastUsed: Date().addingTimeInterval(-3600),
+            audioFileURL: Bundle.main.url(forResource: "James", withExtension: "wav"),
+            transcription: "Judge Victor quickly mixed a beige waxy potion for the zesty, grumpy elf who yawns."
         ),
         Voice(
             name: "Sophie",
             description: "Warm & Friendly",
             language: "English",
             color: .orange.opacity(0.3),
-            lastUsed: Date().addingTimeInterval(-7200)
+            lastUsed: Date().addingTimeInterval(-7200),
+            audioFileURL: Bundle.main.url(forResource: "Sophie", withExtension: "wav"),
+            transcription: "Judge Victor quickly mixed a beige waxy potion for the zesty, grumpy elf who yawns."
         )
     ]
 
diff --git a/Examples/VoicesApp/VoicesApp/ViewModels/STTViewModel.swift b/Examples/VoicesApp/VoicesApp/ViewModels/STTViewModel.swift
index a2d91f0..e2304c5 100644
--- a/Examples/VoicesApp/VoicesApp/ViewModels/STTViewModel.swift
+++ b/Examples/VoicesApp/VoicesApp/ViewModels/STTViewModel.swift
@@ -3,7 +3,7 @@ import SwiftUI
 import MLXAudioSTT
 import MLXAudioCore
 import MLX
-import AVFoundation
+@preconcurrency import AVFoundation
 import Combine
 
 @MainActor
@@ -45,7 +45,7 @@ class STTViewModel {
     var audioLevel: Float { recorder.audioLevel }
 
     private var model: Qwen3ASRModel?
-    private let audioPlayer = AudioPlayerManager()
+    private let audioPlayer = AudioPlayer()
     private let recorder = AudioRecorderManager()
     private var cancellables = Set<AnyCancellable>()
     private var generationTask: Task<Void, Never>?
diff --git a/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift b/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
index 644b803..be0605b 100644
--- a/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
+++ b/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
@@ -77,7 +77,7 @@ class TTSViewModel {
     var streamingPlayback: Bool = true // Play audio as chunks are generated
 
     // Model configuration
-    var modelId: String = "mlx-community/VyvoTTS-EN-Beta-4bit"
+    var modelId: String = "mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit"
     private var loadedModelId: String?
 
     // Audio player state (manually synced from AudioPlayerManager)
@@ -86,7 +86,7 @@ class TTSViewModel {
     var duration: TimeInterval = 0
 
     private var model: SpeechGenerationModel?
-    private let audioPlayer = AudioPlayerManager()
+    private let audioPlayer = AudioPlayer()
     private var cancellables = Set<AnyCancellable>()
     private var generationTask: Task<Void, Never>?
 
@@ -394,9 +394,12 @@ class TTSViewModel {
             audioURL = finalURL
             generationProgress = "" // Clear progress
 
-            // For single chunk, load normally for playback
-            if !useStreaming {
+            // Finalize playback state once generation completes.
+            if useStreaming {
+                audioPlayer.finishStreamingInput()
+            } else {
                 audioPlayer.loadAudio(from: finalURL)
+                audioPlayer.play()
             }
 
         } catch is CancellationError {
diff --git a/Examples/VoicesApp/VoicesApp/Views/ContentView.swift b/Examples/VoicesApp/VoicesApp/Views/ContentView.swift
index e7cd5ac..d9ca5c7 100644
--- a/Examples/VoicesApp/VoicesApp/Views/ContentView.swift
+++ b/Examples/VoicesApp/VoicesApp/Views/ContentView.swift
@@ -89,21 +89,15 @@ struct ContentView: View {
     @Environment(\.scenePhase) private var scenePhase
     @State private var viewModel = TTSViewModel()
     @State private var textInput = ""
-    @State private var selectedVoice: Voice?
+    @State private var selectedVoice: Voice? = Voice.samples.first(where: { $0.name == "Lily" }) ?? Voice.samples.first
     @State private var showVoices = false
     @State private var showSettings = false
     @State private var recentlyUsed: [Voice] = Voice.samples
     @State private var customVoices: [Voice] = Voice.customVoices
 
-    #if os(iOS)
     private let buttonHeight: CGFloat = 44
-    private let buttonFont: Font = .callout
-    private let inputFont: Font = .body
-    #else
-    private let buttonHeight: CGFloat = 44
-    private let buttonFont: Font = .subheadline
+    private let buttonFont: Font = .title3
     private let inputFont: Font = .title2
-    #endif
 
     var body: some View {
         VStack(spacing: 0) {
@@ -113,18 +107,17 @@ struct ContentView: View {
                 .scrollContentBackground(.hidden)
                 .background(.clear)
                 .disabled(viewModel.isGenerating)
-                .padding(.horizontal, 12)
-                .padding(.top, 4)
                 .overlay(alignment: .topLeading) {
                     if textInput.isEmpty {
                         Text("Start typing here...")
                             .font(inputFont)
                             .foregroundStyle(.tertiary)
-                            .padding(.horizontal, 16)
-                            .padding(.top, 12)
+                            .padding(.horizontal, 4)
                             .allowsHitTesting(false)
                     }
                 }
+                .padding(.horizontal, 16)
+                .padding(.top, 16)
 
             // Bottom content (status, player)
             VStack(spacing: 4) {
@@ -169,7 +162,10 @@ struct ContentView: View {
                         }
                         .buttonStyle(.plain)
                         .help("Save audio file")
+                        .padding(.trailing, 12)
                     }
+                    .background(Color.gray.opacity(0.1))
+                    .clipShape(RoundedRectangle(cornerRadius: 8))
                     .padding(.horizontal)
                 }
             }
@@ -245,15 +241,16 @@ struct ContentView: View {
                     }
                     .buttonStyle(.plain)
                     .disabled(!canGenerate)
+                    .keyboardShortcut(.return, modifiers: [.command])
                 }
             }
             .padding(.horizontal)
             .padding(.vertical, 8)
-            #if os(iOS)
+#if os(iOS)
             .background(Color(uiColor: .systemBackground).opacity(0.95))
-            #else
+#else
             .background(.bar)
-            #endif
+#endif
         }
         .frame(maxWidth: .infinity, maxHeight: .infinity)
         .sheet(isPresented: $showVoices) {
@@ -264,17 +261,13 @@ struct ContentView: View {
                 selectedVoice = voice
                 showVoices = false
             }
-            #if os(iOS)
             .presentationDetents([.large])
             .presentationDragIndicator(.visible)
-            #endif
         }
         .sheet(isPresented: $showSettings) {
             SettingsView(viewModel: viewModel)
-                #if os(iOS)
                 .presentationDetents([.large])
                 .presentationDragIndicator(.visible)
-                #endif
         }
         .onChange(of: scenePhase) { _, phase in
             switch phase {
@@ -356,36 +349,6 @@ struct VoiceSelectorButton: View {
     }
 }
 
-// MARK: - Text Input Section
-
-struct TextInputSection: View {
-    @Binding var text: String
-    let isGenerating: Bool
-
-    var body: some View {
-        VStack(alignment: .leading, spacing: 8) {
-            Text("Enter text to synthesize")
-                .font(.subheadline)
-                .foregroundStyle(.secondary)
-
-            TextField("Type something here...", text: $text, axis: .vertical)
-                .textFieldStyle(.plain)
-                .lineLimit(5...15)
-                .padding(12)
-                .background(Color.gray.opacity(0.15))
-                .clipShape(RoundedRectangle(cornerRadius: 12))
-                .disabled(isGenerating)
-
-            HStack {
-                Spacer()
-                Text("\(text.count) characters")
-                    .font(.caption)
-                    .foregroundStyle(.secondary)
-            }
-        }
-    }
-}
-
 // MARK: - Status View
 
 struct StatusView: View {
@@ -448,15 +411,9 @@ struct CompactAudioPlayer: View {
     let onPlayPause: () -> Void
     let onSeek: (TimeInterval) -> Void
 
-    #if os(iOS)
-    private let playButtonSize: CGFloat = 32
-    private let spacing: CGFloat = 8
-    private let padding: CGFloat = 8
-    #else
     private let playButtonSize: CGFloat = 44
     private let spacing: CGFloat = 12
     private let padding: CGFloat = 12
-    #endif
 
     var body: some View {
         HStack(spacing: spacing) {
@@ -492,20 +449,18 @@ struct CompactAudioPlayer: View {
                 // Time labels
                 HStack {
                     Text(formatTime(currentTime))
-                        .font(.caption2)
+                        .font(.body)
                         .foregroundStyle(.secondary)
 
                     Spacer()
 
                     Text(formatTime(duration))
-                        .font(.caption2)
+                        .font(.body)
                         .foregroundStyle(.secondary)
                 }
             }
         }
         .padding(padding)
-        .background(Color.gray.opacity(0.1))
-        .clipShape(RoundedRectangle(cornerRadius: 10))
     }
 
     private func formatTime(_ time: TimeInterval) -> String {
@@ -558,6 +513,10 @@ struct BottomActionBar: View {
     }
 }
 
-#Preview {
+#Preview("CompactAudioPlayer") {
+    CompactAudioPlayer(isPlaying: true, currentTime: 1.0, duration: 5.0, onPlayPause: {}, onSeek: { _ in })
+}
+
+#Preview("ContentView") {
     ContentView()
 }
diff --git a/Examples/VoicesApp/VoicesApp/Views/SettingsView.swift b/Examples/VoicesApp/VoicesApp/Views/SettingsView.swift
index 0f525fe..29177ae 100644
--- a/Examples/VoicesApp/VoicesApp/Views/SettingsView.swift
+++ b/Examples/VoicesApp/VoicesApp/Views/SettingsView.swift
@@ -1,8 +1,19 @@
 import SwiftUI
+#if canImport(UIKit)
+import UIKit
+#endif
+#if canImport(AppKit)
+import AppKit
+#endif
 
 struct SettingsView: View {
     @Environment(\.dismiss) private var dismiss
     @Bindable var viewModel: TTSViewModel
+    @FocusState private var focusedField: Field?
+
+    private enum Field {
+        case modelId
+    }
 
     var body: some View {
         NavigationStack {
@@ -48,9 +59,17 @@ struct SettingsView: View {
                     TextField("Model ID", text: $viewModel.modelId)
                         .font(textFont)
                         .textFieldStyle(.plain)
+#if os(iOS)
+                        .textInputAutocapitalization(.never)
+#endif
+                        .autocorrectionDisabled()
+                        .focused($focusedField, equals: Field.modelId)
                         .padding(8)
                         .background(Color.gray.opacity(0.15))
                         .clipShape(RoundedRectangle(cornerRadius: 6))
+                        .onTapGesture {
+                            focusedField = .modelId
+                        }
 
                     Button(action: {
                         Task {
@@ -147,7 +166,7 @@ struct SettingsView: View {
                         Text("Voice Description")
                             .font(.caption2)
                             .foregroundStyle(.secondary)
-
+                        
                         TextEditor(text: $viewModel.voiceDescription)
                             .font(textFont)
                             .frame(height: 80)
@@ -407,7 +426,7 @@ struct SettingsView: View {
 
             // Reset button
             Button(action: {
-                viewModel.modelId = "mlx-community/VyvoTTS-EN-Beta-4bit"
+                viewModel.modelId = "mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit"
                 viewModel.resetGenerationParameterOverrides()
                 viewModel.useVoiceDesign = false
                 viewModel.voiceDescription = ""
@@ -447,10 +466,19 @@ struct ModelHintRow: View {
             Text(modelId)
                 .font(.system(.caption2, design: .monospaced))
                 .foregroundStyle(.secondary)
-                .textSelection(.enabled)
                 .lineLimit(1)
                 .truncationMode(.middle)
+                #if os(macOS)
+                .textSelection(.enabled)
                 .help("Click to select and copy")
+                #endif
+                #if os(iOS)
+                .contextMenu {
+                    Button("Copy Model ID") {
+                        UIPasteboard.general.string = modelId
+                    }
+                }
+                #endif
         }
     }
 }
diff --git a/Examples/VoicesApp/VoicesApp/Voices/James.wav b/Examples/VoicesApp/VoicesApp/Voices/James.wav
new file mode 100644
index 0000000..3722c10
Binary files /dev/null and b/Examples/VoicesApp/VoicesApp/Voices/James.wav differ
diff --git a/Examples/VoicesApp/VoicesApp/Voices/Lily.wav b/Examples/VoicesApp/VoicesApp/Voices/Lily.wav
new file mode 100644
index 0000000..ce122d3
Binary files /dev/null and b/Examples/VoicesApp/VoicesApp/Voices/Lily.wav differ
diff --git a/Examples/VoicesApp/VoicesApp/Voices/Sophie.wav b/Examples/VoicesApp/VoicesApp/Voices/Sophie.wav
new file mode 100644
index 0000000..0db75cf
Binary files /dev/null and b/Examples/VoicesApp/VoicesApp/Voices/Sophie.wav differ
diff --git a/Examples/VoicesApp/VoicesApp/VoicesApp.swift b/Examples/VoicesApp/VoicesApp/VoicesApp.swift
index 4d28bfb..57eedd7 100644
--- a/Examples/VoicesApp/VoicesApp/VoicesApp.swift
+++ b/Examples/VoicesApp/VoicesApp/VoicesApp.swift
@@ -1,5 +1,8 @@
 
 import SwiftUI
+#if os(macOS)
+import AppKit
+#endif
 
 @main
 struct VoicesApp: App {
@@ -8,12 +11,12 @@ struct VoicesApp: App {
             TabView {
                 ContentView()
                     .tabItem {
-                        Label("TTS", systemImage: "waveform")
+                        Label("Text to Speech", systemImage: "waveform")
                     }
 
                 STTView()
                     .tabItem {
-                        Label("STT", systemImage: "mic")
+                        Label("Speech to Text", systemImage: "mic")
                     }
             }
         }
diff --git a/Package.resolved b/Package.resolved
index 3d80c7a..9b31ab5 100644
--- a/Package.resolved
+++ b/Package.resolved
@@ -1,5 +1,5 @@
 {
-  "originHash" : "b0c154201e571a46413053160137f2f9a5a83f87e7adf7f21a06fc806388ad78",
+  "originHash" : "fc590366ff03347ae861a8e17e92e55d545f4968a556a1acee1472faba0a0090",
   "pins" : [
     {
       "identity" : "async-http-client",
@@ -24,8 +24,8 @@
       "kind" : "remoteSourceControl",
       "location" : "https://github.com/ml-explore/mlx-swift.git",
       "state" : {
-        "revision" : "4dccaeda1d83cf8697f235d2786c2d72ad4bb925",
-        "version" : "0.30.3"
+        "revision" : "6ba4827fb82c97d012eec9ab4b2de21f85c3b33d",
+        "version" : "0.30.6"
       }
     },
     {
diff --git a/Package.swift b/Package.swift
index ce3c15b..4b08e1b 100644
--- a/Package.swift
+++ b/Package.swift
@@ -50,7 +50,7 @@ let package = Package(
 
     ],
     dependencies: [
-        .package(url: "https://github.com/ml-explore/mlx-swift.git", .upToNextMajor(from: "0.30.3")),
+        .package(url: "https://github.com/ml-explore/mlx-swift.git", .upToNextMajor(from: "0.30.6")),
         .package(url: "https://github.com/ml-explore/mlx-swift-lm.git", .upToNextMajor(from: "2.30.3")),
         .package(url: "https://github.com/huggingface/swift-transformers.git", .upToNextMajor(from: "1.1.6")),
         .package(url: "https://github.com/huggingface/swift-huggingface.git", .upToNextMajor(from: "0.6.0"))

commit 3bd7b2588005c2cd6faf7778da0b93188f7ac46b
Author: Lucas Newman <lucas@future.fit>
Date:   Sun Feb 22 14:39:29 2026 -0800

    Semantic VAD example (#67)

diff --git a/Examples/SimpleChat/Package.swift b/Examples/SimpleChat/Package.swift
index 5a58c31..837708f 100644
--- a/Examples/SimpleChat/Package.swift
+++ b/Examples/SimpleChat/Package.swift
@@ -19,7 +19,8 @@ let package = Package(
             name: "SimpleChat",
             dependencies: [
                 .product(name: "MLXAudioTTS", package: "mlx-audio-swift"),
-                .product(name: "MLXAudioCore", package: "mlx-audio-swift")
+                .product(name: "MLXAudioCore", package: "mlx-audio-swift"),
+                .product(name: "MLXAudioVAD", package: "mlx-audio-swift")
             ],
             path: "SimpleChat"
         )
diff --git a/Examples/SimpleChat/SimpleChat/AudioEngine.swift b/Examples/SimpleChat/SimpleChat/AudioEngine.swift
index 056fd8c..9ffbe8b 100644
--- a/Examples/SimpleChat/SimpleChat/AudioEngine.swift
+++ b/Examples/SimpleChat/SimpleChat/AudioEngine.swift
@@ -1,9 +1,17 @@
 @preconcurrency import AVFoundation
 import MLXAudioCore
+import os
+
+struct AudioChunk: Sendable {
+    let samples: [Float]
+    let frameLength: Int
+    let sampleRate: Double
+    let channelCount: Int
+    let isInterleaved: Bool
+}
 
 @MainActor
 protocol AudioEngineDelegate: AnyObject {
-    func audioCaptureEngine(_ engine: AudioEngine, didReceive buffer: AVAudioPCMBuffer)
     func audioCaptureEngine(_ engine: AudioEngine, isSpeakingDidChange speaking: Bool)
 }
 
@@ -26,11 +34,25 @@ final class AudioEngine {
     private var firstBufferQueued = false
     private var queuedBuffers = 0
     private var streamFinished = false
+    private var pendingData = PendingDataBuffer()
+    private let speakingGate = BooleanGate(initialValue: false)
+    private let capturedChunksStream: AsyncStream<AudioChunk>
+    private let capturedChunksContinuation: AsyncStream<AudioChunk>.Continuation
 
     private let inputBufferSize: AVAudioFrameCount
 
+    var capturedChunks: AsyncStream<AudioChunk> {
+        capturedChunksStream
+    }
+
     init(inputBufferSize: AVAudioFrameCount) {
         self.inputBufferSize = inputBufferSize
+        let stream = AsyncStream.makeStream(
+            of: AudioChunk.self,
+            bufferingPolicy: .bufferingNewest(8)
+        )
+        self.capturedChunksStream = stream.stream
+        self.capturedChunksContinuation = stream.continuation
         engine.attach(streamingPlayer)
     }
 
@@ -49,20 +71,26 @@ final class AudioEngine {
 
         let input = engine.inputNode
 #if os(iOS)
-       try input.setVoiceProcessingEnabled(true)
+        try input.setVoiceProcessingEnabled(true)
 #endif
 
         let output = engine.outputNode
 #if os(iOS)
-       try output.setVoiceProcessingEnabled(true)
+        try output.setVoiceProcessingEnabled(true)
 #endif
 
         engine.connect(streamingPlayer, to: output, format: nil)
 
-        let tapHandler: (AVAudioPCMBuffer, AVAudioTime) -> Void = { [weak self] buf, _ in
-            Task { @MainActor [weak self] in
-                self?.processInputBuffer(buf)
-            }
+        let inputMuted: @Sendable () -> Bool = { [weak input] in
+            input?.isVoiceProcessingInputMuted ?? true
+        }
+        let speakingGate = speakingGate
+        let continuation = capturedChunksContinuation
+        let tapHandler: AVAudioNodeTapBlock = { buf, _ in
+            guard !inputMuted() else { return }
+            guard !speakingGate.get() else { return }
+            guard let chunk = buf.asAudioChunk() else { return }
+            continuation.yield(chunk)
         }
         input.installTap(onBus: 0, bufferSize: inputBufferSize, format: nil, block: tapHandler)
 
@@ -112,6 +140,7 @@ final class AudioEngine {
     private func resetStreamingState() {
         streamingPlayer.stop()
         isSpeaking = false
+        speakingGate.set(false)
 
         currentSpeakingTask?.cancel()
         currentSpeakingTask = nil
@@ -145,8 +174,9 @@ final class AudioEngine {
         queuedBuffers += 1
 
         let completion: @Sendable (AVAudioPlayerNodeCompletionCallbackType) -> Void = { [weak self] _ in
-            guard let self else { return }
-            Task { @MainActor in self.handleBufferConsumed() }
+            Task { @MainActor in
+                self?.handleBufferConsumed()
+            }
         }
         streamingPlayer.scheduleBuffer(buffer, completionCallbackType: .dataConsumed, completionHandler: completion)
 
@@ -155,6 +185,7 @@ final class AudioEngine {
             streamingPlayer.play()
             if !isSpeaking {
                 isSpeaking = true
+                speakingGate.set(true)
                 delegate?.audioCaptureEngine(self, isSpeakingDidChange: true)
             }
             print("Starting to speak...")
@@ -165,13 +196,88 @@ final class AudioEngine {
         queuedBuffers -= 1
         if streamFinished, queuedBuffers == 0 {
             isSpeaking = false
+            speakingGate.set(false)
             delegate?.audioCaptureEngine(self, isSpeakingDidChange: false)
             print("Finished speaking.")
         }
     }
+}
+
+// MARK: -
+
+private actor PendingDataBuffer {
+    private var data = Data()
+
+    func append(_ chunk: Data) { data.append(chunk) }
+
+    func extractChunk(ofSize size: Int) -> Data? {
+        guard data.count >= size else { return nil }
+        let chunk = data.prefix(size)
+        data.removeFirst(size)
+        return Data(chunk)
+    }
+
+    func flushRemaining() -> Data {
+        defer { data.removeAll() }
+        return data
+    }
+
+    func reset() { data.removeAll(keepingCapacity: true) }
+}
+
+private final class BooleanGate: @unchecked Sendable {
+    private let lock: OSAllocatedUnfairLock<Bool>
+
+    init(initialValue: Bool) {
+        self.lock = OSAllocatedUnfairLock(initialState: initialValue)
+    }
+
+    func get() -> Bool {
+        lock.withLock { $0 }
+    }
+
+    func set(_ value: Bool) {
+        lock.withLock { $0 = value }
+    }
+}
+
+private extension AVAudioPCMBuffer {
+    func asAudioChunk() -> AudioChunk? {
+        guard format.commonFormat == .pcmFormatFloat32 else {
+            assertionFailure("AudioEngine input tap only supports .pcmFormatFloat32.")
+            return nil
+        }
+        let frameCount = Int(frameLength)
+        let channelCount = Int(format.channelCount)
+        guard frameCount > 0, channelCount > 0 else { return nil }
+        guard let source = floatChannelData else { return nil }
+
+        let sampleCount = frameCount * channelCount
+        var samples = [Float](repeating: 0, count: sampleCount)
+
+        if format.isInterleaved {
+            _ = samples.withUnsafeMutableBufferPointer { destination in
+                memcpy(destination.baseAddress!, source[0], sampleCount * MemoryLayout<Float>.size)
+            }
+        } else {
+            for channel in 0 ..< channelCount {
+                let destinationOffset = channel * frameCount
+                _ = samples.withUnsafeMutableBufferPointer { destination in
+                    memcpy(
+                        destination.baseAddress!.advanced(by: destinationOffset),
+                        source[channel],
+                        frameCount * MemoryLayout<Float>.size
+                    )
+                }
+            }
+        }
 
-    private func processInputBuffer(_ buffer: AVAudioPCMBuffer) {
-        guard !isMicrophoneMuted else { return }
-        delegate?.audioCaptureEngine(self, didReceive: buffer)
+        return AudioChunk(
+            samples: samples,
+            frameLength: frameCount,
+            sampleRate: format.sampleRate,
+            channelCount: channelCount,
+            isInterleaved: format.isInterleaved
+        )
     }
 }
diff --git a/Examples/SimpleChat/SimpleChat/ContentView.swift b/Examples/SimpleChat/SimpleChat/ContentView.swift
index 917177f..ca106e5 100644
--- a/Examples/SimpleChat/SimpleChat/ContentView.swift
+++ b/Examples/SimpleChat/SimpleChat/ContentView.swift
@@ -1,92 +1,82 @@
 import AVFoundation
-import FoundationModels
 import SwiftUI
 #if canImport(UIKit)
 import UIKit
 #endif
 
-@Observable
 @MainActor
+@Observable
 class ContentViewModel {
-    var speechController = SpeechController()
-    
-    private static let instructions = "You are a helpful voice assistant that answers the user's questions with very consise and natural full sentences, as it will be TTS-rendered downstream as speech. You typically answer in three sentences or less. IMPORTANT: Never use lists, emojis, markdown, or other non-essential embellishments."
-    
-    @ObservationIgnored
-    private var session: LanguageModelSession?
-    
+    var conversationController = ConversationController()
+
     init() {
-        speechController.delegate = self
+        conversationController.delegate = self
     }
-    
+
     func startConversation() async throws {
         print("Starting conversation...")
-        
-        session = LanguageModelSession(instructions: Self.instructions)
-        
-        try await speechController.start()
-        
+        try await conversationController.start()
+
 #if canImport(UIKit)
         UIApplication.shared.isIdleTimerDisabled = true
 #endif
     }
-    
+
     func stopConversation() async throws {
-        try await speechController.stop()
-        
+        try await conversationController.stop()
+
 #if canImport(UIKit)
         UIApplication.shared.isIdleTimerDisabled = false
 #endif
-        
+
         print("Stopped conversation.")
     }
 }
 
-extension ContentViewModel: SpeechControllerDelegate {
-    func speechController(_ controller: SpeechController, didFinish transcription: String) {
-        Task { @MainActor in
-            guard !controller.isSpeaking && transcription.count > 1 else { return }
-            
-            print("Got transcription: '\(transcription)'")
-            let response = try await self.session?.respond(to: transcription)
-            print("Got response: \(response?.content ?? "<empty>")")
-            try await self.speechController.speak(text: response?.content ?? "I'm sorry, I didn't get that.")
-        }
+@MainActor
+extension ContentViewModel: ConversationControllerDelegate {
+    func conversationControllerDidStartUserSpeech(_ controller: ConversationController) {
+        // no-op
+    }
+
+    func conversationController(_ controller: ConversationController, didFinish transcription: String) {
+        print("Got transcription: '\(transcription)'")
     }
 }
 
+@MainActor
 struct ContentView: View {
     @State private var permissionStatus: AVAudioApplication.recordPermission = .undetermined
     @State private var viewModel = ContentViewModel()
-    
+
     var body: some View {
         VStack {
             Spacer()
-            
+
             assistantCircle
-            
+
             Spacer()
         }
         .padding()
-        .onChange(of: viewModel.speechController.canSpeak) { _, newValue in
+        .onChange(of: viewModel.conversationController.canSpeak) { _, newValue in
             if newValue {
                 Task {
-                    if !viewModel.speechController.isActive {
+                    if !viewModel.conversationController.isActive {
                         try await viewModel.startConversation()
                     }
                 }
             }
         }
     }
-    
+
     @ViewBuilder
     private var assistantCircle: some View {
-        let isActive = viewModel.speechController.isActive
-        let isSpeaking = viewModel.speechController.isSpeaking
-        
+        let isActive = viewModel.conversationController.isActive
+        let isSpeaking = viewModel.conversationController.isSpeaking
+
         ZStack {
             Button {
-                if viewModel.speechController.canSpeak {
+                if viewModel.conversationController.canSpeak {
                     Task {
                         if permissionStatus == .undetermined {
                             let granted = await AVAudioApplication.requestRecordPermission()
@@ -94,7 +84,7 @@ struct ContentView: View {
                                 permissionStatus = granted ? .granted : .denied
                             }
                         }
-                        
+
                         try await toggleConversation()
                     }
                 }
@@ -109,13 +99,13 @@ struct ContentView: View {
             .padding(64)
         }
         .scaleEffect(CGSize(width: isActive ? 1.0 : 0.7, height: isActive ? 1.0 : 0.7))
-        .animation(.easeOut(duration: 0.2), value: viewModel.speechController.isActive)
-        .animation(.easeOut(duration: 0.4), value: viewModel.speechController.isSpeaking)
+        .animation(.easeOut(duration: 0.2), value: viewModel.conversationController.isActive)
+        .animation(.easeOut(duration: 0.4), value: viewModel.conversationController.isSpeaking)
     }
-    
+
     private func toggleConversation() async throws {
         do {
-            if !viewModel.speechController.isActive {
+            if !viewModel.conversationController.isActive {
                 try await viewModel.startConversation()
             } else {
                 try await viewModel.stopConversation()
diff --git a/Examples/SimpleChat/SimpleChat/ConversationController.swift b/Examples/SimpleChat/SimpleChat/ConversationController.swift
new file mode 100644
index 0000000..3274305
--- /dev/null
+++ b/Examples/SimpleChat/SimpleChat/ConversationController.swift
@@ -0,0 +1,521 @@
+import AVFoundation
+import FoundationModels
+import MLX
+import MLXAudioCore
+import MLXAudioTTS
+import MLXLMCommon
+
+@MainActor
+protocol ConversationControllerDelegate: AnyObject {
+    func conversationControllerDidStartUserSpeech(_ controller: ConversationController)
+    func conversationController(_ controller: ConversationController, didFinish transcription: String)
+}
+
+@MainActor
+@Observable
+final class ConversationController {
+    private enum TurnMarker {
+        case complete
+        case incompleteShort
+        case incompleteLong
+    }
+
+    private enum IncompleteTimeoutKind {
+        case short
+        case long
+
+        var logLabel: String {
+            switch self {
+            case .short: "short"
+            case .long: "long"
+            }
+        }
+    }
+
+    struct UserTurnCompletionConfig: Sendable {
+        var instructions: String?
+        var incompleteShortTimeout: TimeInterval = 3
+        var incompleteLongTimeout: TimeInterval = 10
+        var incompleteShortPrompt: String?
+        var incompleteLongPrompt: String?
+
+        var completionInstructions: String {
+            instructions ?? ConversationController.defaultTurnCompletionInstructions
+        }
+
+        var shortPrompt: String {
+            incompleteShortPrompt ?? ConversationController.defaultIncompleteShortPrompt
+        }
+
+        var longPrompt: String {
+            incompleteLongPrompt ?? ConversationController.defaultIncompleteLongPrompt
+        }
+    }
+
+    private nonisolated static let baseInstructions = "You are a helpful voice assistant. Your goal is to demonstrate your capabilities in a succinct way. Your output will be spoken aloud, so avoid special characters that can't easily be spoken, such as emojis or bullet points."
+
+    private nonisolated static let defaultTurnCompletionInstructions = """
+    CRITICAL INSTRUCTION - MANDATORY RESPONSE FORMAT:
+    Every single response MUST begin with a turn completion indicator. This is not optional.
+
+    TURN COMPLETION DECISION FRAMEWORK:
+    Ask yourself: "Has the user finished speaking for this turn?"
+    This is about conversational endpoint detection, not whether you already have every detail needed.
+
+    Mark as COMPLETE (✓) when:
+    - The user has completed a request, question, or statement
+    - The utterance is syntactically and conversationally complete
+    - You can naturally respond next, even if you need a follow-up clarification
+
+    Mark as INCOMPLETE SHORT (○) when the user will likely continue soon:
+    - The user was clearly cut off mid-sentence or mid-word
+    - The user is in the middle of a thought that got interrupted
+    - Brief technical interruption (they'll resume in a few seconds)
+
+    Mark as INCOMPLETE LONG (◐) when the user needs more time:
+    - The user explicitly asks for time: "let me think", "give me a minute", "hold on"
+    - The user is clearly pondering or deliberating: "hmm", "well...", "that's a good question"
+    - The user acknowledged but hasn't answered yet: "That's interesting..."
+    - The response feels like a preamble before the actual answer
+
+    IMPORTANT DEFAULT:
+    - If uncertain, choose COMPLETE (✓).
+    - A complete request with missing details is still COMPLETE. Ask clarifying questions in your ✓ response.
+      Example: "Can you tell me what the weather is today?" -> `✓ Sure. What city are you in?`
+
+    RESPOND in one of these three formats:
+    1. If COMPLETE: `✓` followed by a space and your full substantive response
+    2. If INCOMPLETE SHORT: ONLY the character `○` (user will continue in a few seconds)
+    3. If INCOMPLETE LONG: ONLY the character `◐` (user needs more time to think)
+
+    FORMAT REQUIREMENTS:
+    - ALWAYS use single-character indicators: `✓` (complete), `○` (short wait), or `◐` (long wait)
+    - For COMPLETE: `✓` followed by a space and your full response
+    - For INCOMPLETE: ONLY the single character (`○` or `◐`) with absolutely nothing else
+    - Your turn indicator must be the very first character in your response
+    """
+
+    private nonisolated static let defaultIncompleteShortPrompt = """
+    The user paused briefly. Generate a brief, natural prompt to encourage them to continue.
+
+    IMPORTANT: You MUST respond with ✓ followed by your message. Do NOT output ○ or ◐ - the user has already been given time to continue.
+
+    Your response should:
+    - Be contextually relevant to what was just discussed
+    - Sound natural and conversational
+    - Be very concise (1 sentence max)
+    - Gently prompt them to continue
+    """
+
+    private nonisolated static let defaultIncompleteLongPrompt = """
+    The user has been quiet for a while. Generate a friendly check-in message.
+
+    IMPORTANT: You MUST respond with ✓ followed by your message. Do NOT output ○ or ◐ - the user has already been given plenty of time.
+
+    Your response should:
+    - Acknowledge they might be thinking or busy
+    - Offer to help or continue when ready
+    - Be warm and understanding
+    - Be brief (1 sentence)
+    """
+
+    @ObservationIgnored
+    weak var delegate: ConversationControllerDelegate?
+
+    private(set) var isActive: Bool = false
+    private(set) var isDetectingSpeech = false
+    private(set) var canSpeak: Bool = false
+    private(set) var isSpeaking: Bool = false
+
+    var isMicrophoneMuted: Bool {
+        audioEngine.isMicrophoneMuted
+    }
+
+    @ObservationIgnored
+    private let audioEngine: AudioEngine
+    @ObservationIgnored
+    private var configuredAudioEngine = false
+    @ObservationIgnored
+    private let vad: SemanticVAD
+    @ObservationIgnored
+    private var model: SpeechGenerationModel?
+    @ObservationIgnored
+    private var captureTask: Task<Void, Never>?
+    @ObservationIgnored
+    private var languageSession: LanguageModelSession?
+    @ObservationIgnored
+    private var incompleteTimeoutTask: Task<Void, Never>?
+    @ObservationIgnored
+    private var llmTurnTask: Task<Void, Never>?
+    @ObservationIgnored
+    private var incompleteTimeoutRevision: Int = 0
+    @ObservationIgnored
+    private var llmTurnRevision: Int = 0
+    @ObservationIgnored
+    private var turnCompletionConfig = UserTurnCompletionConfig()
+
+    init(ttsRepoId: String = "Marvis-AI/marvis-tts-250m-v0.2-MLX-8bit") {
+        self.audioEngine = AudioEngine(inputBufferSize: 1024)
+        self.vad = SemanticVAD()
+        audioEngine.delegate = self
+
+        Task { @MainActor in
+            do {
+                print("Loading TTS model: \(ttsRepoId)")
+                self.model = try await TTS.loadModel(modelRepo: ttsRepoId)
+                print("Loaded TTS model.")
+            } catch {
+                print("Error loading model: \(error)")
+            }
+            self.canSpeak = model != nil
+        }
+    }
+
+    func start() async throws {
+#if os(iOS)
+        let session = AVAudioSession.sharedInstance()
+        try session.setActive(false)
+        try session.setCategory(.playAndRecord, mode: .voiceChat, policy: .default, options: [.defaultToSpeaker])
+        try session.setPreferredIOBufferDuration(0.02)
+        try session.setActive(true)
+#endif
+
+        resetLanguageSession()
+        try await ensureEngineStarted()
+        startCaptureLoopIfNeeded()
+        isActive = true
+    }
+
+    func stop() async throws {
+        cancelTurnHandling()
+        languageSession = nil
+        stopCaptureLoop()
+        audioEngine.endSpeaking()
+        audioEngine.stop()
+        isDetectingSpeech = false
+        await vad.reset()
+#if os(iOS)
+        try AVAudioSession.sharedInstance().setActive(false)
+#endif
+        isActive = false
+    }
+
+    func toggleInputMute(toMuted: Bool?) async {
+        let currentMuted = audioEngine.isMicrophoneMuted
+        let newMuted = toMuted ?? !currentMuted
+        audioEngine.isMicrophoneMuted = newMuted
+
+        if newMuted, isDetectingSpeech {
+            cancelTurnHandling()
+            await vad.reset()
+            isDetectingSpeech = false
+        }
+    }
+
+    func stopSpeaking() async {
+        audioEngine.endSpeaking()
+    }
+
+    func setUserTurnCompletionConfig(_ config: UserTurnCompletionConfig) {
+        turnCompletionConfig = config
+    }
+
+    func speak(text: String) async throws {
+        guard let model else {
+            print("Error: TTS model not yet loaded.")
+            return
+        }
+
+        let audioStream = model.generatePCMBufferStream(
+            text: text,
+            voice: "conversational_a",
+            refAudio: nil,
+            refText: nil,
+            language: "en",
+            generationParameters: model.defaultGenerationParameters
+        )
+        try await ensureEngineStarted()
+
+        audioEngine.speak(buffersStream: audioStream)
+    }
+
+    private func resetLanguageSession() {
+        let instructions = Self.baseInstructions + "\n\n" + turnCompletionConfig.completionInstructions
+        languageSession = LanguageModelSession(instructions: instructions)
+    }
+
+    private func ensureEngineStarted() async throws {
+        if !configuredAudioEngine {
+            try audioEngine.setup()
+            configuredAudioEngine = true
+            print("Configured audio engine.")
+        }
+        try audioEngine.start()
+        audioEngine.isMicrophoneMuted = false
+        print("Started audio engine.")
+    }
+
+    private func startCaptureLoopIfNeeded() {
+        guard captureTask == nil else { return }
+
+        let stream = audioEngine.capturedChunks
+        let vad = vad
+        captureTask = Task(priority: .userInitiated) { [weak self] in
+            await Self.runCaptureLoop(stream: stream, vad: vad) { event in
+                await MainActor.run {
+                    self?.handleVADEvent(event)
+                }
+            }
+        }
+    }
+
+    private func stopCaptureLoop() {
+        captureTask?.cancel()
+        captureTask = nil
+    }
+
+    private func handleVADEvent(_ event: SemanticVAD.Event) {
+        switch event {
+        case .started:
+            isDetectingSpeech = true
+            cancelIncompleteTimeout()
+            cancelLLMTurnTask()
+            delegate?.conversationControllerDidStartUserSpeech(self)
+        case let .stopped(transcription):
+            isDetectingSpeech = false
+            let cleaned = transcription?.trimmingCharacters(in: .whitespacesAndNewlines) ?? ""
+            guard !cleaned.isEmpty else { return }
+            delegate?.conversationController(self, didFinish: cleaned)
+            handleCompletedUserTranscript(cleaned)
+        }
+    }
+
+    private func cancelTurnHandling() {
+        incompleteTimeoutRevision += 1
+        incompleteTimeoutTask?.cancel()
+        incompleteTimeoutTask = nil
+        cancelLLMTurnTask()
+    }
+
+    private func cancelLLMTurnTask() {
+        llmTurnRevision += 1
+        llmTurnTask?.cancel()
+        llmTurnTask = nil
+    }
+
+    private func cancelIncompleteTimeout() {
+        incompleteTimeoutRevision += 1
+        incompleteTimeoutTask?.cancel()
+        incompleteTimeoutTask = nil
+    }
+
+    private func scheduleIncompleteTimeout(_ kind: IncompleteTimeoutKind) {
+        cancelIncompleteTimeout()
+        let revision = incompleteTimeoutRevision
+        let timeout: TimeInterval
+        let prompt: String
+        switch kind {
+        case .short:
+            timeout = turnCompletionConfig.incompleteShortTimeout
+            prompt = turnCompletionConfig.shortPrompt
+        case .long:
+            timeout = turnCompletionConfig.incompleteLongTimeout
+            prompt = turnCompletionConfig.longPrompt
+        }
+        let delayNanos = UInt64(timeout * 1_000_000_000)
+
+        print("Turn marked incomplete (\(kind.logLabel)); scheduling reprompt in \(timeout)s")
+
+        incompleteTimeoutTask = Task { @MainActor [weak self] in
+            do {
+                try await Task.sleep(nanoseconds: delayNanos)
+            } catch {
+                return
+            }
+            guard let self else { return }
+            guard revision == self.incompleteTimeoutRevision else { return }
+            self.incompleteTimeoutTask = nil
+            self.startLLMTurnTask(
+                prompt: prompt,
+                source: "incomplete_\(kind.logLabel)_timeout",
+                originalTranscript: nil
+            )
+        }
+    }
+
+    private func startLLMTurnTask(
+        prompt: String,
+        source: String,
+        originalTranscript: String?
+    ) {
+        cancelLLMTurnTask()
+        let revision = llmTurnRevision
+        llmTurnTask = Task { @MainActor [weak self] in
+            guard let self else { return }
+            await self.requestTurnAwareResponse(
+                prompt: prompt,
+                source: source,
+                originalTranscript: originalTranscript
+            )
+            guard revision == self.llmTurnRevision else { return }
+            self.llmTurnTask = nil
+        }
+    }
+
+    private func requestTurnAwareResponse(
+        prompt: String,
+        source: String,
+        originalTranscript: String? = nil
+    ) async {
+        guard let session = languageSession else { return }
+
+        do {
+            print("Using LLM prompt:\n\(prompt)")
+            let response = try await session.respond(to: prompt)
+            print("LLM turn response [\(source)]: \(response.content)")
+            try await handleTurnResponse(
+                response.content,
+                source: source,
+                originalTranscript: originalTranscript
+            )
+        } catch is CancellationError {
+            // no-op
+        } catch {
+            print("Turn-aware response failed [\(source)]: \(error)")
+        }
+    }
+
+    private func handleTurnResponse(
+        _ text: String,
+        source: String,
+        originalTranscript: String?
+    ) async throws {
+        guard let (marker, payload) = parseTurnResponse(text) else {
+            let fallback = text.trimmingCharacters(in: .whitespacesAndNewlines)
+            guard !fallback.isEmpty else { return }
+            print("Warning: Missing turn marker; speaking raw output.")
+            try await speak(text: fallback)
+            return
+        }
+
+        switch marker {
+        case .complete:
+            cancelIncompleteTimeout()
+            let spoken = payload.trimmingCharacters(in: .whitespacesAndNewlines)
+            guard !spoken.isEmpty else { return }
+            try await speak(text: spoken)
+        case .incompleteShort:
+            if source == "user_transcript",
+               let originalTranscript,
+               isLikelyCompleteUtterance(originalTranscript) {
+                print("Guardrail: Model returned incomplete for a likely complete transcript; forcing immediate response.")
+                await requestForcedCompleteResponse(for: originalTranscript)
+                return
+            }
+            scheduleIncompleteTimeout(.short)
+        case .incompleteLong:
+            if source == "user_transcript",
+               let originalTranscript,
+               isLikelyCompleteUtterance(originalTranscript) {
+                print("Guardrail: Model returned incomplete for a likely complete transcript; forcing immediate response.")
+                await requestForcedCompleteResponse(for: originalTranscript)
+                return
+            }
+            scheduleIncompleteTimeout(.long)
+        }
+    }
+
+    private func requestForcedCompleteResponse(for transcript: String) async {
+        let prompt = """
+        The user has completed their turn. Respond now with a helpful assistant reply.
+
+        User transcript:
+        \(transcript)
+
+        IMPORTANT: Your response MUST start with ✓ followed by your response text.
+        Do NOT output ○ or ◐.
+        """
+        await requestTurnAwareResponse(
+            prompt: prompt,
+            source: "forced_complete_guardrail",
+            originalTranscript: nil
+        )
+    }
+
+    private func isLikelyCompleteUtterance(_ transcript: String) -> Bool {
+        let trimmed = transcript.trimmingCharacters(in: .whitespacesAndNewlines)
+        guard !trimmed.isEmpty else { return false }
+
+        if let last = trimmed.last, [".", "?", "!"].contains(last) {
+            return true
+        }
+        return false
+    }
+
+    private func parseTurnResponse(_ text: String) -> (TurnMarker, String)? {
+        let matches: [(marker: TurnMarker, index: String.Index)] = [
+            (.complete, text.firstIndex(of: "✓")),
+            (.incompleteShort, text.firstIndex(of: "○")),
+            (.incompleteLong, text.firstIndex(of: "◐")),
+        ].compactMap { marker, index in
+            guard let index else { return nil }
+            return (marker, index)
+        }
+
+        guard let firstMatch = matches.min(by: { $0.index < $1.index }) else { return nil }
+
+        switch firstMatch.marker {
+        case .complete:
+            var withoutMarker = text
+            withoutMarker.remove(at: firstMatch.index)
+            let content = withoutMarker.trimmingCharacters(in: .whitespacesAndNewlines)
+            return (.complete, content)
+        case .incompleteShort:
+            return (.incompleteShort, "")
+        case .incompleteLong:
+            return (.incompleteLong, "")
+        }
+    }
+
+    private func handleCompletedUserTranscript(_ transcription: String) {
+        guard !isSpeaking, transcription.count > 1 else { return }
+        let prompt = """
+        Determine whether the user has completed their turn, then respond in the required marker format.
+
+        User transcript:
+        \(transcription)
+
+        IMPORTANT:
+        - Your response MUST begin with exactly one turn marker: ✓, ○, or ◐.
+        - If COMPLETE, respond with ✓ followed by a space and your response text.
+        - If INCOMPLETE, respond with ONLY ○ or ONLY ◐ and no additional text.
+        """
+        startLLMTurnTask(
+            prompt: prompt,
+            source: "user_transcript",
+            originalTranscript: transcription
+        )
+    }
+
+    @concurrent
+    private static func runCaptureLoop(
+        stream: AsyncStream<AudioChunk>,
+        vad: SemanticVAD,
+        onEvent: @escaping @Sendable (SemanticVAD.Event) async -> Void
+    ) async {
+        for await chunk in stream {
+            if Task.isCancelled { break }
+            if let event = await vad.process(chunk: chunk) {
+                await onEvent(event)
+            }
+        }
+    }
+}
+
+// MARK: - AudioEngineDelegate
+
+extension ConversationController: @MainActor AudioEngineDelegate {
+    func audioCaptureEngine(_ engine: AudioEngine, isSpeakingDidChange speaking: Bool) {
+        isSpeaking = speaking
+    }
+}
diff --git a/Examples/SimpleChat/SimpleChat/SemanticVAD.swift b/Examples/SimpleChat/SimpleChat/SemanticVAD.swift
new file mode 100644
index 0000000..8f63842
--- /dev/null
+++ b/Examples/SimpleChat/SimpleChat/SemanticVAD.swift
@@ -0,0 +1,529 @@
+@preconcurrency import AVFoundation
+import MLX
+import MLXAudioCore
+import MLXAudioVAD
+import os
+import Speech
+
+actor SemanticVAD {
+    enum Event: Sendable {
+        case started
+        case stopped(transcription: String?)
+    }
+
+    private enum LifecycleState {
+        case idle
+        case starting
+        case ready
+        case failed
+    }
+
+    var hangTime: TimeInterval
+
+    private var analyzer: SpeechAnalyzer?
+    private var analyzerInputContinuation: AsyncStream<AnalyzerInput>.Continuation?
+    private var analysisFormat: AVAudioFormat?
+    private var analyzerConverter: AVAudioConverter?
+    private var transcriberTask: Task<Void, Never>?
+    private var smartTurnLoadTask: Task<Void, Never>?
+    private var lifecycleState: LifecycleState = .idle
+    private var isListening = false
+    private var lastSpeechTime: TimeInterval?
+    private var hasRunSmartTurnEndpointCheck = false
+    private var utteranceSamples: [Float] = []
+    private var utteranceSampleRate: Int?
+    private let detectionRMSThreshold: Float
+    private let smartTurnRepoID: String
+    private let smartTurnThreshold: Float?
+    private var smartTurnModel: SmartTurnModel?
+
+    private let transcriptState = TranscriptState()
+
+    init(
+        hangTime: TimeInterval = 0.8,
+        detectionRMSThreshold: Float = 0.01,
+        smartTurnRepoID: String = "mlx-community/smart-turn-v3",
+        smartTurnThreshold: Float? = nil
+    ) {
+        self.hangTime = hangTime
+        self.detectionRMSThreshold = detectionRMSThreshold
+        self.smartTurnRepoID = smartTurnRepoID
+        self.smartTurnThreshold = smartTurnThreshold
+
+        Task {
+            await setupSpeechPipeline()
+        }
+    }
+
+    deinit {
+        analyzerInputContinuation?.finish()
+        transcriberTask?.cancel()
+        smartTurnLoadTask?.cancel()
+    }
+
+    func process(chunk: AudioChunk) async -> Event? {
+        guard lifecycleState == .ready else { return nil }
+        guard let buffer = AVAudioPCMBuffer.makeFrom(chunk: chunk) else { return nil }
+        return await processBuffer(buffer)
+    }
+
+    func reset() async {
+        isListening = false
+        lastSpeechTime = nil
+        hasRunSmartTurnEndpointCheck = false
+        utteranceSamples.removeAll(keepingCapacity: true)
+        utteranceSampleRate = nil
+        await transcriptState.reset()
+    }
+
+    private func setupSpeechPipeline() async {
+        guard lifecycleState == .idle else { return }
+        lifecycleState = .starting
+
+        guard let locale = await SpeechTranscriber.supportedLocale(equivalentTo: Locale.current) else {
+            print("Warning: Current locale (\(Locale.current)) is not supported for speech transcription.")
+            lifecycleState = .failed
+            return
+        }
+
+        let transcriber = SpeechTranscriber(locale: locale, preset: .progressiveTranscription)
+        do {
+            try await prepareAssets(for: transcriber)
+        } catch {
+            print("Error: Unable to prepare on-device transcription: \(error)")
+            lifecycleState = .failed
+            return
+        }
+
+        guard let format = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [transcriber]) else {
+            print("Error: Speech transcriber unavailable until required assets are installed.")
+            lifecycleState = .failed
+            return
+        }
+
+        let inputStream = AsyncStream<AnalyzerInput> { continuation in
+            analyzerInputContinuation = continuation
+            continuation.onTermination = { _ in
+                Task {
+                    await self.clearAnalyzerContinuation()
+                }
+            }
+        }
+
+        let analyzer = SpeechAnalyzer(
+            modules: [transcriber],
+            options: .init(priority: .userInitiated, modelRetention: .lingering)
+        )
+
+        self.analyzer = analyzer
+        analysisFormat = format
+        analyzerConverter = nil
+        lifecycleState = .ready
+
+        Task {
+            do {
+                try await analyzer.start(inputSequence: inputStream)
+            } catch {
+                print("Error: Speech analyzer failed: \(error)")
+                lifecycleState = .failed
+            }
+        }
+
+        transcriberTask?.cancel()
+        transcriberTask = Task {
+            do {
+                for try await result in transcriber.results {
+                    await transcriptState.recordResult(result)
+                }
+            } catch {
+                print("Error: Transcriber results failed: \(error)")
+            }
+        }
+
+        startSmartTurnLoadIfNeeded()
+    }
+
+    private func clearAnalyzerContinuation() {
+        analyzerInputContinuation = nil
+    }
+
+    private func prepareAssets(for transcriber: SpeechTranscriber) async throws {
+        if let installationRequest = try await AssetInventory.assetInstallationRequest(supporting: [transcriber]) {
+            try await installationRequest.downloadAndInstall()
+        }
+    }
+
+    private func startSmartTurnLoadIfNeeded() {
+        guard smartTurnModel == nil, smartTurnLoadTask == nil else { return }
+
+        let repoID = smartTurnRepoID
+        smartTurnLoadTask = Task {
+            do {
+                let model = try await SmartTurnModel.fromPretrained(repoID)
+                self.installSmartTurnModel(model)
+            } catch {
+                self.handleSmartTurnLoadFailure(error)
+            }
+        }
+    }
+
+    private func installSmartTurnModel(_ model: SmartTurnModel) {
+        smartTurnModel = model
+        smartTurnLoadTask = nil
+        print("Loaded SmartTurn endpoint model from \(smartTurnRepoID).")
+    }
+
+    private func handleSmartTurnLoadFailure(_ error: Error) {
+        smartTurnLoadTask = nil
+        print("Warning: Failed to load SmartTurn endpoint model (\(smartTurnRepoID)): \(error)")
+    }
+
+    private func processBuffer(_ buffer: AVAudioPCMBuffer) async -> Event? {
+        enqueueForSpeechTranscription(buffer)
+
+        let now = CACurrentMediaTime()
+        let isSpeechFrame = buffer.rmsLevel() >= detectionRMSThreshold
+
+        if isSpeechFrame {
+            if !isListening {
+                isListening = true
+                hasRunSmartTurnEndpointCheck = false
+                utteranceSamples.removeAll(keepingCapacity: true)
+                utteranceSampleRate = nil
+                await transcriptState.beginUtterance()
+                print("Did start listening (RMS VAD).")
+                lastSpeechTime = now
+                appendUtteranceSamples(from: buffer)
+                return .started
+            }
+            appendUtteranceSamples(from: buffer)
+            lastSpeechTime = now
+            return nil
+        }
+
+        guard isListening, let lastSpeechTime else { return nil }
+        appendUtteranceSamples(from: buffer)
+        let finalizedTranscript = await transcriptState.currentFinalizedTranscript()
+
+        if !hasRunSmartTurnEndpointCheck, finalizedTranscript != nil {
+            hasRunSmartTurnEndpointCheck = true
+            if smartTurnDetectedEndpoint() {
+                let transcription = await consumeUtteranceTranscription()
+                print("SmartTurn detected endpoint, short-circuiting hang time after \(now - lastSpeechTime) seconds.")
+                return .stopped(transcription: transcription)
+            }
+        }
+
+        let idleDuration = now - lastSpeechTime
+        guard idleDuration > hangTime else { return nil }
+
+        let transcription = await consumeUtteranceTranscription()
+        print("Did stop listening after \(idleDuration)s below RMS threshold.")
+        return .stopped(transcription: transcription)
+    }
+
+    private func consumeUtteranceTranscription() async -> String? {
+        isListening = false
+        lastSpeechTime = nil
+        hasRunSmartTurnEndpointCheck = false
+        utteranceSamples.removeAll(keepingCapacity: true)
+        utteranceSampleRate = nil
+        return await transcriptState.consumeTranscript()
+    }
+
+    private func appendUtteranceSamples(from buffer: AVAudioPCMBuffer) {
+        let frameLength = Int(buffer.frameLength)
+        let channelCount = Int(buffer.format.channelCount)
+        guard frameLength > 0, channelCount > 0 else { return }
+        guard let channelData = buffer.floatChannelData else { return }
+        let sampleRate = Int(buffer.format.sampleRate.rounded())
+        guard sampleRate > 0 else { return }
+
+        let monoSamples: [Float]
+        if channelCount == 1 {
+            monoSamples = Array(UnsafeBufferPointer(start: channelData[0], count: frameLength))
+        } else if buffer.format.isInterleaved {
+            let interleaved = UnsafeBufferPointer(start: channelData[0], count: frameLength * channelCount)
+            var downmixed = [Float](repeating: 0, count: frameLength)
+            let gain = 1.0 / Float(channelCount)
+            for frameIdx in 0 ..< frameLength {
+                var sum: Float = 0
+                let base = frameIdx * channelCount
+                for channel in 0 ..< channelCount {
+                    sum += interleaved[base + channel]
+                }
+                downmixed[frameIdx] = sum * gain
+            }
+            monoSamples = downmixed
+        } else {
+            var downmixed = [Float](repeating: 0, count: frameLength)
+            let gain = 1.0 / Float(channelCount)
+            for channel in 0 ..< channelCount {
+                let channelPtr = UnsafeBufferPointer(start: channelData[channel], count: frameLength)
+                for frameIdx in 0 ..< frameLength {
+                    downmixed[frameIdx] += channelPtr[frameIdx] * gain
+                }
+            }
+            monoSamples = downmixed
+        }
+
+        guard !monoSamples.isEmpty else { return }
+        if let utteranceSampleRate {
+            if utteranceSampleRate == sampleRate {
+                utteranceSamples.append(contentsOf: monoSamples)
+            } else {
+                do {
+                    let resampled = try resampleAudio(monoSamples, from: sampleRate, to: utteranceSampleRate)
+                    utteranceSamples.append(contentsOf: resampled)
+                } catch {
+                    print("Warning: Failed to resample utterance chunk from \(sampleRate)Hz to \(utteranceSampleRate)Hz: \(error)")
+                }
+            }
+        } else {
+            utteranceSampleRate = sampleRate
+            utteranceSamples.append(contentsOf: monoSamples)
+        }
+    }
+
+    private func smartTurnDetectedEndpoint() -> Bool {
+        guard let smartTurnModel else { return false }
+        guard !utteranceSamples.isEmpty else { return false }
+        let sourceRate = utteranceSampleRate ?? 16000
+
+        do {
+            let resampledSamples: [Float] = if sourceRate == 16000 {
+                utteranceSamples
+            } else {
+                try resampleAudio(utteranceSamples, from: sourceRate, to: 16000)
+            }
+
+            guard !resampledSamples.isEmpty else { return false }
+            let audio = MLXArray(resampledSamples)
+            let endpoint = try smartTurnModel.predictEndpoint(
+                audio,
+                sampleRate: 16000,
+                threshold: smartTurnThreshold
+            )
+            print("SmartTurn endpoint prediction=\(endpoint.prediction) probability=\(endpoint.probability)")
+            return endpoint.prediction == 1
+        } catch {
+            print("Warning: SmartTurn endpoint detection failed: \(error)")
+            return false
+        }
+    }
+
+    private func enqueueForSpeechTranscription(_ buffer: AVAudioPCMBuffer) {
+        guard let continuation = analyzerInputContinuation else { return }
+        guard let converted = convertBufferIfNeeded(buffer) else { return }
+        continuation.yield(AnalyzerInput(buffer: converted))
+    }
+
+    private func convertBufferIfNeeded(_ buffer: AVAudioPCMBuffer) -> AVAudioPCMBuffer? {
+        guard let analysisFormat else { return buffer }
+        if formatsMatch(buffer.format, analysisFormat) { return buffer }
+
+        if analyzerConverter == nil ||
+            !formatsMatch(analyzerConverter?.inputFormat, buffer.format) ||
+            !formatsMatch(analyzerConverter?.outputFormat, analysisFormat) {
+            analyzerConverter = AVAudioConverter(from: buffer.format, to: analysisFormat)
+        }
+
+        guard let converter = analyzerConverter else {
+            print("Error: Unable to create audio converter for speech transcription.")
+            return nil
+        }
+
+        let ratio = analysisFormat.sampleRate / buffer.format.sampleRate
+        let capacity = max(AVAudioFrameCount(Double(buffer.frameLength) * ratio + 1), 1)
+        guard let outBuffer = AVAudioPCMBuffer(pcmFormat: analysisFormat, frameCapacity: capacity) else {
+            return nil
+        }
+
+        var error: NSError?
+        let inputState = OSAllocatedUnfairLock(initialState: false)
+        converter.convert(to: outBuffer, error: &error) { _, outStatus in
+            let shouldProvideInput = inputState.withLock { didProvideInput in
+                if didProvideInput {
+                    return false
+                }
+                didProvideInput = true
+                return true
+            }
+            if !shouldProvideInput {
+                outStatus.pointee = .noDataNow
+                return nil
+            }
+            outStatus.pointee = .haveData
+            return buffer
+        }
+
+        if let error {
+            print("Error: Audio conversion failed for speech transcription: \(error)")
+            return nil
+        }
+        return outBuffer.frameLength > 0 ? outBuffer : nil
+    }
+
+    private func formatsMatch(_ lhs: AVAudioFormat?, _ rhs: AVAudioFormat?) -> Bool {
+        guard let lhs, let rhs else { return false }
+        return lhs.sampleRate == rhs.sampleRate &&
+            lhs.channelCount == rhs.channelCount &&
+            lhs.commonFormat == rhs.commonFormat &&
+            lhs.isInterleaved == rhs.isInterleaved
+    }
+}
+
+private actor TranscriptState {
+    private var finalizedTranscript = ""
+    private var latestHypothesis: String?
+
+    func reset() {
+        finalizedTranscript = ""
+        latestHypothesis = nil
+    }
+
+    func beginUtterance() {
+        finalizedTranscript = ""
+        latestHypothesis = nil
+    }
+
+    func recordResult(_ result: SpeechTranscriber.Result) {
+        let text = String(result.text.characters).trimmingCharacters(in: .whitespacesAndNewlines)
+        guard !text.isEmpty else { return }
+        let isFinalized = CMTimeCompare(result.resultsFinalizationTime, result.range.end) >= 0
+
+        if isFinalized {
+            mergeFinalizedText(text)
+            latestHypothesis = nil
+        } else {
+            latestHypothesis = text
+        }
+    }
+
+    func consumeTranscript() -> String? {
+        let transcription = currentTranscript()
+
+        finalizedTranscript = ""
+        latestHypothesis = nil
+        return transcription
+    }
+
+    func currentTranscript() -> String? {
+        let finalized = finalizedTranscript.trimmingCharacters(in: .whitespacesAndNewlines)
+        let hypothesis = latestHypothesis?.trimmingCharacters(in: .whitespacesAndNewlines)
+        if !finalized.isEmpty {
+            return finalized
+        }
+        if let hypothesis, !hypothesis.isEmpty {
+            return hypothesis
+        }
+        return nil
+    }
+
+    func currentFinalizedTranscript() -> String? {
+        let finalized = finalizedTranscript.trimmingCharacters(in: .whitespacesAndNewlines)
+        return finalized.isEmpty ? nil : finalized
+    }
+
+    private func mergeFinalizedText(_ text: String) {
+        if finalizedTranscript.isEmpty {
+            finalizedTranscript = text
+            return
+        }
+        if text == finalizedTranscript || finalizedTranscript.hasSuffix(text) {
+            return
+        }
+        if text.hasPrefix(finalizedTranscript) {
+            finalizedTranscript = text
+            return
+        }
+        finalizedTranscript += " " + text
+    }
+}
+
+// MARK: - AVAudioPCMBuffer Helpers
+
+private extension AVAudioPCMBuffer {
+    static func makeFrom(chunk: AudioChunk) -> AVAudioPCMBuffer? {
+        guard chunk.channelCount > 0, chunk.frameLength > 0 else { return nil }
+        guard chunk.samples.count == chunk.frameLength * chunk.channelCount else { return nil }
+
+        guard let format = AVAudioFormat(
+            commonFormat: .pcmFormatFloat32,
+            sampleRate: chunk.sampleRate,
+            channels: AVAudioChannelCount(chunk.channelCount),
+            interleaved: chunk.isInterleaved
+        ) else {
+            return nil
+        }
+
+        guard let buffer = AVAudioPCMBuffer(
+            pcmFormat: format,
+            frameCapacity: AVAudioFrameCount(chunk.frameLength)
+        ) else {
+            return nil
+        }
+
+        buffer.frameLength = AVAudioFrameCount(chunk.frameLength)
+
+        guard let destination = buffer.floatChannelData else { return nil }
+
+        if chunk.isInterleaved {
+            _ = chunk.samples.withUnsafeBufferPointer { source in
+                memcpy(
+                    destination[0],
+                    source.baseAddress!,
+                    chunk.samples.count * MemoryLayout<Float>.size
+                )
+            }
+        } else {
+            for channel in 0 ..< chunk.channelCount {
+                let sourceOffset = channel * chunk.frameLength
+                _ = chunk.samples.withUnsafeBufferPointer { source in
+                    memcpy(
+                        destination[channel],
+                        source.baseAddress!.advanced(by: sourceOffset),
+                        chunk.frameLength * MemoryLayout<Float>.size
+                    )
+                }
+            }
+        }
+
+        return buffer
+    }
+
+    func rmsLevel() -> Float {
+        guard format.commonFormat == .pcmFormatFloat32 else {
+            assertionFailure("SemanticVAD only supports .pcmFormatFloat32.")
+            return 0
+        }
+
+        let frameCount = Int(frameLength)
+        let channelCount = Int(format.channelCount)
+        guard frameCount > 0, channelCount > 0 else { return 0 }
+        guard let data = floatChannelData else { return 0 }
+
+        var sumSquares = 0.0
+        var sampleCount = 0
+
+        if format.isInterleaved {
+            let totalSamples = frameCount * channelCount
+            for idx in 0 ..< totalSamples {
+                let sample = Double(data[0][idx])
+                sumSquares += sample * sample
+            }
+            sampleCount = totalSamples
+        } else {
+            for channel in 0 ..< channelCount {
+                for frame in 0 ..< frameCount {
+                    let sample = Double(data[channel][frame])
+                    sumSquares += sample * sample
+                }
+            }
+            sampleCount = frameCount * channelCount
+        }
+
+        guard sampleCount > 0 else { return 0 }
+        return Float(sqrt(sumSquares / Double(sampleCount)))
+    }
+}
diff --git a/Examples/SimpleChat/SimpleChat/SimpleVAD.swift b/Examples/SimpleChat/SimpleChat/SimpleVAD.swift
deleted file mode 100644
index 2b7077f..0000000
--- a/Examples/SimpleChat/SimpleChat/SimpleVAD.swift
+++ /dev/null
@@ -1,377 +0,0 @@
-@preconcurrency import AVFoundation
-import os
-import Speech
-
-@MainActor
-protocol SimpleVADDelegate: AnyObject {
-    func didStartSpeaking()
-    func didStopSpeaking(transcription: String?)
-}
-
-@MainActor
-final class SimpleVAD {
-    private enum LifecycleState {
-        case idle
-        case starting
-        case ready
-        case failed
-    }
-
-    weak var delegate: SimpleVADDelegate?
-    var hangTime: TimeInterval
-
-    private var analyzer: SpeechAnalyzer?
-    private var analyzerInputContinuation: AsyncStream<AnalyzerInput>.Continuation?
-    private var analysisFormat: AVAudioFormat?
-    private var analyzerConverter: AVAudioConverter?
-    private var transcriberTask: Task<Void, Never>?
-    private var lifecycleState: LifecycleState = .idle
-
-    private let transcriptState = TranscriptState()
-
-    init(hangTime: TimeInterval = 2.0) {
-        self.hangTime = hangTime
-
-        Task {
-            await setupSpeechPipeline()
-        }
-    }
-
-    deinit {
-        analyzerInputContinuation?.finish()
-        transcriberTask?.cancel()
-    }
-
-    func process(buffer: AVAudioPCMBuffer) {
-        guard lifecycleState == .ready, let copied = buffer.deepCopy() else { return }
-        processBuffer(copied)
-    }
-
-    func reset() {
-        Task { [weak self] in
-            await self?.transcriptState.reset()
-        }
-    }
-
-    private func setupSpeechPipeline() async {
-        guard lifecycleState == .idle else { return }
-        lifecycleState = .starting
-
-        guard let locale = await SpeechTranscriber.supportedLocale(equivalentTo: Locale.current) else {
-            print("Warning: Current locale (\(Locale.current)) is not supported for speech detection.")
-            lifecycleState = .failed
-            return
-        }
-
-        let transcriber = SpeechTranscriber(locale: locale, preset: .progressiveTranscription)
-        do {
-            try await prepareAssets(for: transcriber)
-        } catch {
-            print("Error: Unable to prepare on-device transcription: \(error)")
-            lifecycleState = .failed
-            return
-        }
-
-        let detectionOptions = SpeechDetector.DetectionOptions(sensitivityLevel: .medium)
-        let detector = SpeechDetector(detectionOptions: detectionOptions, reportResults: false)
-
-        guard let format = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [detector, transcriber]) else {
-            print("Error: Speech detector unavailable until required assets are installed.")
-            lifecycleState = .failed
-            return
-        }
-
-        let inputStream = AsyncStream<AnalyzerInput> { [weak self] continuation in
-            Task { @MainActor [weak self] in
-                self?.analyzerInputContinuation = continuation
-            }
-            continuation.onTermination = { [weak self] _ in
-                Task { @MainActor [weak self] in
-                    self?.analyzerInputContinuation = nil
-                }
-            }
-        }
-
-        let analyzer = SpeechAnalyzer(
-            modules: [detector, transcriber],
-            options: .init(priority: .userInitiated, modelRetention: .lingering)
-        )
-
-        self.analyzer = analyzer
-        analysisFormat = format
-        analyzerConverter = nil
-        lifecycleState = .ready
-
-        Task { [weak self] in
-            guard let self else { return }
-            do {
-                try await analyzer.start(inputSequence: inputStream)
-            } catch {
-                print("Error: Speech analyzer failed: \(error)")
-                lifecycleState = .failed
-            }
-        }
-
-        transcriberTask?.cancel()
-        transcriberTask = Task { [weak self] in
-            guard let self else { return }
-            do {
-                for try await result in transcriber.results {
-                    let update = await transcriptState.recordResult(result, now: CACurrentMediaTime())
-                    if update.didStart {
-                        delegate?.didStartSpeaking()
-                        print("Did start listening.")
-                    }
-                    guard case let .stop(transcription, reason, idleDuration) = update.stopDecision else { continue }
-                    logStop(reason: reason, idleDuration: idleDuration)
-                    delegate?.didStopSpeaking(transcription: transcription)
-                }
-            } catch {
-                print("Error: Transcriber results failed: \(error)")
-            }
-        }
-    }
-
-    private func prepareAssets(for transcriber: SpeechTranscriber) async throws {
-        if let installationRequest = try await AssetInventory.assetInstallationRequest(supporting: [transcriber]) {
-            try await installationRequest.downloadAndInstall()
-        }
-    }
-
-    private func processBuffer(_ buffer: AVAudioPCMBuffer) {
-        enqueueForSpeechDetection(buffer)
-
-        let now = CACurrentMediaTime()
-        let timeout = hangTime
-        Task { [weak self] in
-            guard let self else { return }
-            let stopDecision = await transcriptState.timeoutDecision(now: now, timeout: timeout)
-            guard case let .stop(transcription, reason, idleDuration) = stopDecision else { return }
-            logStop(reason: reason, idleDuration: idleDuration)
-            delegate?.didStopSpeaking(transcription: transcription)
-        }
-    }
-
-    private func enqueueForSpeechDetection(_ buffer: AVAudioPCMBuffer) {
-        guard let continuation = analyzerInputContinuation else { return }
-        guard let converted = convertBufferIfNeeded(buffer) else { return }
-        continuation.yield(AnalyzerInput(buffer: converted))
-    }
-
-    private func convertBufferIfNeeded(_ buffer: AVAudioPCMBuffer) -> AVAudioPCMBuffer? {
-        guard let analysisFormat else { return buffer }
-        if formatsMatch(buffer.format, analysisFormat) { return buffer }
-
-        if analyzerConverter == nil ||
-            !formatsMatch(analyzerConverter?.inputFormat, buffer.format) ||
-            !formatsMatch(analyzerConverter?.outputFormat, analysisFormat) {
-            analyzerConverter = AVAudioConverter(from: buffer.format, to: analysisFormat)
-        }
-
-        guard let converter = analyzerConverter else {
-            print("Error: Unable to create audio converter for speech detection.")
-            return nil
-        }
-
-        let ratio = analysisFormat.sampleRate / buffer.format.sampleRate
-        let capacity = max(AVAudioFrameCount(Double(buffer.frameLength) * ratio + 1), 1)
-        guard let outBuffer = AVAudioPCMBuffer(pcmFormat: analysisFormat, frameCapacity: capacity) else {
-            return nil
-        }
-
-        var error: NSError?
-        let inputState = OSAllocatedUnfairLock(initialState: false)
-        converter.convert(to: outBuffer, error: &error) { _, outStatus in
-            let shouldProvideInput = inputState.withLock { didProvideInput in
-                if didProvideInput {
-                    return false
-                }
-                didProvideInput = true
-                return true
-            }
-            if !shouldProvideInput {
-                outStatus.pointee = .noDataNow
-                return nil
-            }
-            outStatus.pointee = .haveData
-            return buffer
-        }
-
-        if let error {
-            print("Error: Audio conversion failed for speech detection: \(error)")
-            return nil
-        }
-        return outBuffer.frameLength > 0 ? outBuffer : nil
-    }
-
-    private func logStop(reason: TranscriptState.StopReason, idleDuration: TimeInterval) {
-        switch reason {
-        case .finalizedSegment:
-            print("Did stop listening for finalized segment.")
-        case .timeout:
-            print("Did stop listening after \(idleDuration)s without transcription updates.")
-        }
-    }
-
-    private func formatsMatch(_ lhs: AVAudioFormat?, _ rhs: AVAudioFormat?) -> Bool {
-        guard let lhs, let rhs else { return false }
-        return lhs.sampleRate == rhs.sampleRate &&
-            lhs.channelCount == rhs.channelCount &&
-            lhs.commonFormat == rhs.commonFormat &&
-            lhs.isInterleaved == rhs.isInterleaved
-    }
-
-}
-
-private actor TranscriptState {
-    enum StopReason: Sendable {
-        case finalizedSegment
-        case timeout
-    }
-
-    enum StopDecision: Sendable {
-        case none
-        case stop(transcription: String?, reason: StopReason, idleDuration: TimeInterval)
-    }
-
-    struct Update: Sendable {
-        let didStart: Bool
-        let stopDecision: StopDecision
-
-        static let none = Update(didStart: false, stopDecision: .none)
-    }
-
-    private var isListening = false
-    private var lastSpeechTime: TimeInterval?
-    private var finalizedTranscript = ""
-    private var restartBoundary: TimeInterval?
-    private var latestFinalizationTime: TimeInterval = 0
-    private var segmentMaxRangeEnd: TimeInterval = 0
-    private let finalizationEpsilon: TimeInterval = 0.001
-
-    func reset() {
-        isListening = false
-        lastSpeechTime = nil
-        finalizedTranscript = ""
-        restartBoundary = nil
-        latestFinalizationTime = 0
-        segmentMaxRangeEnd = 0
-    }
-
-    func recordResult(_ result: SpeechTranscriber.Result, now: TimeInterval) -> Update {
-        let text = String(result.text.characters).trimmingCharacters(in: .whitespacesAndNewlines)
-        let rangeStart = seconds(result.range.start)
-        let rangeEnd = seconds(result.range.end)
-        let resultsFinalization = seconds(result.resultsFinalizationTime)
-        let isFinalized = CMTimeCompare(result.resultsFinalizationTime, result.range.end) >= 0
-
-        var didStartNow = false
-
-        latestFinalizationTime = max(latestFinalizationTime, resultsFinalization)
-
-        if !isListening, let restartBoundary, rangeStart < restartBoundary {
-            return .none
-        }
-
-        if !isListening {
-            restartBoundary = nil
-            isListening = true
-            finalizedTranscript = ""
-            segmentMaxRangeEnd = 0
-            didStartNow = true
-        }
-        lastSpeechTime = now
-        segmentMaxRangeEnd = max(segmentMaxRangeEnd, rangeEnd)
-
-        if isFinalized {
-            if rangeEnd + finalizationEpsilon < segmentMaxRangeEnd {
-                segmentMaxRangeEnd = max(rangeEnd, resultsFinalization)
-            }
-            mergeFinalizedText(text)
-        }
-
-        if latestFinalizationTime + finalizationEpsilon >= segmentMaxRangeEnd, segmentMaxRangeEnd > 0 {
-            return Update(
-                didStart: didStartNow,
-                stopDecision: finishListening(reason: .finalizedSegment, idleDuration: 0, fallbackTranscription: text)
-            )
-        }
-
-        return Update(didStart: didStartNow, stopDecision: .none)
-    }
-
-    func timeoutDecision(now: TimeInterval, timeout: TimeInterval) -> StopDecision {
-        guard isListening, let lastSpeechTime else { return .none }
-
-        let idleDuration = now - lastSpeechTime
-        guard idleDuration > timeout else { return .none }
-
-        return finishListening(reason: .timeout, idleDuration: idleDuration, fallbackTranscription: nil)
-    }
-
-    private func mergeFinalizedText(_ text: String) {
-        guard !text.isEmpty else { return }
-
-        if finalizedTranscript.isEmpty {
-            finalizedTranscript = text
-            return
-        }
-        if text == finalizedTranscript || finalizedTranscript.hasSuffix(text) {
-            return
-        }
-        if text.hasPrefix(finalizedTranscript) {
-            finalizedTranscript = text
-            return
-        }
-        finalizedTranscript += " " + text
-    }
-
-    private func seconds(_ time: CMTime) -> TimeInterval {
-        let value = CMTimeGetSeconds(time)
-        return value.isFinite ? value : 0
-    }
-
-    private func finishListening(reason: StopReason, idleDuration: TimeInterval, fallbackTranscription: String?) -> StopDecision {
-        let finalized = finalizedTranscript.trimmingCharacters(in: .whitespacesAndNewlines)
-        let fallback = fallbackTranscription?.trimmingCharacters(in: .whitespacesAndNewlines)
-        let transcription: String? = if !finalized.isEmpty {
-            finalized
-        } else if let fallback, !fallback.isEmpty {
-            fallback
-        } else {
-            nil
-        }
-
-        let boundary = max(latestFinalizationTime, segmentMaxRangeEnd)
-
-        isListening = false
-        lastSpeechTime = nil
-        finalizedTranscript = ""
-        segmentMaxRangeEnd = 0
-        restartBoundary = boundary
-
-        return .stop(transcription: transcription, reason: reason, idleDuration: idleDuration)
-    }
-}
-
-// MARK: - AVAudioPCMBuffer Helpers
-
-private extension AVAudioPCMBuffer {
-    func deepCopy() -> AVAudioPCMBuffer? {
-        guard let copied = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameLength) else {
-            return nil
-        }
-        copied.frameLength = frameLength
-
-        guard let src = floatChannelData, let dst = copied.floatChannelData else {
-            return nil
-        }
-
-        let channelCount = Int(format.channelCount)
-        let bytes = Int(frameLength) * MemoryLayout<Float>.size
-        for channel in 0 ..< channelCount {
-            memcpy(dst[channel], src[channel], bytes)
-        }
-        return copied
-    }
-}
diff --git a/Examples/SimpleChat/SimpleChat/SpeechController.swift b/Examples/SimpleChat/SimpleChat/SpeechController.swift
deleted file mode 100644
index 0681278..0000000
--- a/Examples/SimpleChat/SimpleChat/SpeechController.swift
+++ /dev/null
@@ -1,151 +0,0 @@
-import AVFoundation
-import MLX
-import MLXAudioCore
-import MLXAudioTTS
-import MLXLMCommon
-
-@MainActor
-protocol SpeechControllerDelegate: AnyObject {
-    func speechController(_ controller: SpeechController, didFinish transcription: String)
-}
-
-@Observable
-@MainActor
-final class SpeechController {
-    @ObservationIgnored
-    weak var delegate: SpeechControllerDelegate?
-
-    private(set) var isActive: Bool = false
-    private(set) var isDetectingSpeech = false
-    private(set) var canSpeak: Bool = false
-    private(set) var isSpeaking: Bool = false
-
-    var isMicrophoneMuted: Bool {
-        audioEngine.isMicrophoneMuted
-    }
-
-    @ObservationIgnored
-    private let audioEngine: AudioEngine
-    @ObservationIgnored
-    private var configuredAudioEngine = false
-    @ObservationIgnored
-    private let vad: SimpleVAD
-    @ObservationIgnored
-    private var model: SpeechGenerationModel?
-
-    init(ttsRepoId: String = "mlx-community/pocket-tts") {
-        self.audioEngine = AudioEngine(inputBufferSize: 1024)
-        self.vad = SimpleVAD()
-        audioEngine.delegate = self
-        vad.delegate = self
-
-        Task { @MainActor in
-            do {
-                print("Loading TTS model: \(ttsRepoId)")
-                self.model = try await TTS.loadModel(modelRepo: ttsRepoId)
-                print("Loaded TTS model.")
-            } catch {
-                print("Error loading model: \(error)")
-            }
-            self.canSpeak = model != nil
-        }
-    }
-
-    func start() async throws {
-#if os(iOS)
-        let session = AVAudioSession.sharedInstance()
-        try session.setActive(false)
-        try session.setCategory(.playAndRecord, mode: .voiceChat, policy: .default, options: [.defaultToSpeaker])
-        try session.setPreferredIOBufferDuration(0.02)
-        try session.setActive(true)
-#endif
-
-        try await ensureEngineStarted()
-        isActive = true
-    }
-
-    func stop() async throws {
-        audioEngine.endSpeaking()
-        audioEngine.stop()
-        isDetectingSpeech = false
-        vad.reset()
-#if os(iOS)
-        try AVAudioSession.sharedInstance().setActive(false)
-#endif
-        isActive = false
-    }
-
-    func toggleInputMute(toMuted: Bool?) async {
-        let currentMuted = audioEngine.isMicrophoneMuted
-        let newMuted = toMuted ?? !currentMuted
-        audioEngine.isMicrophoneMuted = newMuted
-
-        if newMuted, isDetectingSpeech {
-            vad.reset()
-            isDetectingSpeech = false
-        }
-    }
-
-    func stopSpeaking() async {
-        audioEngine.endSpeaking()
-    }
-
-    func speak(text: String) async throws {
-        guard let model else {
-            print("Error: TTS model not yet loaded.")
-            return
-        }
-
-        let audioStream = model.generatePCMBufferStream(
-            text: text,
-            voice: "cosette",
-            refAudio: nil,
-            refText: nil,
-            language: "en"
-        )
-        try await ensureEngineStarted()
-
-        audioEngine.speak(buffersStream: audioStream)
-    }
-
-    private func ensureEngineStarted() async throws {
-        if !configuredAudioEngine {
-            try audioEngine.setup()
-            configuredAudioEngine = true
-            print("Configured audio engine.")
-        }
-        try audioEngine.start()
-        audioEngine.isMicrophoneMuted = false
-        print("Started audio engine.")
-    }
-}
-
-// MARK: - AudioEngineDelegate
-
-extension SpeechController: AudioEngineDelegate {
-    func audioCaptureEngine(_ engine: AudioEngine, didReceive buffer: AVAudioPCMBuffer) {
-        guard !audioEngine.isSpeaking else { return }
-
-        vad.process(buffer: buffer)
-    }
-
-    func audioCaptureEngine(_ engine: AudioEngine, isSpeakingDidChange speaking: Bool) {
-        isSpeaking = speaking
-    }
-}
-
-// MARK: - SimpleVADDelegate
-
-extension SpeechController: SimpleVADDelegate {
-    func didStartSpeaking() {
-        isDetectingSpeech = true
-    }
-
-    func didStopSpeaking(transcription: String?) {
-        if let transcription {
-            delegate?.speechController(self, didFinish: transcription)
-        }
-        vad.reset()
-        isDetectingSpeech = false
-    }
-}

commit ba7be5f2484e098c6d5e0e2a6af60d5a32ed1435
Author: Ben Racicot <1815385+BenRacicot@users.noreply.github.com>
Date:   Sat Feb 21 16:59:17 2026 -0500

    Fix Swift 6.2 compilation errors: @Sendable closures and OptionSet type inference (#66)
    
    Two patterns fail under Swift 6.2 (Xcode 26.2):
    
    1. Missing @Sendable on progressHandler closures — HubClient.downloadSnapshot
       expects @Sendable but fromPretrained methods omit it, causing:
       "passing non-Sendable parameter 'progressHandler' to function expecting
       a @Sendable closure"
    
    2. [.all] fails OptionSet type inference — since .all already represents
       all options, the array literal wrapper is unnecessary and causes:
       "type of expression is ambiguous without a type annotation"
    
    Fix: Add @Sendable to all progressHandler closure parameters (3 files),
    replace verify: [.all] with verify: .all (13 files).
    
    Closes #65

diff --git a/Sources/MLXAudioCodecs/Mimi/Mimi.swift b/Sources/MLXAudioCodecs/Mimi/Mimi.swift
index a405e1a..e776e59 100644
--- a/Sources/MLXAudioCodecs/Mimi/Mimi.swift
+++ b/Sources/MLXAudioCodecs/Mimi/Mimi.swift
@@ -237,7 +237,7 @@ public extension Mimi {
         repoId: String = "kyutai/moshiko-pytorch-bf16",
         filename: String = "tokenizer-e351c8d8-checkpoint125.safetensors",
         cache: HubCache = .default,
-        progressHandler: @escaping (Progress) -> Void
+        progressHandler: @Sendable @escaping (Progress) -> Void
     ) async throws -> Mimi {
         print("[Mimi] Starting Mimi model loading from \(repoId)")
 
@@ -320,7 +320,7 @@ public extension Mimi {
         print("[Mimi] Updating model parameters...")
         let updateStart = CFAbsoluteTimeGetCurrent()
         let parameters = ModuleParameters.unflattened(weights)
-        try model.update(parameters: parameters, verify: [.all])
+        try model.update(parameters: parameters, verify: .all)
         let updateTime = CFAbsoluteTimeGetCurrent() - updateStart
         print(String(format: "[Mimi] Model parameters updated in %.2f seconds", updateTime))
 
diff --git a/Sources/MLXAudioCodecs/SNAC/SNACDecoder.swift b/Sources/MLXAudioCodecs/SNAC/SNACDecoder.swift
index ec792cf..66c1db0 100644
--- a/Sources/MLXAudioCodecs/SNAC/SNACDecoder.swift
+++ b/Sources/MLXAudioCodecs/SNAC/SNACDecoder.swift
@@ -179,7 +179,7 @@ public class SNAC: Module {
         let snac = try fromConfig(configPath)
 
         let weights = try loadArrays(url: weightsPath)
-        try snac.update(parameters: ModuleParameters.unflattened(weights), verify: [.all])
+        try snac.update(parameters: ModuleParameters.unflattened(weights), verify: .all)
         eval(snac)
 
         return snac
diff --git a/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift b/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
index 272a61f..762b9d1 100644
--- a/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
+++ b/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
@@ -358,7 +358,7 @@ public final class MossFormer2SEModel: STSModel {
             }
         }
 
-        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: .all)
         eval(model)
         return MossFormer2SEModel(model: model, config: config)
     }
diff --git a/Sources/MLXAudioSTT/Models/GLMASR/GLMASR.swift b/Sources/MLXAudioSTT/Models/GLMASR/GLMASR.swift
index 884f122..e9b6119 100644
--- a/Sources/MLXAudioSTT/Models/GLMASR/GLMASR.swift
+++ b/Sources/MLXAudioSTT/Models/GLMASR/GLMASR.swift
@@ -658,7 +658,7 @@ public class GLMASRModel: Module {
                 }
             }
         }
-        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: .all)
 
         eval(model)
 
diff --git a/Sources/MLXAudioSTT/Models/Parakeet/ParakeetModel.swift b/Sources/MLXAudioSTT/Models/Parakeet/ParakeetModel.swift
index c687c49..1cc2df8 100644
--- a/Sources/MLXAudioSTT/Models/Parakeet/ParakeetModel.swift
+++ b/Sources/MLXAudioSTT/Models/Parakeet/ParakeetModel.swift
@@ -551,7 +551,7 @@ public extension ParakeetModel {
             }
         }
 
-        try model.update(parameters: ModuleParameters.unflattened(sanitized), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitized), verify: .all)
         eval(model)
         return model
     }
diff --git a/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ASR.swift b/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ASR.swift
index 2f0abca..c24321c 100644
--- a/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ASR.swift
+++ b/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ASR.swift
@@ -1518,7 +1518,7 @@ public class Qwen3ASRModel: Module {
         }
 
         // Load weights into model
-        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: .all)
         eval(model)
 
         return model
diff --git a/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ForcedAligner.swift b/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ForcedAligner.swift
index be5e97e..6f8a8a7 100644
--- a/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ForcedAligner.swift
+++ b/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ForcedAligner.swift
@@ -589,7 +589,7 @@ public class Qwen3ForcedAlignerModel: Module {
             }
         }
 
-        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: .all)
         eval(model)
 
         return model
diff --git a/Sources/MLXAudioSTT/Models/VoxtralRealtime/VoxtralRealtime.swift b/Sources/MLXAudioSTT/Models/VoxtralRealtime/VoxtralRealtime.swift
index 15a5fd6..9862a5b 100644
--- a/Sources/MLXAudioSTT/Models/VoxtralRealtime/VoxtralRealtime.swift
+++ b/Sources/MLXAudioSTT/Models/VoxtralRealtime/VoxtralRealtime.swift
@@ -417,7 +417,7 @@ public extension VoxtralRealtimeModel {
             }
         }
 
-        try model.update(parameters: ModuleParameters.unflattened(sanitized), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitized), verify: .all)
         model.ensureAdaScales(transcriptionDelayMs: config.transcriptionDelayMs)
         eval(model)
 
diff --git a/Sources/MLXAudioTTS/Models/Llama/LlamaTTS.swift b/Sources/MLXAudioTTS/Models/Llama/LlamaTTS.swift
index 65abe9b..9d283cd 100644
--- a/Sources/MLXAudioTTS/Models/Llama/LlamaTTS.swift
+++ b/Sources/MLXAudioTTS/Models/Llama/LlamaTTS.swift
@@ -957,7 +957,7 @@ public class LlamaTTSModel: Module, KVCacheDimensionProvider, SpeechGenerationMo
             }
         }
 
-        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: .all)
         eval(model)
 
         try await model.post_load_hook(model: model, modelDir: modelDir, cache: cache)
diff --git a/Sources/MLXAudioTTS/Models/Marvis/MarvisTTSModel.swift b/Sources/MLXAudioTTS/Models/Marvis/MarvisTTSModel.swift
index 95838d3..554205e 100644
--- a/Sources/MLXAudioTTS/Models/Marvis/MarvisTTSModel.swift
+++ b/Sources/MLXAudioTTS/Models/Marvis/MarvisTTSModel.swift
@@ -53,7 +53,7 @@ public final class MarvisTTSModel: Module {
         hub: HubApi = .shared,
         repoId: String,
         promptURLs: [URL]? = nil,
-        progressHandler: @escaping (Progress) -> Void
+        progressHandler: @Sendable @escaping (Progress) -> Void
     ) async throws {
         let textTokenizer = try await loadTokenizer(configuration: ModelConfiguration(id: repoId), hub: hub)
         let codec = try await Mimi.fromPretrained(progressHandler: progressHandler)
@@ -144,7 +144,7 @@ public extension MarvisTTSModel {
     static func fromPretrained(
         _ modelRepo: String = "Marvis-AI/marvis-tts-250m-v0.2-MLX-8bit",
         cache: HubCache = .default,
-        progressHandler: @escaping (Progress) -> Void = { _ in }
+        progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
     ) async throws -> MarvisTTSModel {
         Memory.cacheLimit = 100 * 1024 * 1024
 
@@ -206,7 +206,7 @@ public extension MarvisTTSModel {
         }
         
         let parameters = ModuleParameters.unflattened(weights)
-        try model.update(parameters: parameters, verify: [.all])
+        try model.update(parameters: parameters, verify: .all)
         
         eval(model)
         return model
@@ -216,7 +216,7 @@ public extension MarvisTTSModel {
         hub: HubApi = .shared,
         repoId: String = "Marvis-AI/marvis-tts-250m-v0.2-MLX-8bit",
         cache: HubCache = .default,
-        progressHandler: @escaping (Progress) -> Void
+        progressHandler: @Sendable @escaping (Progress) -> Void
     ) async throws -> MarvisTTSModel {
         _ = hub
         return try await fromPretrained(repoId, cache: cache, progressHandler: progressHandler)
diff --git a/Sources/MLXAudioTTS/Models/PocketTTS/PocketTTSModel.swift b/Sources/MLXAudioTTS/Models/PocketTTS/PocketTTSModel.swift
index 69efc11..f18a0b6 100644
--- a/Sources/MLXAudioTTS/Models/PocketTTS/PocketTTSModel.swift
+++ b/Sources/MLXAudioTTS/Models/PocketTTS/PocketTTSModel.swift
@@ -118,7 +118,7 @@ public final class PocketTTSModel: Module, SpeechGenerationModel, @unchecked Sen
     public static func loadPredefinedVoice(
         _ voiceName: String,
         modelFolder: URL,
-        progressHandler: @escaping (Progress) -> Void = { _ in }
+        progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
     ) async throws -> MLXArray? {
         _ = progressHandler
         let fileURL = modelFolder.appendingPathComponent("embeddings/\(voiceName).safetensors")
@@ -135,7 +135,7 @@ public final class PocketTTSModel: Module, SpeechGenerationModel, @unchecked Sen
     private func resolveAudioPrompt(
         voice: String?,
         refAudio: MLXArray?,
-        progressHandler: @escaping (Progress) -> Void = { _ in }
+        progressHandler: @Sendable @escaping (Progress) -> Void = { _ in }
     ) async throws -> AudioPrompt {
         if let refAudio {
             return .audio(normalizeAudio(refAudio))
@@ -355,7 +355,7 @@ public final class PocketTTSModel: Module, SpeechGenerationModel, @unchecked Sen
 
         let model = try await PocketTTSModel.fromConfig(config, modelFolder: modelDir)
         let weights = try await loadPocketTTSWeights(modelDir: modelDir)
-        try model.update(parameters: ModuleParameters.unflattened(weights), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(weights), verify: .all)
 
         eval(model)
         return model
diff --git a/Sources/MLXAudioTTS/Models/Qwen3/Qwen3.swift b/Sources/MLXAudioTTS/Models/Qwen3/Qwen3.swift
index 64c7986..1bbe98b 100644
--- a/Sources/MLXAudioTTS/Models/Qwen3/Qwen3.swift
+++ b/Sources/MLXAudioTTS/Models/Qwen3/Qwen3.swift
@@ -920,7 +920,7 @@ public class Qwen3Model: Module, KVCacheDimensionProvider, SpeechGenerationModel
 
 
 
-        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: .all)
         eval(model)
 
         try await model.post_load_hook(model: model, modelDir: modelDir, cache: cache)
diff --git a/Sources/MLXAudioTTS/Models/Soprano/Soprano.swift b/Sources/MLXAudioTTS/Models/Soprano/Soprano.swift
index 9d89e8f..c44171a 100644
--- a/Sources/MLXAudioTTS/Models/Soprano/Soprano.swift
+++ b/Sources/MLXAudioTTS/Models/Soprano/Soprano.swift
@@ -947,7 +947,7 @@ public class SopranoModel: Module, KVCacheDimensionProvider, SpeechGenerationMod
             }
         }
 
-        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
+        try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: .all)
 
         eval(model)
 

commit 57dfad2851fef84a56ef95fe205692eba1f90989
Author: Lucas Newman <lucas@future.fit>
Date:   Sat Feb 21 13:58:50 2026 -0800

    Add Smart Turn v3 model (#64)
    
    * Add Smart Turn v3 model.
    
    * Add link to license.
    
    ---------
    
    Co-authored-by: Prince Canuma <prince.gdt@gmail.com>

diff --git a/Sources/MLXAudioCore/AudioUtils.swift b/Sources/MLXAudioCore/AudioUtils.swift
index d395f58..05be64b 100644
--- a/Sources/MLXAudioCore/AudioUtils.swift
+++ b/Sources/MLXAudioCore/AudioUtils.swift
@@ -3,63 +3,91 @@ import Foundation
 import MLX
 
 public class AudioUtils {
-  enum AudioUtilsErrors: Error {
-    case cannotCreateAVAudioFormat
-  }
+    public enum AudioUtilsErrors: Error, LocalizedError {
+        case cannotCreateAVAudioFormat
+        case cannotCreateAudioBuffer
+        case cannotReadFloatChannelData
+        case invalidSampleRate(Int)
+        case resamplingFailed
 
-  private init() {}
-
-  // Debug method to write output to .wav file for checking the speech generation
-  public static func writeWavFile(samples: [Float], sampleRate: Double, fileURL: URL) throws {
-    let frameCount = AVAudioFrameCount(samples.count)
-
-    guard let format = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: sampleRate, channels: 1, interleaved: false),
-          let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount)
-    else {
-      throw AudioUtilsErrors.cannotCreateAVAudioFormat
+        public var errorDescription: String? {
+            switch self {
+            case .cannotCreateAVAudioFormat:
+                "Failed to create AVAudioFormat."
+            case .cannotCreateAudioBuffer:
+                "Failed to create audio buffer."
+            case .cannotReadFloatChannelData:
+                "Failed to access float channel data."
+            case .invalidSampleRate(let sampleRate):
+                "Sample rate must be positive, got \(sampleRate)."
+            case .resamplingFailed:
+                "Audio resampling failed."
+            }
+        }
     }
 
-    buffer.frameLength = frameCount
-    let channelData = buffer.floatChannelData![0]
-    for i in 0 ..< Int(frameCount) {
-      channelData[i] = samples[i]
-    }
+    private init() {}
 
-    let audioFile = try AVAudioFile(
-      forWriting: fileURL,
-      settings: format.settings,
-      commonFormat: format.commonFormat,
-      interleaved: format.isInterleaved
-    )
+    public static func writeWavFile(samples: [Float], sampleRate: Double, fileURL: URL) throws {
+        let frameCount = AVAudioFrameCount(samples.count)
 
-    try audioFile.write(from: buffer)
-  }
-}
+        guard let format = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: sampleRate, channels: 1, interleaved: false),
+              let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount)
+        else {
+            throw AudioUtilsErrors.cannotCreateAVAudioFormat
+        }
 
+        buffer.frameLength = frameCount
+        let channelData = buffer.floatChannelData![0]
+        for i in 0 ..< Int(frameCount) {
+            channelData[i] = samples[i]
+        }
 
+        let audioFile = try AVAudioFile(
+            forWriting: fileURL,
+            settings: format.settings,
+            commonFormat: format.commonFormat,
+            interleaved: format.isInterleaved
+        )
 
+        try audioFile.write(from: buffer)
+    }
+}
 
 /// Load audio from a file and return the sample rate and audio data.
-public func loadAudioArray(from url: URL) throws -> (Int, MLXArray) {
+public func loadAudioArray(from url: URL, sampleRate: Int? = nil) throws -> (Int, MLXArray) {
     let audioFile = try AVAudioFile(forReading: url)
     let format = audioFile.processingFormat
     let frameCount = AVAudioFrameCount(audioFile.length)
 
     guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount) else {
-        throw NSError(domain: "TestHelpers", code: 1, userInfo: [NSLocalizedDescriptionKey: "Failed to create audio buffer"])
+        throw AudioUtils.AudioUtilsErrors.cannotCreateAudioBuffer
     }
 
     try audioFile.read(into: buffer)
 
     guard let floatChannelData = buffer.floatChannelData else {
-        throw NSError(domain: "TestHelpers", code: 2, userInfo: [NSLocalizedDescriptionKey: "Failed to get float channel data"])
+        throw AudioUtils.AudioUtilsErrors.cannotReadFloatChannelData
     }
 
-    let sampleRate = Int(format.sampleRate)
+    let sourceSampleRate = Int(format.sampleRate)
     let samples = Array(UnsafeBufferPointer(start: floatChannelData[0], count: Int(buffer.frameLength)))
-    let audioData = MLXArray(samples)
+    let targetSampleRate = sampleRate ?? sourceSampleRate
+
+    if targetSampleRate <= 0 {
+        throw AudioUtils.AudioUtilsErrors.invalidSampleRate(targetSampleRate)
+    }
 
-    return (sampleRate, audioData)
+    if targetSampleRate == sourceSampleRate {
+        return (sourceSampleRate, MLXArray(samples))
+    }
+
+    let resampled = try resampleAudio(
+        samples,
+        from: sourceSampleRate,
+        to: targetSampleRate
+    )
+    return (targetSampleRate, MLXArray(resampled))
 }
 
 /// Save audio data to a WAV file.
@@ -71,13 +99,13 @@ func saveAudioArray(_ audio: MLXArray, sampleRate: Double, to url: URL) throws {
 
     let frameCount = AVAudioFrameCount(samples.count)
     guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount) else {
-        throw NSError(domain: "TestHelpers", code: 3, userInfo: [NSLocalizedDescriptionKey: "Failed to create audio buffer"])
+        throw AudioUtils.AudioUtilsErrors.cannotCreateAudioBuffer
     }
 
     buffer.frameLength = frameCount
 
     if let channelData = buffer.floatChannelData {
-        for i in 0..<samples.count {
+        for i in 0 ..< samples.count {
             channelData[0][i] = samples[i]
         }
     }
@@ -85,6 +113,100 @@ func saveAudioArray(_ audio: MLXArray, sampleRate: Double, to url: URL) throws {
     try audioFile.write(from: buffer)
 }
 
+private final class AudioConverterInputProvider: @unchecked Sendable {
+    let inputBuffer: AVAudioPCMBuffer
+    var consumedInput = false
+
+    init(inputBuffer: AVAudioPCMBuffer) {
+        self.inputBuffer = inputBuffer
+    }
+}
+
+/// Resample audio to a target sample rate.
+public func resampleAudio(
+    _ samples: [Float],
+    from sourceSampleRate: Int,
+    to targetSampleRate: Int
+) throws -> [Float] {
+    if samples.isEmpty || sourceSampleRate == targetSampleRate {
+        return samples
+    }
+    guard sourceSampleRate > 0 else {
+        throw AudioUtils.AudioUtilsErrors.resamplingFailed
+    }
+    guard targetSampleRate > 0 else {
+        throw AudioUtils.AudioUtilsErrors.resamplingFailed
+    }
+
+    guard let inputFormat = AVAudioFormat(
+        commonFormat: .pcmFormatFloat32,
+        sampleRate: Double(sourceSampleRate),
+        channels: 1,
+        interleaved: false
+    ), let outputFormat = AVAudioFormat(
+        commonFormat: .pcmFormatFloat32,
+        sampleRate: Double(targetSampleRate),
+        channels: 1,
+        interleaved: false
+    ) else {
+        throw AudioUtils.AudioUtilsErrors.resamplingFailed
+    }
+
+    guard let converter = AVAudioConverter(from: inputFormat, to: outputFormat) else {
+        throw AudioUtils.AudioUtilsErrors.resamplingFailed
+    }
+
+    let inputFrameCount = AVAudioFrameCount(samples.count)
+    guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: inputFormat, frameCapacity: inputFrameCount) else {
+        throw AudioUtils.AudioUtilsErrors.resamplingFailed
+    }
+    inputBuffer.frameLength = inputFrameCount
+    samples.withUnsafeBufferPointer { ptr in
+        guard let src = ptr.baseAddress else { return }
+        memcpy(
+            inputBuffer.floatChannelData![0],
+            src,
+            samples.count * MemoryLayout<Float>.size
+        )
+    }
+
+    let ratio = Double(targetSampleRate) / Double(sourceSampleRate)
+    let estimatedFrames = max(1, Int(ceil(Double(samples.count) * ratio)) + 64)
+    guard let outputBuffer = AVAudioPCMBuffer(
+        pcmFormat: outputFormat,
+        frameCapacity: AVAudioFrameCount(estimatedFrames)
+    ) else {
+        throw AudioUtils.AudioUtilsErrors.resamplingFailed
+    }
+
+    let provider = AudioConverterInputProvider(inputBuffer: inputBuffer)
+    var conversionError: NSError?
+    let status = converter.convert(to: outputBuffer, error: &conversionError) { _, outStatus in
+        if provider.consumedInput {
+            outStatus.pointee = .endOfStream
+            return nil
+        }
+        provider.consumedInput = true
+        outStatus.pointee = .haveData
+        return provider.inputBuffer
+    }
+
+    if let conversionError {
+        _ = conversionError
+        throw AudioUtils.AudioUtilsErrors.resamplingFailed
+    }
+
+    guard status == .haveData || status == .endOfStream || status == .inputRanDry else {
+        throw AudioUtils.AudioUtilsErrors.resamplingFailed
+    }
+
+    let outCount = Int(outputBuffer.frameLength)
+    guard let outData = outputBuffer.floatChannelData?[0], outCount > 0 else {
+        return []
+    }
+    return Array(UnsafeBufferPointer(start: outData, count: outCount))
+}
+
 /// A streaming WAV writer that allows writing audio chunks incrementally to a file.
 /// This is more memory efficient than collecting all audio in memory before writing.
 public class StreamingWAVWriter {
@@ -92,7 +214,7 @@ public class StreamingWAVWriter {
     private let sampleRate: Double
     private var audioFile: AVAudioFile?
     private let format: AVAudioFormat
-    private(set) public var framesWritten: Int = 0
+    public private(set) var framesWritten: Int = 0
 
     public init(url: URL, sampleRate: Double) throws {
         self.url = url
@@ -111,7 +233,7 @@ public class StreamingWAVWriter {
 
     /// Write a chunk of audio samples to the file.
     public func writeChunk(_ samples: [Float]) throws {
-        guard let audioFile = audioFile else {
+        guard let audioFile else {
             throw NSError(
                 domain: "StreamingWAVWriter",
                 code: 2,
@@ -131,7 +253,7 @@ public class StreamingWAVWriter {
         buffer.frameLength = frameCount
 
         if let channelData = buffer.floatChannelData {
-            for i in 0..<samples.count {
+            for i in 0 ..< samples.count {
                 channelData[0][i] = samples[i]
             }
         }
@@ -143,7 +265,7 @@ public class StreamingWAVWriter {
     /// Finalize the WAV file and return the URL.
     /// After calling this method, no more chunks can be written.
     public func finalize() -> URL {
-        audioFile = nil  // Close the file by releasing the reference
+        audioFile = nil // Close the file by releasing the reference
         return url
     }
 }
diff --git a/Sources/MLXAudioVAD/Models/SmartTurn/README.md b/Sources/MLXAudioVAD/Models/SmartTurn/README.md
new file mode 100644
index 0000000..907f37c
--- /dev/null
+++ b/Sources/MLXAudioVAD/Models/SmartTurn/README.md
@@ -0,0 +1,29 @@
+# Smart Turn v3 Endpoint Detection
+
+Smart Turn v3.2 endpoint detection for conversational turns.
+
+Model: [mlx-community/smart-turn-v3](https://huggingface.co/mlx-community/smart-turn-v3)
+
+## Quick Start
+
+```swift
+import MLX
+import MLXAudioVAD
+
+let model = try await SmartTurnModel.fromPretrained("mlx-community/smart-turn-v3")
+let audio = MLXArray.zeros([16000], type: Float.self)
+let result = try model.predictEndpoint(audio, sampleRate: 16000, threshold: 0.5)
+
+print(result.prediction)   // 0 or 1
+print(result.probability)  // sigmoid probability
+```
+
+## Notes
+
+- Input audio is expected to be mono and will be resampled to 16 kHz.
+- Audio shorter than 8 seconds is left-padded with zeros.
+- Audio longer than 8 seconds uses the latest 8 seconds.
+
+## License
+
+This code is licensed under the BSD 2-Clause "Simplified" License. See the [LICENSE](https://github.com/pipecat-ai/smart-turn/blob/main/LICENSE) for more information.
diff --git a/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurn.swift b/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurn.swift
new file mode 100644
index 0000000..2823083
--- /dev/null
+++ b/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurn.swift
@@ -0,0 +1,359 @@
+import Foundation
+import HuggingFace
+import MLX
+import MLXAudioCore
+import MLXLMCommon
+import MLXNN
+
+public struct SmartTurnEndpointOutput: Sendable {
+    public let prediction: Int
+    public let probability: Float
+
+    public init(prediction: Int, probability: Float) {
+        self.prediction = prediction
+        self.probability = probability
+    }
+}
+
+public enum SmartTurnModelError: Error, LocalizedError {
+    case invalidRepositoryID(String)
+
+    public var errorDescription: String? {
+        switch self {
+        case .invalidRepositoryID(let repoID):
+            return "Invalid repository ID: \(repoID)"
+        }
+    }
+}
+
+private class SmartTurnWhisperAttention: Module {
+    let numHeads: Int
+    let headDim: Int
+    let scaling: Float
+
+    @ModuleInfo(key: "q_proj") var qProj: Linear
+    @ModuleInfo(key: "k_proj") var kProj: Linear
+    @ModuleInfo(key: "v_proj") var vProj: Linear
+    @ModuleInfo(key: "out_proj") var outProj: Linear
+
+    init(_ config: SmartTurnEncoderConfig) {
+        numHeads = config.encoderAttentionHeads
+        headDim = config.dModel / config.encoderAttentionHeads
+        scaling = sqrt(Float(headDim))
+
+        self._qProj.wrappedValue = Linear(config.dModel, config.dModel, bias: true)
+        self._kProj.wrappedValue = Linear(config.dModel, config.dModel, bias: config.kProjBias)
+        self._vProj.wrappedValue = Linear(config.dModel, config.dModel, bias: true)
+        self._outProj.wrappedValue = Linear(config.dModel, config.dModel, bias: true)
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        let bsz = x.dim(0)
+        let seqLen = x.dim(1)
+
+        var q = qProj(x).reshaped(bsz, seqLen, numHeads, headDim)
+        var k = kProj(x).reshaped(bsz, seqLen, numHeads, headDim)
+        var v = vProj(x).reshaped(bsz, seqLen, numHeads, headDim)
+
+        q = q.transposed(0, 2, 1, 3)
+        k = k.transposed(0, 2, 3, 1)
+        v = v.transposed(0, 2, 1, 3)
+
+        var attn = MLX.matmul(q, k) / scaling
+        attn = softmax(attn, axis: -1)
+
+        var out = MLX.matmul(attn, v)
+        out = out.transposed(0, 2, 1, 3).reshaped(bsz, seqLen, numHeads * headDim)
+        return outProj(out)
+    }
+}
+
+private class SmartTurnWhisperEncoderLayer: Module {
+    @ModuleInfo(key: "self_attn_layer_norm") var selfAttnLayerNorm: LayerNorm
+    @ModuleInfo(key: "self_attn") var selfAttn: SmartTurnWhisperAttention
+    @ModuleInfo(key: "fc1") var fc1: Linear
+    @ModuleInfo(key: "fc2") var fc2: Linear
+    @ModuleInfo(key: "final_layer_norm") var finalLayerNorm: LayerNorm
+
+    init(_ config: SmartTurnEncoderConfig) {
+        self._selfAttnLayerNorm.wrappedValue = LayerNorm(dimensions: config.dModel)
+        self._selfAttn.wrappedValue = SmartTurnWhisperAttention(config)
+        self._fc1.wrappedValue = Linear(config.dModel, config.encoderFfnDim, bias: true)
+        self._fc2.wrappedValue = Linear(config.encoderFfnDim, config.dModel, bias: true)
+        self._finalLayerNorm.wrappedValue = LayerNorm(dimensions: config.dModel)
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        var residual = x
+        var h = selfAttnLayerNorm(x)
+        h = selfAttn(h)
+        h = h + residual
+
+        residual = h
+        h = finalLayerNorm(h)
+        h = fc2(gelu(fc1(h)))
+        h = h + residual
+        return h
+    }
+}
+
+private class SmartTurnWhisperEncoder: Module {
+    let config: SmartTurnEncoderConfig
+
+    @ModuleInfo(key: "conv1") var conv1: Conv1d
+    @ModuleInfo(key: "conv2") var conv2: Conv1d
+    @ModuleInfo(key: "embed_positions") var embedPositions: Embedding
+    @ModuleInfo(key: "layers") var layers: [SmartTurnWhisperEncoderLayer]
+    @ModuleInfo(key: "layer_norm") var layerNorm: LayerNorm
+
+    init(_ config: SmartTurnEncoderConfig) {
+        self.config = config
+
+        self._conv1.wrappedValue = Conv1d(
+            inputChannels: config.numMelBins,
+            outputChannels: config.dModel,
+            kernelSize: 3,
+            padding: 1
+        )
+        self._conv2.wrappedValue = Conv1d(
+            inputChannels: config.dModel,
+            outputChannels: config.dModel,
+            kernelSize: 3,
+            stride: 2,
+            padding: 1
+        )
+        self._embedPositions.wrappedValue = Embedding(
+            embeddingCount: config.maxSourcePositions,
+            dimensions: config.dModel
+        )
+        self._layers.wrappedValue = (0..<config.encoderLayers).map { _ in
+            SmartTurnWhisperEncoderLayer(config)
+        }
+        self._layerNorm.wrappedValue = LayerNorm(dimensions: config.dModel)
+    }
+
+    func callAsFunction(_ inputFeatures: MLXArray) -> MLXArray {
+        // Input follows HF convention: (batch, n_mels, n_frames)
+        var x = inputFeatures.transposed(0, 2, 1)
+        x = gelu(conv1(x))
+        x = gelu(conv2(x))
+
+        let positions = MLXArray(0..<x.dim(1))
+        x = x + embedPositions(positions)
+
+        for layer in layers {
+            x = layer(x)
+        }
+
+        return layerNorm(x)
+    }
+}
+
+public class SmartTurnModel: Module {
+    public let config: SmartTurnConfig
+
+    @ModuleInfo(key: "encoder") private var encoder: SmartTurnWhisperEncoder
+    @ModuleInfo(key: "pool_attention_0") private var poolAttention0: Linear
+    @ModuleInfo(key: "pool_attention_2") private var poolAttention2: Linear
+    @ModuleInfo(key: "classifier_0") private var classifier0: Linear
+    @ModuleInfo(key: "classifier_1") private var classifier1: LayerNorm
+    @ModuleInfo(key: "classifier_4") private var classifier4: Linear
+    @ModuleInfo(key: "classifier_6") private var classifier6: Linear
+
+    public init(_ config: SmartTurnConfig) {
+        self.config = config
+        let dModel = config.encoderConfig.dModel
+
+        self._encoder.wrappedValue = SmartTurnWhisperEncoder(config.encoderConfig)
+        self._poolAttention0.wrappedValue = Linear(dModel, 256)
+        self._poolAttention2.wrappedValue = Linear(256, 1)
+        self._classifier0.wrappedValue = Linear(dModel, 256)
+        self._classifier1.wrappedValue = LayerNorm(dimensions: 256)
+        self._classifier4.wrappedValue = Linear(256, 64)
+        self._classifier6.wrappedValue = Linear(64, 1)
+    }
+
+    public var modelDType: DType {
+        config.dtype == "float16" ? .float16 : .float32
+    }
+
+    public func callAsFunction(_ inputFeatures: MLXArray, returnLogits: Bool = false) -> MLXArray {
+        var features = inputFeatures
+        if features.ndim == 2 {
+            features = features.expandedDimensions(axis: 0)
+        }
+
+        let hidden = encoder(features.asType(modelDType))
+        var attn = poolAttention2(tanh(poolAttention0(hidden)))
+        attn = softmax(attn, axis: 1)
+        let pooled = MLX.sum(hidden * attn, axis: 1)
+
+        var x = classifier0(pooled)
+        x = classifier1(x)
+        x = gelu(x)
+        x = classifier4(x)
+        x = gelu(x)
+        let logits = classifier6(x)
+
+        if returnLogits {
+            return logits
+        }
+        return sigmoid(logits)
+    }
+
+    func prepareAudioSamples(_ audio: MLXArray, sampleRate: Int? = nil) throws -> [Float] {
+        try smartTurnPrepareAudioSamples(
+            audio,
+            sampleRate: sampleRate,
+            processor: config.processorConfig
+        )
+    }
+
+    public func prepareInputFeatures(_ audio: MLXArray, sampleRate: Int? = nil) throws -> MLXArray {
+        let prepared = try prepareAudioSamples(audio, sampleRate: sampleRate)
+        var mel = smartTurnLogMelSpectrogram(
+            prepared,
+            sampleRate: config.processorConfig.samplingRate,
+            nFft: config.processorConfig.nFft,
+            hopLength: config.processorConfig.hopLength,
+            nMels: config.processorConfig.nMels
+        )
+
+        guard mel.ndim == 2 else {
+            throw SmartTurnError.invalidFeatureShape(mel.shape)
+        }
+
+        if mel.dim(1) != config.processorConfig.nMels {
+            mel = mel.transposed(1, 0)
+        }
+
+        let frames = mel.dim(0)
+        let targetFrames = config.processorConfig.maxAudioSeconds
+            * config.processorConfig.samplingRate
+            / config.processorConfig.hopLength
+
+        if frames > targetFrames {
+            mel = mel[(frames - targetFrames)..., 0...]
+        } else if frames < targetFrames {
+            mel = MLX.padded(
+                mel,
+                widths: [.init((targetFrames - frames, 0)), .init((0, 0))]
+            )
+        }
+
+        // Return HF-style shape: (n_mels, n_frames)
+        return mel.transposed(1, 0).asType(modelDType)
+    }
+
+    public func prepareInputFeatures(audioURL: URL) throws -> MLXArray {
+        let targetSampleRate = config.processorConfig.samplingRate
+        let (_, audio) = try loadAudioArray(from: audioURL, sampleRate: targetSampleRate)
+        return try prepareInputFeatures(audio, sampleRate: targetSampleRate)
+    }
+
+    public func predictEndpoint(
+        _ audio: MLXArray,
+        sampleRate: Int? = nil,
+        threshold: Float? = nil
+    ) throws -> SmartTurnEndpointOutput {
+        let features = try prepareInputFeatures(audio, sampleRate: sampleRate)
+        let probability = self(features)[0, 0].item(Float.self)
+        let thresholdValue = threshold ?? config.processorConfig.threshold
+        let prediction = probability > thresholdValue ? 1 : 0
+        return SmartTurnEndpointOutput(prediction: prediction, probability: probability)
+    }
+
+    public func predictEndpoint(audioURL: URL, threshold: Float? = nil) throws -> SmartTurnEndpointOutput {
+        let features = try prepareInputFeatures(audioURL: audioURL)
+        let probability = self(features)[0, 0].item(Float.self)
+        let thresholdValue = threshold ?? config.processorConfig.threshold
+        let prediction = probability > thresholdValue ? 1 : 0
+        return SmartTurnEndpointOutput(prediction: prediction, probability: probability)
+    }
+
+    private static func remapKey(_ key: String) -> String {
+        var out = key
+        if out.hasPrefix("inner.") {
+            out.removeFirst("inner.".count)
+        }
+
+        out = out.replacingOccurrences(of: "pool_attention.0.", with: "pool_attention_0.")
+        out = out.replacingOccurrences(of: "pool_attention.2.", with: "pool_attention_2.")
+        out = out.replacingOccurrences(of: "classifier.0.", with: "classifier_0.")
+        out = out.replacingOccurrences(of: "classifier.1.", with: "classifier_1.")
+        out = out.replacingOccurrences(of: "classifier.4.", with: "classifier_4.")
+        out = out.replacingOccurrences(of: "classifier.6.", with: "classifier_6.")
+        return out
+    }
+
+    public static func sanitize(_ weights: [String: MLXArray]) -> [String: MLXArray] {
+        var sanitized = [String: MLXArray]()
+
+        for (key, var value) in weights {
+            if key.hasPrefix("val_") {
+                continue
+            }
+
+            let targetKey = remapKey(key)
+
+            if (targetKey == "encoder.conv1.weight" || targetKey == "encoder.conv2.weight"),
+               value.ndim == 3 {
+                value = value.transposed(0, 2, 1)
+            }
+
+            if targetKey.hasSuffix("fc1.weight"), value.ndim == 2, value.dim(0) < value.dim(1) {
+                value = value.transposed(1, 0)
+            }
+
+            if targetKey.hasSuffix("fc2.weight"), value.ndim == 2, value.dim(0) > value.dim(1) {
+                value = value.transposed(1, 0)
+            }
+
+            if targetKey == "pool_attention_0.weight", value.ndim == 2, value.dim(0) != 256 {
+                value = value.transposed(1, 0)
+            }
+
+            if targetKey == "pool_attention_2.weight", value.ndim == 2, value.dim(0) != 1 {
+                value = value.transposed(1, 0)
+            }
+
+            sanitized[targetKey] = value
+        }
+
+        return sanitized
+    }
+
+    public static func fromPretrained(_ repoId: String) async throws -> SmartTurnModel {
+        guard let repoID = Repo.ID(rawValue: repoId) else {
+            throw SmartTurnModelError.invalidRepositoryID(repoId)
+        }
+
+        let modelURL = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: "safetensors"
+        )
+
+        let configURL = modelURL.appendingPathComponent("config.json")
+        let configData = try Data(contentsOf: configURL)
+        let config = try JSONDecoder().decode(SmartTurnConfig.self, from: configData)
+
+        let model = SmartTurnModel(config)
+        let weightFiles = try FileManager.default.contentsOfDirectory(
+            at: modelURL,
+            includingPropertiesForKeys: nil
+        ).filter { $0.pathExtension == "safetensors" }
+
+        var allWeights = [String: MLXArray]()
+        for file in weightFiles {
+            let weights = try loadArrays(url: file)
+            for (k, v) in weights {
+                allWeights[k] = v
+            }
+        }
+
+        let sanitized = sanitize(allWeights)
+        try model.update(parameters: ModuleParameters.unflattened(sanitized), verify: .noUnusedKeys)
+        eval(model.parameters())
+        return model
+    }
+}
diff --git a/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurnConfig.swift b/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurnConfig.swift
new file mode 100644
index 0000000..b9e4857
--- /dev/null
+++ b/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurnConfig.swift
@@ -0,0 +1,178 @@
+import Foundation
+
+public struct SmartTurnEncoderConfig: Codable, Sendable {
+    public var modelType: String
+    public var numMelBins: Int
+    public var maxSourcePositions: Int
+    public var dModel: Int
+    public var encoderAttentionHeads: Int
+    public var encoderLayers: Int
+    public var encoderFfnDim: Int
+    public var kProjBias: Bool
+
+    enum CodingKeys: String, CodingKey {
+        case modelType = "model_type"
+        case numMelBins = "num_mel_bins"
+        case maxSourcePositions = "max_source_positions"
+        case dModel = "d_model"
+        case encoderAttentionHeads = "encoder_attention_heads"
+        case encoderLayers = "encoder_layers"
+        case encoderFfnDim = "encoder_ffn_dim"
+        case kProjBias = "k_proj_bias"
+    }
+
+    public init(
+        modelType: String = "smart_turn_encoder",
+        numMelBins: Int = 80,
+        maxSourcePositions: Int = 400,
+        dModel: Int = 384,
+        encoderAttentionHeads: Int = 6,
+        encoderLayers: Int = 4,
+        encoderFfnDim: Int = 1536,
+        kProjBias: Bool = false
+    ) {
+        self.modelType = modelType
+        self.numMelBins = numMelBins
+        self.maxSourcePositions = maxSourcePositions
+        self.dModel = dModel
+        self.encoderAttentionHeads = encoderAttentionHeads
+        self.encoderLayers = encoderLayers
+        self.encoderFfnDim = encoderFfnDim
+        self.kProjBias = kProjBias
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+        modelType = try c.decodeIfPresent(String.self, forKey: .modelType) ?? "smart_turn_encoder"
+        numMelBins = try c.decodeIfPresent(Int.self, forKey: .numMelBins) ?? 80
+        maxSourcePositions = try c.decodeIfPresent(Int.self, forKey: .maxSourcePositions) ?? 400
+        dModel = try c.decodeIfPresent(Int.self, forKey: .dModel) ?? 384
+        encoderAttentionHeads = try c.decodeIfPresent(Int.self, forKey: .encoderAttentionHeads) ?? 6
+        encoderLayers = try c.decodeIfPresent(Int.self, forKey: .encoderLayers) ?? 4
+        encoderFfnDim = try c.decodeIfPresent(Int.self, forKey: .encoderFfnDim) ?? 1536
+        kProjBias = try c.decodeIfPresent(Bool.self, forKey: .kProjBias) ?? false
+    }
+}
+
+public struct SmartTurnProcessorConfig: Codable, Sendable {
+    public var samplingRate: Int
+    public var maxAudioSeconds: Int
+    public var nFft: Int
+    public var hopLength: Int
+    public var nMels: Int
+    public var normalizeAudio: Bool
+    public var threshold: Float
+
+    enum CodingKeys: String, CodingKey {
+        case samplingRate = "sampling_rate"
+        case maxAudioSeconds = "max_audio_seconds"
+        case nFft = "n_fft"
+        case hopLength = "hop_length"
+        case nMels = "n_mels"
+        case normalizeAudio = "normalize_audio"
+        case threshold
+    }
+
+    public init(
+        samplingRate: Int = 16000,
+        maxAudioSeconds: Int = 8,
+        nFft: Int = 400,
+        hopLength: Int = 160,
+        nMels: Int = 80,
+        normalizeAudio: Bool = true,
+        threshold: Float = 0.5
+    ) {
+        self.samplingRate = samplingRate
+        self.maxAudioSeconds = maxAudioSeconds
+        self.nFft = nFft
+        self.hopLength = hopLength
+        self.nMels = nMels
+        self.normalizeAudio = normalizeAudio
+        self.threshold = threshold
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+        samplingRate = try c.decodeIfPresent(Int.self, forKey: .samplingRate) ?? 16000
+        maxAudioSeconds = try c.decodeIfPresent(Int.self, forKey: .maxAudioSeconds) ?? 8
+        nFft = try c.decodeIfPresent(Int.self, forKey: .nFft) ?? 400
+        hopLength = try c.decodeIfPresent(Int.self, forKey: .hopLength) ?? 160
+        nMels = try c.decodeIfPresent(Int.self, forKey: .nMels) ?? 80
+        normalizeAudio = try c.decodeIfPresent(Bool.self, forKey: .normalizeAudio) ?? true
+        threshold = try c.decodeIfPresent(Float.self, forKey: .threshold) ?? 0.5
+    }
+}
+
+public struct SmartTurnConfig: Codable, Sendable {
+    public var modelType: String
+    public var architecture: String
+    public var dtype: String
+    public var encoderConfig: SmartTurnEncoderConfig
+    public var processorConfig: SmartTurnProcessorConfig
+
+    // Compatibility keys from conversion scripts.
+    public var sampleRate: Int
+    public var maxAudioSeconds: Int
+    public var threshold: Float
+
+    enum CodingKeys: String, CodingKey {
+        case modelType = "model_type"
+        case architecture
+        case dtype
+        case encoderConfig = "encoder_config"
+        case processorConfig = "processor_config"
+        case sampleRate = "sample_rate"
+        case maxAudioSeconds = "max_audio_seconds"
+        case threshold
+    }
+
+    public init(
+        modelType: String = "smart_turn",
+        architecture: String = "smart_turn",
+        dtype: String = "float32",
+        encoderConfig: SmartTurnEncoderConfig = SmartTurnEncoderConfig(),
+        processorConfig: SmartTurnProcessorConfig? = nil,
+        sampleRate: Int = 16000,
+        maxAudioSeconds: Int = 8,
+        threshold: Float = 0.5
+    ) {
+        self.modelType = modelType
+        self.architecture = architecture
+        self.dtype = dtype
+        self.encoderConfig = encoderConfig
+        self.sampleRate = sampleRate
+        self.maxAudioSeconds = maxAudioSeconds
+        self.threshold = threshold
+        self.processorConfig = processorConfig ?? SmartTurnProcessorConfig(
+            samplingRate: sampleRate,
+            maxAudioSeconds: maxAudioSeconds,
+            nMels: encoderConfig.numMelBins,
+            threshold: threshold
+        )
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+
+        modelType = try c.decodeIfPresent(String.self, forKey: .modelType) ?? "smart_turn"
+        architecture = try c.decodeIfPresent(String.self, forKey: .architecture) ?? "smart_turn"
+        dtype = try c.decodeIfPresent(String.self, forKey: .dtype) ?? "float32"
+        sampleRate = try c.decodeIfPresent(Int.self, forKey: .sampleRate) ?? 16000
+        maxAudioSeconds = try c.decodeIfPresent(Int.self, forKey: .maxAudioSeconds) ?? 8
+        threshold = try c.decodeIfPresent(Float.self, forKey: .threshold) ?? 0.5
+
+        encoderConfig = try c.decodeIfPresent(SmartTurnEncoderConfig.self, forKey: .encoderConfig)
+            ?? SmartTurnEncoderConfig()
+
+        if let decodedProcessor = try c.decodeIfPresent(SmartTurnProcessorConfig.self, forKey: .processorConfig) {
+            processorConfig = decodedProcessor
+        } else {
+            processorConfig = SmartTurnProcessorConfig(
+                samplingRate: sampleRate,
+                maxAudioSeconds: maxAudioSeconds,
+                nMels: encoderConfig.numMelBins,
+                threshold: threshold
+            )
+        }
+    }
+}
diff --git a/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurnFeatures.swift b/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurnFeatures.swift
new file mode 100644
index 0000000..aa0b7bf
--- /dev/null
+++ b/Sources/MLXAudioVAD/Models/SmartTurn/SmartTurnFeatures.swift
@@ -0,0 +1,81 @@
+import Foundation
+import MLX
+import MLXAudioCore
+
+enum SmartTurnError: Error {
+    case invalidAudioShape([Int])
+    case invalidFeatureShape([Int])
+}
+
+func smartTurnPrepareAudioSamples(
+    _ audio: MLXArray,
+    sampleRate: Int?,
+    processor: SmartTurnProcessorConfig
+) throws -> [Float] {
+    guard audio.ndim == 1 else {
+        throw SmartTurnError.invalidAudioShape(audio.shape)
+    }
+
+    let sourceRate = sampleRate ?? processor.samplingRate
+    let input = audio.asArray(Float.self)
+    var prepared = try resampleAudio(
+        input,
+        from: sourceRate,
+        to: processor.samplingRate
+    )
+
+    let maxSamples = processor.maxAudioSeconds * processor.samplingRate
+    if prepared.count > maxSamples {
+        prepared = Array(prepared[(prepared.count - maxSamples)...])
+    } else if prepared.count < maxSamples {
+        let pad = [Float](repeating: 0, count: maxSamples - prepared.count)
+        prepared = pad + prepared
+    }
+
+    if processor.normalizeAudio, !prepared.isEmpty {
+        let mean = prepared.reduce(0, +) / Float(prepared.count)
+        let variance = prepared.reduce(0) { acc, x in
+            let d = x - mean
+            return acc + d * d
+        } / Float(prepared.count)
+        let std = max(sqrt(variance), 1e-7)
+        prepared = prepared.map { ($0 - mean) / std }
+    }
+
+    return prepared
+}
+
+func smartTurnLogMelSpectrogram(
+    _ audio: [Float],
+    sampleRate: Int,
+    nFft: Int,
+    hopLength: Int,
+    nMels: Int
+) -> MLXArray {
+    let audioArray = MLXArray(audio)
+    let window = hanningWindow(size: nFft)
+    let freqs = stft(audio: audioArray, window: window, nFft: nFft, hopLength: hopLength)
+
+    let magnitudes: MLXArray
+    if freqs.dim(0) > 1 {
+        magnitudes = MLX.abs(freqs[0..<(freqs.dim(0) - 1), 0...]).square()
+    } else {
+        magnitudes = MLX.abs(freqs).square()
+    }
+
+    let filters = melFilters(
+        sampleRate: sampleRate,
+        nFft: nFft,
+        nMels: nMels,
+        norm: "slaney",
+        melScale: .slaney
+    )
+
+    var melSpec = MLX.matmul(magnitudes, filters)
+    melSpec = MLX.maximum(melSpec, MLXArray(Float(1e-10)))
+    melSpec = MLX.log10(melSpec)
+    let maxVal = melSpec.max()
+    melSpec = MLX.maximum(melSpec, maxVal - MLXArray(Float(8.0)))
+    melSpec = (melSpec + MLXArray(Float(4.0))) / MLXArray(Float(4.0))
+    return melSpec
+}
diff --git a/Tests/MLXAudioVADTests.swift b/Tests/MLXAudioVADTests.swift
index e74dac7..59e1d65 100644
--- a/Tests/MLXAudioVADTests.swift
+++ b/Tests/MLXAudioVADTests.swift
@@ -488,3 +488,293 @@ struct SortformerPostprocessingTests {
     }
 }
 
+// MARK: - Smart Turn Config Tests
+
+struct SmartTurnConfigTests {
+
+    @Test func smartTurnConfigDefaults() throws {
+        let json = "{}"
+        let data = json.data(using: .utf8)!
+        let cfg = try JSONDecoder().decode(SmartTurnConfig.self, from: data)
+
+        #expect(cfg.modelType == "smart_turn")
+        #expect(cfg.architecture == "smart_turn")
+        #expect(cfg.dtype == "float32")
+        #expect(cfg.encoderConfig.numMelBins == 80)
+        #expect(cfg.processorConfig.samplingRate == 16000)
+        #expect(cfg.processorConfig.maxAudioSeconds == 8)
+    }
+
+    @Test func smartTurnConfigFromDict() throws {
+        let json = """
+        {
+            "dtype": "float16",
+            "sample_rate": 22050,
+            "max_audio_seconds": 6,
+            "threshold": 0.42,
+            "encoder_config": {
+                "num_mel_bins": 8,
+                "max_source_positions": 64,
+                "d_model": 16,
+                "encoder_attention_heads": 2,
+                "encoder_layers": 1,
+                "encoder_ffn_dim": 32,
+                "k_proj_bias": false
+            },
+            "processor_config": {
+                "sampling_rate": 16000,
+                "max_audio_seconds": 8,
+                "n_fft": 400,
+                "hop_length": 160,
+                "n_mels": 8,
+                "normalize_audio": true,
+                "threshold": 0.5
+            }
+        }
+        """
+        let data = json.data(using: .utf8)!
+        let cfg = try JSONDecoder().decode(SmartTurnConfig.self, from: data)
+
+        #expect(cfg.dtype == "float16")
+        #expect(cfg.sampleRate == 22050)
+        #expect(cfg.maxAudioSeconds == 6)
+        #expect(abs(cfg.threshold - 0.42) < 1e-6)
+        #expect(cfg.encoderConfig.dModel == 16)
+        #expect(cfg.processorConfig.nMels == 8)
+    }
+
+    @Test func smartTurnSynthesizesProcessorConfig() throws {
+        let json = """
+        {
+            "sample_rate": 24000,
+            "max_audio_seconds": 5,
+            "threshold": 0.33,
+            "encoder_config": { "num_mel_bins": 64 }
+        }
+        """
+        let data = json.data(using: .utf8)!
+        let cfg = try JSONDecoder().decode(SmartTurnConfig.self, from: data)
+
+        #expect(cfg.processorConfig.samplingRate == 24000)
+        #expect(cfg.processorConfig.maxAudioSeconds == 5)
+        #expect(cfg.processorConfig.nMels == 64)
+        #expect(abs(cfg.processorConfig.threshold - 0.33) < 1e-6)
+    }
+}
+
+// MARK: - Smart Turn Model Tests
+
+private func makeTinySmartTurnConfig(dtype: String = "float32") -> SmartTurnConfig {
+    let encoder = SmartTurnEncoderConfig(
+        numMelBins: 8,
+        maxSourcePositions: 64,
+        dModel: 16,
+        encoderAttentionHeads: 2,
+        encoderLayers: 1,
+        encoderFfnDim: 32,
+        kProjBias: false
+    )
+    let processor = SmartTurnProcessorConfig(
+        samplingRate: 16000,
+        maxAudioSeconds: 8,
+        nFft: 400,
+        hopLength: 160,
+        nMels: 8,
+        normalizeAudio: true,
+        threshold: 0.5
+    )
+    return SmartTurnConfig(dtype: dtype, encoderConfig: encoder, processorConfig: processor)
+}
+
+private func makeTinySmartTurnModel(dtype: String = "float32") throws -> SmartTurnModel {
+    let model = SmartTurnModel(makeTinySmartTurnConfig(dtype: dtype))
+    eval(model.parameters())
+
+    if dtype == "float16" {
+        let casted = Dictionary(
+            uniqueKeysWithValues: model.parameters().flattened().map { key, value in
+                (key, value.asType(.float16))
+            }
+        )
+        try model.update(parameters: ModuleParameters.unflattened(casted), verify: .noUnusedKeys)
+        eval(model.parameters())
+    }
+
+    return model
+}
+
+struct SmartTurnForwardTests {
+
+    @Test func smartTurnForwardShapeAndRange() throws {
+        let model = try makeTinySmartTurnModel()
+        let input = MLXArray.zeros([1, 8, 64], type: Float.self)
+        let out = model(input)
+        eval(out)
+
+        #expect(out.shape == [1, 1])
+        let minVal = out.min().item(Float.self)
+        let maxVal = out.max().item(Float.self)
+        #expect(minVal >= 0.0)
+        #expect(maxVal <= 1.0)
+    }
+
+    @Test func smartTurnForwardReturnLogits() throws {
+        let model = try makeTinySmartTurnModel()
+        let input = MLXArray.zeros([1, 8, 64], type: Float.self)
+        let logits = model(input, returnLogits: true)
+        eval(logits)
+
+        #expect(logits.shape == [1, 1])
+    }
+
+    @Test func smartTurnForwardBatchDimension() throws {
+        let model = try makeTinySmartTurnModel()
+        let input = MLXArray.zeros([2, 8, 64], type: Float.self)
+        let out = model(input)
+        eval(out)
+
+        #expect(out.shape == [2, 1])
+    }
+
+    @Test func smartTurnDTypePropagation() throws {
+        let fp32Model = try makeTinySmartTurnModel(dtype: "float32")
+        let fp32In = MLXArray.zeros([1, 8, 64], type: Float.self)
+        let fp32Out = fp32Model(fp32In)
+        eval(fp32Out)
+        #expect(fp32Model.modelDType == .float32)
+        #expect(fp32Out.dtype == .float32)
+
+        let fp16Model = try makeTinySmartTurnModel(dtype: "float16")
+        let fp16In = MLXArray.zeros([1, 8, 64], type: Float.self).asType(.float16)
+        let fp16Out = fp16Model(fp16In)
+        eval(fp16Out)
+        #expect(fp16Model.modelDType == .float16)
+        #expect(fp16Out.dtype == .float16)
+    }
+
+    @Test func smartTurnPrepareAudioArrayLengths() throws {
+        let model = try makeTinySmartTurnModel()
+        let maxSamples = model.config.processorConfig.maxAudioSeconds * model.config.processorConfig.samplingRate
+
+        let short = MLXArray.ones([16000], type: Float.self)
+        let shortOut = try model.prepareAudioSamples(short, sampleRate: 16000)
+        #expect(shortOut.count == maxSamples)
+
+        let long = MLXArray.ones([200000], type: Float.self)
+        let longOut = try model.prepareAudioSamples(long, sampleRate: 16000)
+        #expect(longOut.count == maxSamples)
+    }
+
+    @Test func smartTurnPrepareAudioArrayResample() throws {
+        let model = try makeTinySmartTurnModel()
+        let maxSamples = model.config.processorConfig.maxAudioSeconds * model.config.processorConfig.samplingRate
+
+        let audio8k = MLXArray.ones([8000], type: Float.self)
+        let out = try model.prepareAudioSamples(audio8k, sampleRate: 8000)
+        #expect(out.count == maxSamples)
+    }
+
+    @Test func smartTurnPrepareInputFeaturesShape() throws {
+        let model = try makeTinySmartTurnModel()
+        let audio = MLXArray.zeros([16000], type: Float.self)
+        let features = try model.prepareInputFeatures(audio, sampleRate: 16000)
+        eval(features)
+
+        #expect(features.shape == [8, 800])
+    }
+
+    @Test func smartTurnPredictEndpointReturnsOutput() throws {
+        let model = try makeTinySmartTurnModel()
+        let audio = MLXArray.zeros([16000], type: Float.self)
+        let result = try model.predictEndpoint(audio, sampleRate: 16000, threshold: 0.5)
+
+        #expect(result.prediction == 0 || result.prediction == 1)
+        #expect(result.probability >= 0.0 && result.probability <= 1.0)
+    }
+}
+
+// MARK: - Smart Turn Sanitization Tests
+
+struct SmartTurnSanitizeTests {
+
+    @Test func smartTurnSanitizeDropsValConstants() {
+        let sanitized = SmartTurnModel.sanitize([
+            "val_17": MLXArray.zeros([16, 16], type: Float.self),
+            "val_123": MLXArray.zeros([1], type: Float.self)
+        ])
+        #expect(sanitized.isEmpty)
+    }
+
+    @Test func smartTurnSanitizeRemapsPrefixes() {
+        let sanitized = SmartTurnModel.sanitize([
+            "inner.classifier.0.weight": MLXArray.zeros([16, 16], type: Float.self),
+            "inner.pool_attention.2.bias": MLXArray.zeros([1], type: Float.self)
+        ])
+        #expect(sanitized["classifier_0.weight"] != nil)
+        #expect(sanitized["pool_attention_2.bias"] != nil)
+    }
+
+    @Test func smartTurnSanitizeConv1dTranspose() {
+        let weights: [String: MLXArray] = [
+            "encoder.conv1.weight": MLXArray.zeros([16, 8, 3], type: Float.self)
+        ]
+        let sanitized = SmartTurnModel.sanitize(weights)
+        #expect(sanitized["encoder.conv1.weight"]?.shape == [16, 3, 8])
+    }
+
+    @Test func smartTurnSanitizeFCTransposeHeuristics() {
+        let weights: [String: MLXArray] = [
+            "encoder.layers.0.fc1.weight": MLXArray.zeros([16, 32], type: Float.self),
+            "encoder.layers.0.fc2.weight": MLXArray.zeros([32, 16], type: Float.self),
+        ]
+        let sanitized = SmartTurnModel.sanitize(weights)
+        #expect(sanitized["encoder.layers.0.fc1.weight"]?.shape == [32, 16])
+        #expect(sanitized["encoder.layers.0.fc2.weight"]?.shape == [16, 32])
+    }
+
+    @Test func smartTurnSanitizePoolTransposeHeuristics() {
+        let weights: [String: MLXArray] = [
+            "pool_attention.0.weight": MLXArray.zeros([16, 256], type: Float.self),
+            "pool_attention.2.weight": MLXArray.zeros([256, 1], type: Float.self),
+        ]
+        let sanitized = SmartTurnModel.sanitize(weights)
+        #expect(sanitized["pool_attention_0.weight"]?.shape == [256, 16])
+        #expect(sanitized["pool_attention_2.weight"]?.shape == [1, 256])
+    }
+}
+
+// MARK: - Smart Turn Network Tests
+
+struct SmartTurnNetworkTests {
+
+    @Test func smartTurnFromPretrainedEvaluatesConversationalAudio() async throws {
+        let env = ProcessInfo.processInfo.environment
+        guard env["MLXAUDIO_ENABLE_NETWORK_TESTS"] == "1" else {
+            print("Skipping network SmartTurn test. Set MLXAUDIO_ENABLE_NETWORK_TESTS=1 to enable.")
+            return
+        }
+
+        let repo = env["MLXAUDIO_SMARTTURN_REPO"] ?? "mlx-community/smart-turn-v3"
+        let model = try await SmartTurnModel.fromPretrained(repo)
+
+        let audioURLTrue = Bundle.module.url(
+            forResource: "conversational_a",
+            withExtension: "wav",
+            subdirectory: "media"
+        )!
+        let (_, audioTrue) = try loadAudioArray(from: audioURLTrue, sampleRate: 16000)
+        let resultTrue = try model.predictEndpoint(audioTrue, sampleRate: 16000, threshold: 0.5)
+        #expect(resultTrue.prediction == 1)
+        #expect(resultTrue.probability >= 0.5 && resultTrue.probability <= 1.0)
+
+        let audioURLFalse = Bundle.module.url(
+            forResource: "false-turn",
+            withExtension: "wav",
+            subdirectory: "media"
+        )!
+        let (_, audioFalse) = try loadAudioArray(from: audioURLFalse, sampleRate: 16000)
+        let resultFalse = try model.predictEndpoint(audioFalse, sampleRate: 16000, threshold: 0.5)
+        #expect(resultFalse.prediction == 0)
+        #expect(resultFalse.probability >= 0.0 && resultFalse.probability < 0.5)
+    }
+}
diff --git a/Tests/media/false-turn.wav b/Tests/media/false-turn.wav
new file mode 100644
index 0000000..4467074
Binary files /dev/null and b/Tests/media/false-turn.wav differ

commit 3d858d9df37f2be0424ea48af9316bc2655b5c1b
Author: Lucas Newman <lucas@future.fit>
Date:   Sat Feb 21 09:29:26 2026 -0800

    Add streaming support to the built-in audio player. (#63)
    
    Co-authored-by: Prince Canuma <prince.gdt@gmail.com>

diff --git a/Examples/SimpleChat/SimpleChat/AudioEngine.swift b/Examples/SimpleChat/SimpleChat/AudioEngine.swift
index 7392cc1..056fd8c 100644
--- a/Examples/SimpleChat/SimpleChat/AudioEngine.swift
+++ b/Examples/SimpleChat/SimpleChat/AudioEngine.swift
@@ -1,11 +1,13 @@
 @preconcurrency import AVFoundation
-import os
+import MLXAudioCore
 
+@MainActor
 protocol AudioEngineDelegate: AnyObject {
     func audioCaptureEngine(_ engine: AudioEngine, didReceive buffer: AVAudioPCMBuffer)
     func audioCaptureEngine(_ engine: AudioEngine, isSpeakingDidChange speaking: Bool)
 }
 
+@MainActor
 final class AudioEngine {
     weak var delegate: AudioEngineDelegate?
 
@@ -24,16 +26,8 @@ final class AudioEngine {
     private var firstBufferQueued = false
     private var queuedBuffers = 0
     private var streamFinished = false
-    private var pendingData = PendingDataBuffer()
 
     private let inputBufferSize: AVAudioFrameCount
-    private lazy var streamingInputFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: 24_000, channels: 1, interleaved: false)!
-
-    private lazy var requiredBytesForChunk: Int = {
-        let f = streamingInputFormat
-        let bytesPerFrame = Int(f.streamDescription.pointee.mBytesPerFrame)
-        return bytesPerFrame * Int(f.sampleRate) * 1
-    }()
 
     init(inputBufferSize: AVAudioFrameCount) {
         self.inputBufferSize = inputBufferSize
@@ -54,14 +48,18 @@ final class AudioEngine {
         }
 
         let input = engine.inputNode
-//        try input.setVoiceProcessingEnabled(true)
+#if os(iOS)
+       try input.setVoiceProcessingEnabled(true)
+#endif
 
         let output = engine.outputNode
-//        try output.setVoiceProcessingEnabled(true)
+#if os(iOS)
+       try output.setVoiceProcessingEnabled(true)
+#endif
 
         engine.connect(streamingPlayer, to: output, format: nil)
 
-        let tapHandler: @Sendable (AVAudioPCMBuffer, AVAudioTime) -> Void = { [weak self] buf, _ in
+        let tapHandler: (AVAudioPCMBuffer, AVAudioTime) -> Void = { [weak self] buf, _ in
             Task { @MainActor [weak self] in
                 self?.processInputBuffer(buf)
             }
@@ -82,13 +80,13 @@ final class AudioEngine {
         if engine.isRunning { engine.stop() }
     }
 
-    func speak(samplesStream: AsyncThrowingStream<[Float], any Error>) {
+    func speak(buffersStream: AsyncThrowingStream<AVAudioPCMBuffer, any Error>) {
         resetStreamingState()
 
         currentSpeakingTask = Task { [weak self] in
             guard let self else { return }
             do {
-                try await stream(samplesStream: samplesStream)
+                try await stream(buffersStream: buffersStream)
             } catch is CancellationError {
                 // no-op
             } catch {
@@ -97,16 +95,6 @@ final class AudioEngine {
         }
     }
 
-    func speak(samples: [Float]) {
-        let stream = AsyncThrowingStream<[Float], any Error> { continuation in
-            if !samples.isEmpty {
-                continuation.yield(samples)
-            }
-            continuation.finish()
-        }
-        speak(samplesStream: stream)
-    }
-
     func endSpeaking() {
         resetStreamingState()
     }
@@ -128,7 +116,6 @@ final class AudioEngine {
         currentSpeakingTask?.cancel()
         currentSpeakingTask = nil
 
-        Task { await pendingData.reset() }
         firstBufferQueued = false
         queuedBuffers = 0
         streamFinished = false
@@ -136,109 +123,22 @@ final class AudioEngine {
         print("Resetting streaming state...")
     }
 
-    private func stream(samplesStream: AsyncThrowingStream<[Float], any Error>) async throws {
-        let inputFormat = streamingInputFormat
-        let outputFormat = engine.outputNode.inputFormat(forBus: 0)
+    private func stream(buffersStream: AsyncThrowingStream<AVAudioPCMBuffer, any Error>) async throws {
+        let converter = PCMStreamConverter(outputFormat: engine.outputNode.inputFormat(forBus: 0))
 
-        guard let converter = AVAudioConverter(from: inputFormat, to: outputFormat) else {
-            throw NSError(domain: "AudioEngine", code: -1, userInfo: [NSLocalizedDescriptionKey: "Unable to create converter"])
-        }
-
-        @inline(__always)
-        func dataFromFloats(_ floats: [Float]) -> Data {
-            guard !floats.isEmpty else { return Data() }
-            return floats.withUnsafeBufferPointer { Data(buffer: $0) }
-        }
-
-        for try await batch in samplesStream {
-            if !batch.isEmpty {
-                await pendingData.append(dataFromFloats(batch))
-            }
-            while let chunk = await pendingData.extractChunk(ofSize: requiredBytesForChunk) {
-                try convertAndQueue(chunk: chunk, inputFormat: inputFormat, converter: converter)
+        for try await buffer in buffersStream {
+            let convertedBuffers = try converter.push(buffer)
+            for convertedBuffer in convertedBuffers {
+                enqueue(convertedBuffer)
             }
         }
 
-        let leftover = await pendingData.flushRemaining()
-        if !leftover.isEmpty {
-            try convertAndQueue(chunk: leftover, inputFormat: inputFormat, converter: converter)
+        let trailingBuffers = try converter.finish()
+        for trailingBuffer in trailingBuffers {
+            enqueue(trailingBuffer)
         }
-        try flushConverter(converter, inputFormat: inputFormat)
-        streamFinished = true
-    }
-
-    private func convertAndQueue(chunk: Data, inputFormat: AVAudioFormat, converter: AVAudioConverter) throws {
-        var remaining = chunk
-        while let buf = try convertOnce(&remaining, inputFormat: inputFormat, converter: converter, endOfStream: false) {
-            enqueue(buf)
-            if remaining.isEmpty { break }
-        }
-    }
 
-    private func flushConverter(_ converter: AVAudioConverter, inputFormat: AVAudioFormat) throws {
-        var dummy = Data()
-        while let buf = try convertOnce(&dummy, inputFormat: inputFormat, converter: converter, endOfStream: true) {
-            enqueue(buf)
-        }
-    }
-
-    private func convertOnce(
-        _ pending: inout Data,
-        inputFormat: AVAudioFormat,
-        converter: AVAudioConverter,
-        endOfStream: Bool
-    ) throws -> AVAudioPCMBuffer? {
-        let bytesPerFrame = Int(inputFormat.streamDescription.pointee.mBytesPerFrame)
-        let framesInPending = pending.count / bytesPerFrame
-
-        if framesInPending == 0, !endOfStream { return nil }
-
-        let srcBuffer: AVAudioPCMBuffer? = {
-            guard framesInPending > 0 else { return nil }
-            let b = AVAudioPCMBuffer(pcmFormat: inputFormat, frameCapacity: AVAudioFrameCount(framesInPending))!
-            b.frameLength = AVAudioFrameCount(framesInPending)
-            _ = pending.withUnsafeBytes { raw in
-                memcpy(b.floatChannelData![0], raw.baseAddress!, framesInPending * bytesPerFrame)
-            }
-            return b
-        }()
-
-        let ratio = converter.outputFormat.sampleRate / converter.inputFormat.sampleRate
-        let dstCapacity = AVAudioFrameCount(Double(framesInPending) * ratio) + 512
-        let dstBuffer = AVAudioPCMBuffer(pcmFormat: converter.outputFormat, frameCapacity: max(dstCapacity, 512))!
-
-        var error: NSError?
-        let didConsumeInput = OSAllocatedUnfairLock(initialState: false)
-
-        _ = converter.convert(to: dstBuffer, error: &error) { _, outStatus in
-            if let src = srcBuffer {
-                let shouldProvideInput = didConsumeInput.withLock { consumed in
-                    if consumed {
-                        return false
-                    }
-                    consumed = true
-                    return true
-                }
-                if shouldProvideInput {
-                    outStatus.pointee = .haveData
-                    return src
-                }
-            }
-            if endOfStream {
-                outStatus.pointee = .endOfStream
-                return nil
-            } else {
-                outStatus.pointee = .noDataNow
-                return nil
-            }
-        }
-
-        if let error { throw error }
-        if didConsumeInput.withLock({ $0 }) {
-            pending.removeAll()
-        }
-
-        return dstBuffer.frameLength > 0 ? dstBuffer : nil
+        streamFinished = true
     }
 
     private func enqueue(_ buffer: AVAudioPCMBuffer) {
@@ -275,25 +175,3 @@ final class AudioEngine {
         delegate?.audioCaptureEngine(self, didReceive: buffer)
     }
 }
-
-// MARK: -
-
-private actor PendingDataBuffer {
-    private var data = Data()
-
-    func append(_ chunk: Data) { data.append(chunk) }
-
-    func extractChunk(ofSize size: Int) -> Data? {
-        guard data.count >= size else { return nil }
-        let chunk = data.prefix(size)
-        data.removeFirst(size)
-        return Data(chunk)
-    }
-
-    func flushRemaining() -> Data {
-        defer { data.removeAll() }
-        return data
-    }
-
-    func reset() { data.removeAll(keepingCapacity: true) }
-}
diff --git a/Examples/SimpleChat/SimpleChat/ContentView.swift b/Examples/SimpleChat/SimpleChat/ContentView.swift
index 92e3d44..917177f 100644
--- a/Examples/SimpleChat/SimpleChat/ContentView.swift
+++ b/Examples/SimpleChat/SimpleChat/ContentView.swift
@@ -1,12 +1,16 @@
 import AVFoundation
 import FoundationModels
 import SwiftUI
+#if canImport(UIKit)
+import UIKit
+#endif
 
 @Observable
+@MainActor
 class ContentViewModel {
     var speechController = SpeechController()
     
-    private static let instructions = "You are a helpful voice assistant that answers the user's questions with very consise and natural full sentences, as it will be TTS-rendered downstream as speech. You typically answer in three sentences or less. You NEVER use lists, emojis, markdown, or other non-essential embellishments."
+    private static let instructions = "You are a helpful voice assistant that answers the user's questions with very consise and natural full sentences, as it will be TTS-rendered downstream as speech. You typically answer in three sentences or less. IMPORTANT: Never use lists, emojis, markdown, or other non-essential embellishments."
     
     @ObservationIgnored
     private var session: LanguageModelSession?
@@ -22,17 +26,17 @@ class ContentViewModel {
         
         try await speechController.start()
         
-        Task { @MainActor in
-            UIApplication.shared.isIdleTimerDisabled = true
-        }
+#if canImport(UIKit)
+        UIApplication.shared.isIdleTimerDisabled = true
+#endif
     }
     
     func stopConversation() async throws {
         try await speechController.stop()
         
-        Task { @MainActor in
-            UIApplication.shared.isIdleTimerDisabled = false
-        }
+#if canImport(UIKit)
+        UIApplication.shared.isIdleTimerDisabled = false
+#endif
         
         print("Stopped conversation.")
     }
@@ -40,10 +44,10 @@ class ContentViewModel {
 
 extension ContentViewModel: SpeechControllerDelegate {
     func speechController(_ controller: SpeechController, didFinish transcription: String) {
-        Task {
-            guard !controller.isSpeaking else { return }
+        Task { @MainActor in
+            guard !controller.isSpeaking && transcription.count > 1 else { return }
             
-            print("Got transcription: \(transcription)")
+            print("Got transcription: '\(transcription)'")
             let response = try await self.session?.respond(to: transcription)
             print("Got response: \(response?.content ?? "<empty>")")
             try await self.speechController.speak(text: response?.content ?? "I'm sorry, I didn't get that.")
@@ -101,6 +105,7 @@ struct ContentView: View {
                     .clipShape(Circle())
                     .contentShape(Circle())
             }
+            .buttonStyle(.plain)
             .padding(64)
         }
         .scaleEffect(CGSize(width: isActive ? 1.0 : 0.7, height: isActive ? 1.0 : 0.7))
diff --git a/Examples/SimpleChat/SimpleChat/SimpleVAD.swift b/Examples/SimpleChat/SimpleChat/SimpleVAD.swift
index 4964203..2b7077f 100644
--- a/Examples/SimpleChat/SimpleChat/SimpleVAD.swift
+++ b/Examples/SimpleChat/SimpleChat/SimpleVAD.swift
@@ -2,6 +2,7 @@
 import os
 import Speech
 
+@MainActor
 protocol SimpleVADDelegate: AnyObject {
     func didStartSpeaking()
     func didStopSpeaking(transcription: String?)
diff --git a/Examples/SimpleChat/SimpleChat/SpeechController.swift b/Examples/SimpleChat/SimpleChat/SpeechController.swift
index 3f775bd..0681278 100644
--- a/Examples/SimpleChat/SimpleChat/SpeechController.swift
+++ b/Examples/SimpleChat/SimpleChat/SpeechController.swift
@@ -4,11 +4,13 @@ import MLXAudioCore
 import MLXAudioTTS
 import MLXLMCommon
 
+@MainActor
 protocol SpeechControllerDelegate: AnyObject {
     func speechController(_ controller: SpeechController, didFinish transcription: String)
 }
 
 @Observable
+@MainActor
 final class SpeechController {
     @ObservationIgnored
     weak var delegate: SpeechControllerDelegate?
@@ -37,7 +39,7 @@ final class SpeechController {
         audioEngine.delegate = self
         vad.delegate = self
 
-        Task {
+        Task { @MainActor in
             do {
                 print("Loading TTS model: \(ttsRepoId)")
                 self.model = try await TTS.loadModel(modelRepo: ttsRepoId)
@@ -50,11 +52,13 @@ final class SpeechController {
     }
 
     func start() async throws {
+#if os(iOS)
         let session = AVAudioSession.sharedInstance()
         try session.setActive(false)
         try session.setCategory(.playAndRecord, mode: .voiceChat, policy: .default, options: [.defaultToSpeaker])
         try session.setPreferredIOBufferDuration(0.02)
         try session.setActive(true)
+#endif
 
         try await ensureEngineStarted()
         isActive = true
@@ -65,7 +69,9 @@ final class SpeechController {
         audioEngine.stop()
         isDetectingSpeech = false
         vad.reset()
+#if os(iOS)
         try AVAudioSession.sharedInstance().setActive(false)
+#endif
         isActive = false
     }
 
@@ -90,7 +96,7 @@ final class SpeechController {
             return
         }
 
-        let audioStream = model.generateStream(
+        let audioStream = model.generatePCMBufferStream(
             text: text,
             voice: "cosette",
             refAudio: nil,
@@ -99,12 +105,7 @@ final class SpeechController {
         )
         try await ensureEngineStarted()
 
-        audioEngine.speak(samplesStream: proxyAudioStream(audioStream, extract: {
-            switch $0 {
-            case .audio(let samples): samples.asArray(Float.self)
-            default: fatalError("Unsupported sample type.")
-            }
-        }))
+        audioEngine.speak(buffersStream: audioStream)
     }
 
     private func ensureEngineStarted() async throws {
@@ -117,24 +118,6 @@ final class SpeechController {
         audioEngine.isMicrophoneMuted = false
         print("Started audio engine.")
     }
-
-    private func proxyAudioStream<T>(_ upstream: AsyncThrowingStream<T, any Error>, extract: @escaping (T) -> [Float]) -> AsyncThrowingStream<[Float], any Error> {
-        AsyncThrowingStream<[Float], any Error> { continuation in
-            let task = Task {
-                do {
-                    for try await value in upstream {
-                        continuation.yield(extract(value))
-                    }
-                    continuation.finish()
-                } catch is CancellationError {
-                    continuation.finish(throwing: CancellationError())
-                } catch {
-                    continuation.finish(throwing: error)
-                }
-            }
-            continuation.onTermination = { @Sendable _ in task.cancel() }
-        }
-    }
 }
 
 // MARK: - AudioEngineDelegate
@@ -143,9 +126,7 @@ extension SpeechController: AudioEngineDelegate {
     func audioCaptureEngine(_ engine: AudioEngine, didReceive buffer: AVAudioPCMBuffer) {
         guard !audioEngine.isSpeaking else { return }
 
-        Task {
-            vad.process(buffer: buffer)
-        }
+        vad.process(buffer: buffer)
     }
 
     func audioCaptureEngine(_ engine: AudioEngine, isSpeakingDidChange speaking: Bool) {
diff --git a/Sources/MLXAudioCore/AudioPlayerManager.swift b/Sources/MLXAudioCore/AudioPlayer.swift
similarity index 50%
rename from Sources/MLXAudioCore/AudioPlayerManager.swift
rename to Sources/MLXAudioCore/AudioPlayer.swift
index 05bf172..7754da0 100644
--- a/Sources/MLXAudioCore/AudioPlayerManager.swift
+++ b/Sources/MLXAudioCore/AudioPlayer.swift
@@ -1,21 +1,20 @@
-//
-//  AudioPlayerManager.swift
-//  MLXAudio
-//
-//  Created by Rudrank Riyam on 6/11/25.
-//
-
 import Foundation
 import AVFoundation
 import Combine
 
 @MainActor
-public class AudioPlayerManager: NSObject, ObservableObject {
+public class AudioPlayer: NSObject, ObservableObject {
     // Published properties for UI binding
-    @Published public var isPlaying: Bool = false
-    @Published public var currentTime: TimeInterval = 0
-    @Published public var duration: TimeInterval = 0
-    @Published public var currentAudioURL: URL?
+    @Published public private(set) var isPlaying: Bool = false
+    @Published public private(set) var isSpeaking: Bool = false
+    @Published public private(set) var currentTime: TimeInterval = 0
+    @Published public private(set) var duration: TimeInterval = 0
+    @Published public private(set) var currentAudioURL: URL?
+
+    /// Optional callback for speaking-state transitions during playback/streaming.
+    public var onSpeakingStateChanged: ((Bool) -> Void)?
+    /// Optional callback fired when a streaming source has finished and all queued audio is consumed.
+    public var onDidFinishStreaming: (() -> Void)?
 
     private var player: AVAudioPlayer?
     private var timer: Timer?
@@ -25,7 +24,11 @@ public class AudioPlayerManager: NSObject, ObservableObject {
     private var playerNode: AVAudioPlayerNode?
     private var streamingFormat: AVAudioFormat?
     private var isStreaming: Bool = false
-    private var scheduledSamples: Int = 0
+    private var scheduledFrames: Int = 0
+    private var queuedBuffers: Int = 0
+    private var streamFinished: Bool = false
+    private var streamingTask: Task<Void, Never>?
+    private var configurationChangeObserver: Task<Void, Never>?
 
     public override init() {
         super.init()
@@ -70,6 +73,7 @@ public class AudioPlayerManager: NSObject, ObservableObject {
 
         player.play()
         isPlaying = true
+        setSpeaking(true)
         startTimer()
     }
 
@@ -80,6 +84,7 @@ public class AudioPlayerManager: NSObject, ObservableObject {
             player?.pause()
         }
         isPlaying = false
+        setSpeaking(false)
         stopTimer()
     }
 
@@ -90,6 +95,7 @@ public class AudioPlayerManager: NSObject, ObservableObject {
             if isStreaming {
                 playerNode?.play()
                 isPlaying = true
+                setSpeaking(queuedBuffers > 0)
                 startTimer()
             } else {
                 play()
@@ -102,10 +108,11 @@ public class AudioPlayerManager: NSObject, ObservableObject {
             stopStreaming()
         } else {
             player?.stop()
+            isPlaying = false
+            setSpeaking(false)
+            stopTimer()
+            currentTime = 0
         }
-        isPlaying = false
-        stopTimer()
-        currentTime = 0
     }
 
     public func seek(to time: TimeInterval) {
@@ -125,38 +132,79 @@ public class AudioPlayerManager: NSObject, ObservableObject {
         AudioSessionManager.shared.setupAudioSession()
         #endif
 
-        audioEngine = AVAudioEngine()
-        playerNode = AVAudioPlayerNode()
+        streamingFormat = AVAudioFormat(standardFormatWithSampleRate: sampleRate, channels: 1)
+        scheduledFrames = 0
+        queuedBuffers = 0
+        streamFinished = false
+        duration = 0
 
-        guard let engine = audioEngine, let node = playerNode else { return }
+        do {
+            try startStreamingEngine(connectionFormat: streamingFormat)
+        } catch {
+            print("Failed to start audio engine: \(error)")
+        }
+    }
 
-        streamingFormat = AVAudioFormat(standardFormatWithSampleRate: sampleRate, channels: 1)
-        scheduledSamples = 0
+    /// Play a stream of PCM buffers, converting source format to the active output format.
+    public func play(stream: AsyncThrowingStream<AVAudioPCMBuffer, Error>) {
+        stop()
 
-        engine.attach(node)
-        engine.connect(node, to: engine.mainMixerNode, format: streamingFormat)
+        #if os(iOS)
+        AudioSessionManager.shared.setupAudioSession()
+        #endif
 
         do {
-            try engine.start()
-            node.play()
-            isStreaming = true
-            isPlaying = true
-            startStreamingTimer()
+            try startStreamingEngine(connectionFormat: nil)
         } catch {
             print("Failed to start audio engine: \(error)")
+            return
         }
+
+        guard let engine = audioEngine else { return }
+        let converter = PCMStreamConverter(outputFormat: engine.outputNode.inputFormat(forBus: 0))
+
+        streamingTask = Task { @MainActor [weak self] in
+            guard let self else { return }
+
+            do {
+                for try await inputBuffer in stream {
+                    let convertedBuffers = try converter.push(inputBuffer)
+                    for buffer in convertedBuffers {
+                        scheduleStreamingBuffer(buffer)
+                    }
+                }
+
+                let trailingBuffers = try converter.finish()
+                for trailingBuffer in trailingBuffers {
+                    scheduleStreamingBuffer(trailingBuffer)
+                }
+
+                streamFinished = true
+                finishStreamIfDrained()
+            } catch is CancellationError {
+                // no-op
+            } catch {
+                print("Streaming playback failed: \(error)")
+                stopStreaming()
+            }
+        }
+    }
+
+    /// Mark an open chunk stream as complete so completion can fire once queued audio drains.
+    public func finishStreamingInput() {
+        streamFinished = true
+        finishStreamIfDrained()
     }
 
     /// Schedule audio samples for streaming playback
     public func scheduleAudioChunk(_ samples: [Float], withCrossfade: Bool = true) {
         guard isStreaming,
-              let node = playerNode,
               let format = streamingFormat else { return }
 
         var processedSamples = samples
 
         // Apply fade-in to first chunk, crossfade to subsequent chunks
-        if scheduledSamples == 0 {
+        if scheduledFrames == 0 {
             // Fade in the first chunk (10ms)
             let fadeInSamples = min(Int(format.sampleRate * 0.01), samples.count)
             for i in 0..<fadeInSamples {
@@ -184,20 +232,34 @@ public class AudioPlayerManager: NSObject, ObservableObject {
             }
         }
 
-        node.scheduleBuffer(buffer)
-        scheduledSamples += samples.count
-        duration = Double(scheduledSamples) / format.sampleRate
+        scheduleStreamingBuffer(buffer)
     }
 
     /// Stop streaming and clean up
     public func stopStreaming() {
+        streamingTask?.cancel()
+        streamingTask = nil
+
+        configurationChangeObserver?.cancel()
+        configurationChangeObserver = nil
+
         playerNode?.stop()
         audioEngine?.stop()
         audioEngine = nil
         playerNode = nil
         streamingFormat = nil
+
         isStreaming = false
-        scheduledSamples = 0
+        isPlaying = false
+        setSpeaking(false)
+
+        scheduledFrames = 0
+        queuedBuffers = 0
+        streamFinished = false
+
+        currentTime = 0
+        duration = 0
+        stopTimer()
     }
 
     /// Check if currently in streaming mode
@@ -236,13 +298,101 @@ public class AudioPlayerManager: NSObject, ObservableObject {
         timer?.invalidate()
         timer = nil
     }
+
+    private func startStreamingEngine(connectionFormat: AVAudioFormat?) throws {
+        audioEngine = AVAudioEngine()
+        playerNode = AVAudioPlayerNode()
+
+        guard let engine = audioEngine, let node = playerNode else { return }
+
+        engine.attach(node)
+        engine.connect(node, to: engine.mainMixerNode, format: connectionFormat)
+        engine.prepare()
+        try engine.start()
+        node.play()
+
+        isStreaming = true
+        isPlaying = true
+        setSpeaking(false)
+        scheduledFrames = 0
+        queuedBuffers = 0
+        streamFinished = false
+        currentTime = 0
+        duration = 0
+
+        startStreamingTimer()
+        observeEngineConfigurationChanges()
+    }
+
+    private func observeEngineConfigurationChanges() {
+        guard configurationChangeObserver == nil else { return }
+        configurationChangeObserver = Task { @MainActor [weak self] in
+            guard let self else { return }
+
+            for await _ in NotificationCenter.default.notifications(named: .AVAudioEngineConfigurationChange) {
+                guard isStreaming, let engine = audioEngine else { continue }
+                if !engine.isRunning {
+                    do {
+                        try engine.start()
+                        if isPlaying {
+                            playerNode?.play()
+                        }
+                    } catch {
+                        print("Failed to restart audio engine after configuration change: \(error)")
+                    }
+                }
+            }
+        }
+    }
+
+    private func scheduleStreamingBuffer(_ buffer: AVAudioPCMBuffer) {
+        guard let node = playerNode else { return }
+
+        queuedBuffers += 1
+        scheduledFrames += Int(buffer.frameLength)
+        duration = Double(scheduledFrames) / buffer.format.sampleRate
+
+        let completion: @Sendable (AVAudioPlayerNodeCompletionCallbackType) -> Void = { [weak self] _ in
+            Task { @MainActor [weak self] in
+                self?.handleScheduledBufferConsumed()
+            }
+        }
+        node.scheduleBuffer(buffer, completionCallbackType: .dataConsumed, completionHandler: completion)
+
+        if !isSpeaking {
+            setSpeaking(true)
+        }
+    }
+
+    private func handleScheduledBufferConsumed() {
+        queuedBuffers = max(queuedBuffers - 1, 0)
+        finishStreamIfDrained()
+    }
+
+    private func finishStreamIfDrained() {
+        guard streamFinished, queuedBuffers == 0 else { return }
+        isPlaying = false
+        setSpeaking(false)
+        stopTimer()
+        onDidFinishStreaming?()
+    }
+
+    private func setSpeaking(_ speaking: Bool) {
+        guard isSpeaking != speaking else { return }
+        isSpeaking = speaking
+        onSpeakingStateChanged?(speaking)
+    }
 }
 
 // MARK: - AVAudioPlayerDelegate
 
-extension AudioPlayerManager: @MainActor AVAudioPlayerDelegate {
+@available(*, deprecated, renamed: "AudioPlayer", message: "Use AudioPlayer instead.")
+public typealias AudioPlayerManager = AudioPlayer
+
+extension AudioPlayer: @MainActor AVAudioPlayerDelegate {
     public func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
         isPlaying = false
+        setSpeaking(false)
         stopTimer()
         currentTime = 0
     }
@@ -250,7 +400,7 @@ extension AudioPlayerManager: @MainActor AVAudioPlayerDelegate {
     public func audioPlayerDecodeErrorDidOccur(_ player: AVAudioPlayer, error: Error?) {
         print("Audio decode error: \(error?.localizedDescription ?? "unknown")")
         isPlaying = false
+        setSpeaking(false)
         stopTimer()
     }
 }
-
diff --git a/Sources/MLXAudioCore/PCMStreamConverter.swift b/Sources/MLXAudioCore/PCMStreamConverter.swift
new file mode 100644
index 0000000..303c28c
--- /dev/null
+++ b/Sources/MLXAudioCore/PCMStreamConverter.swift
@@ -0,0 +1,154 @@
+@preconcurrency import AVFoundation
+import os
+
+public enum PCMStreamConverterError: Error, LocalizedError {
+    case converterCreationFailed
+    case conversionFailed
+    case flushFailed
+    case outputBufferCreationFailed
+
+    public var errorDescription: String? {
+        switch self {
+        case .converterCreationFailed:
+            return "Unable to create audio converter."
+        case .conversionFailed:
+            return "Audio conversion failed."
+        case .flushFailed:
+            return "Audio converter flush failed."
+        case .outputBufferCreationFailed:
+            return "Unable to create output audio buffer."
+        }
+    }
+}
+
+public final class PCMStreamConverter {
+    private let outputFormat: AVAudioFormat
+    private var converter: AVAudioConverter?
+    private var converterInputFormat: AVAudioFormat?
+
+    public init(outputFormat: AVAudioFormat) {
+        self.outputFormat = outputFormat
+    }
+
+    public func push(_ inputBuffer: AVAudioPCMBuffer) throws -> [AVAudioPCMBuffer] {
+        guard inputBuffer.frameLength > 0 else { return [] }
+
+        var outputBuffers: [AVAudioPCMBuffer] = []
+        if converter == nil || !isEquivalentAudioFormat(converterInputFormat, inputBuffer.format) {
+            if let converter {
+                outputBuffers.append(contentsOf: try flush(converter))
+            }
+            converter = try makeConverter(from: inputBuffer.format)
+            converterInputFormat = inputBuffer.format
+        }
+
+        if let converter {
+            outputBuffers.append(contentsOf: try convert(inputBuffer, using: converter))
+        }
+
+        return outputBuffers
+    }
+
+    public func finish() throws -> [AVAudioPCMBuffer] {
+        guard let converter else { return [] }
+        return try flush(converter)
+    }
+}
+
+private extension PCMStreamConverter {
+    func makeConverter(from inputFormat: AVAudioFormat) throws -> AVAudioConverter {
+        guard let converter = AVAudioConverter(from: inputFormat, to: outputFormat) else {
+            throw PCMStreamConverterError.converterCreationFailed
+        }
+        return converter
+    }
+
+    func convert(_ inputBuffer: AVAudioPCMBuffer, using converter: AVAudioConverter) throws -> [AVAudioPCMBuffer] {
+        let ratio = converter.outputFormat.sampleRate / converter.inputFormat.sampleRate
+        let outputCapacity = AVAudioFrameCount(Double(inputBuffer.frameLength) * ratio) + 512
+        let didProvideInput = OSAllocatedUnfairLock(initialState: false)
+        var outputBuffers: [AVAudioPCMBuffer] = []
+
+        while true {
+            let dstBuffer = try makeOutputBuffer(converter: converter, frameCapacity: max(outputCapacity, 512))
+            var error: NSError?
+            let status = converter.convert(to: dstBuffer, error: &error) { _, outStatus in
+                let shouldProvideInput = didProvideInput.withLock { didProvide in
+                    if didProvide {
+                        return false
+                    }
+                    didProvide = true
+                    return true
+                }
+
+                if shouldProvideInput {
+                    outStatus.pointee = .haveData
+                    return inputBuffer
+                } else {
+                    outStatus.pointee = .noDataNow
+                    return nil
+                }
+            }
+
+            if let error { throw error }
+            if dstBuffer.frameLength > 0 {
+                outputBuffers.append(dstBuffer)
+            }
+
+            switch status {
+            case .haveData:
+                continue
+            case .inputRanDry, .endOfStream:
+                return outputBuffers
+            case .error:
+                throw PCMStreamConverterError.conversionFailed
+            @unknown default:
+                return outputBuffers
+            }
+        }
+    }
+
+    func flush(_ converter: AVAudioConverter) throws -> [AVAudioPCMBuffer] {
+        var outputBuffers: [AVAudioPCMBuffer] = []
+
+        while true {
+            let dstBuffer = try makeOutputBuffer(converter: converter, frameCapacity: 4096)
+            var error: NSError?
+            let status = converter.convert(to: dstBuffer, error: &error) { _, outStatus in
+                outStatus.pointee = .endOfStream
+                return nil
+            }
+
+            if let error { throw error }
+            if dstBuffer.frameLength > 0 {
+                outputBuffers.append(dstBuffer)
+            }
+
+            switch status {
+            case .haveData:
+                continue
+            case .inputRanDry, .endOfStream:
+                return outputBuffers
+            case .error:
+                throw PCMStreamConverterError.flushFailed
+            @unknown default:
+                return outputBuffers
+            }
+        }
+    }
+
+    func makeOutputBuffer(converter: AVAudioConverter, frameCapacity: AVAudioFrameCount) throws -> AVAudioPCMBuffer {
+        guard let buffer = AVAudioPCMBuffer(pcmFormat: converter.outputFormat, frameCapacity: frameCapacity) else {
+            throw PCMStreamConverterError.outputBufferCreationFailed
+        }
+        return buffer
+    }
+
+    func isEquivalentAudioFormat(_ lhs: AVAudioFormat?, _ rhs: AVAudioFormat) -> Bool {
+        guard let lhs else { return false }
+        return lhs.commonFormat == rhs.commonFormat &&
+            lhs.sampleRate == rhs.sampleRate &&
+            lhs.channelCount == rhs.channelCount &&
+            lhs.isInterleaved == rhs.isInterleaved
+    }
+}
diff --git a/Sources/MLXAudioTTS/Generation.swift b/Sources/MLXAudioTTS/Generation.swift
index 59e8dd4..d70a4b6 100644
--- a/Sources/MLXAudioTTS/Generation.swift
+++ b/Sources/MLXAudioTTS/Generation.swift
@@ -1,6 +1,9 @@
 @preconcurrency import MLX
 import MLXAudioCore
 @preconcurrency import MLXLMCommon
+#if canImport(AVFoundation)
+@preconcurrency import AVFoundation
+#endif
 
 public protocol SpeechGenerationModel: AnyObject {
     var sampleRate: Int { get }
@@ -36,4 +39,113 @@ public extension SpeechGenerationModel {
     ) async throws -> MLXArray {
         try await generate(text: text, voice: voice, refAudio: refAudio, refText: refText, language: language, generationParameters: generationParameters ?? defaultGenerationParameters)
     }
+
+    func generateSamplesStream(
+        text: String,
+        voice: String?,
+        refAudio: MLXArray?,
+        refText: String?,
+        language: String?,
+        generationParameters: GenerateParameters? = nil
+    ) -> AsyncThrowingStream<[Float], Error> {
+        let stream = generateStream(
+            text: text,
+            voice: voice,
+            refAudio: refAudio,
+            refText: refText,
+            language: language,
+            generationParameters: generationParameters ?? defaultGenerationParameters
+        )
+        return proxyAudioStream(stream, extract: {
+            guard case .audio(let samples) = $0 else { return nil }
+            return samples.asArray(Float.self)
+        })
+    }
+
+#if canImport(AVFoundation)
+    @MainActor
+    func generatePCMBufferStream(
+        text: String,
+        voice: String?,
+        refAudio: MLXArray?,
+        refText: String?,
+        language: String?,
+        generationParameters: GenerateParameters? = nil
+    ) -> AsyncThrowingStream<AVAudioPCMBuffer, Error> {
+        let sampleStream = generateSamplesStream(
+            text: text,
+            voice: voice,
+            refAudio: refAudio,
+            refText: refText,
+            language: language,
+            generationParameters: generationParameters
+        )
+
+        let (stream, continuation) = AsyncThrowingStream<AVAudioPCMBuffer, Error>.makeStream()
+        let sampleRate = self.sampleRate
+
+        Task { @MainActor in
+            do {
+                for try await samples in sampleStream {
+                    let buffer = try makePCMBuffer(samples: samples, sampleRate: sampleRate)
+                    continuation.yield(buffer)
+                }
+                continuation.finish()
+            } catch is CancellationError {
+                continuation.finish(throwing: CancellationError())
+            } catch {
+                continuation.finish(throwing: error)
+            }
+        }
+
+        return stream
+    }
+#endif
+}
+
+private func proxyAudioStream<T: Sendable, U: Sendable>(
+    _ upstream: AsyncThrowingStream<T, Error>,
+    extract: @Sendable @escaping (T) -> U?
+) -> AsyncThrowingStream<U, Error> {
+    AsyncThrowingStream<U, Error> { continuation in
+        let task = Task { @Sendable in
+            do {
+                for try await value in upstream {
+                    guard let extracted = extract(value) else { continue }
+                    continuation.yield(extracted)
+                }
+                continuation.finish()
+            } catch is CancellationError {
+                continuation.finish(throwing: CancellationError())
+            } catch {
+                continuation.finish(throwing: error)
+            }
+        }
+        continuation.onTermination = { @Sendable _ in task.cancel() }
+    }
+}
+
+#if canImport(AVFoundation)
+@MainActor
+private func makePCMBuffer(samples: [Float], sampleRate: Int) throws -> AVAudioPCMBuffer {
+    let frameCount = AVAudioFrameCount(samples.count)
+    guard
+        let format = AVAudioFormat(
+            commonFormat: .pcmFormatFloat32,
+            sampleRate: Double(sampleRate),
+            channels: 1,
+            interleaved: false
+        ),
+        let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount),
+        let channel = buffer.floatChannelData?[0]
+    else {
+        throw AudioGenerationError.audioDecodingFailed("Failed to create AVAudioPCMBuffer")
+    }
+
+    buffer.frameLength = frameCount
+    for i in 0 ..< samples.count {
+        channel[i] = samples[i]
+    }
+    return buffer
 }
+#endif

commit 67e63166d9a44e163375732aa5f685f2b367cac6
Author: Lucas Newman <lucas@future.fit>
Date:   Sat Feb 21 09:19:54 2026 -0800

    Remove dead code and fix all warnings. (#62)

diff --git a/Sources/MLXAudioCore/ConvWeighted.swift b/Sources/MLXAudioCore/ConvWeighted.swift
deleted file mode 100644
index 5d11ff1..0000000
--- a/Sources/MLXAudioCore/ConvWeighted.swift
+++ /dev/null
@@ -1,123 +0,0 @@
-//
-//  Kokoro-tts-lib
-//
-import Foundation
-import MLX
-import MLXNN
-
-public func computeNorm(
-  x: MLXArray,
-  p: Int,
-  dim: [Int]? = nil,
-  keepdim: Bool = false
-) -> MLXArray {
-  guard p == 1 || p == 2 else {
-    fatalError("Only p-norms with p of 1 or 2 are supported")
-  }
-
-  let dimensions: [Int]
-  if let dim = dim {
-    dimensions = dim
-  } else {
-    dimensions = Array(0 ..< x.ndim)
-  }
-
-  if p == 1 {
-    // L1 norm
-    return MLX.sum(MLX.abs(x), axes: dimensions, keepDims: keepdim)
-  } else {
-    // L2 norm
-    return MLX.sqrt(MLX.sum(x * x, axes: dimensions, keepDims: keepdim))
-  }
-}
-
-public func weightNorm(
-  weightV: MLXArray,
-  weightG: MLXArray,
-  dim: Int? = nil
-) -> MLXArray {
-  let rank = weightV.shape.count
-
-  var axes: [Int]
-
-  if let dim = dim {
-    var adjustedDim = dim
-    if dim < 0 {
-      adjustedDim += rank
-    }
-
-    axes = Array(0 ..< rank)
-    if adjustedDim != -1 {
-      axes.removeAll(where: { $0 == adjustedDim })
-    }
-  } else {
-    axes = Array(0 ..< rank)
-  }
-
-  let normV = computeNorm(x: weightV, p: 2, dim: axes, keepdim: true)
-
-  let normalizedWeight = weightV / (normV + 1e-7) // Add epsilon for numerical stability
-  return normalizedWeight * weightG
-}
-
-/// Conv1d with weight normalization
-public class ConvWeighted: Module {
-  public var weightG: MLXArray
-  public var weightV: MLXArray
-  public var bias: MLXArray?
-
-  public let stride: Int
-  public let padding: Int
-  public let dilation: Int
-  public let groups: Int
-
-  public init(
-    weightG: MLXArray,
-    weightV: MLXArray,
-    bias: MLXArray?,
-    stride: Int = 1,
-    padding: Int = 1,
-    dilation: Int = 1,
-    groups: Int = 1
-  ) {
-    self.stride = stride
-    self.padding = padding
-    self.dilation = dilation
-    self.groups = groups
-
-    // Store parameters
-    self.weightG = weightG
-    self.weightV = weightV
-    self.bias = bias
-
-    super.init()
-  }
-
-  public func callAsFunction(_ x: MLXArray, conv: (MLXArray, MLXArray, Int, Int, Int, Int, StreamOrDevice) -> MLXArray) -> MLXArray {
-    let weight = weightNorm(weightV: weightV, weightG: weightG, dim: 0)
-    bias = bias?.reshaped([1, 1, -1])
-
-    func applyConv(x: MLXArray, weightToUse: MLXArray) -> MLXArray {
-      let result = conv(
-        x,
-        weightToUse,
-        self.stride,
-        padding,
-        dilation,
-        groups,
-        .default
-      )
-
-      if let bias = bias {
-        return result + bias
-      }
-      return result
-    }
-
-    if x.shape.last == weight.shape.last || groups > 1 {
-      return applyConv(x: x, weightToUse: weight)
-    } else {
-      return applyConv(x: x, weightToUse: weight.transposed())
-    }
-  }
-}
diff --git a/Sources/MLXAudioCore/MLX+Extensions.swift b/Sources/MLXAudioCore/MLX+Extensions.swift
deleted file mode 100644
index 5607d97..0000000
--- a/Sources/MLXAudioCore/MLX+Extensions.swift
+++ /dev/null
@@ -1,184 +0,0 @@
-import MLX
-
-extension MLXArray {
-    
-    /// Scatter updates into a source array along a specified axis based on indices.
-    ///
-    /// This function mimics the behavior of scatter operations found in other frameworks.
-    /// It updates elements in a copy of the source array at positions specified by `indices`
-    /// along the given `axis` with values from the `updates` array.
-    ///
-    /// - Parameters:
-    ///   - source: The initial array.
-    ///   - indices: An array of indices where updates should be placed. Must be Int32 or Int64.
-    ///              The shape of `indices` should be broadcastable with `updates`.
-    ///   - updates: The array containing values to scatter into the source array.
-    ///              Its shape must be broadcastable with `indices` and match the source
-    ///              array's shape along non-axis dimensions.
-    ///   - axis: The axis along which to scatter the updates.
-    /// - Returns: A new array with the updates applied.
-    ///
-    /// - Note: This is a basic implementation and might not cover all edge cases or
-    ///         optimizations of native scatter operations. It currently uses `put`
-    ///         internally which might have limitations depending on the MLX-Swift version.
-    ///         Direct iteration might be needed if `put` is insufficient or unavailable.
-    public static func scatter(
-        _ source: MLXArray,
-        indices: MLXArray,
-        updates: MLXArray,
-        axis: Int = 0
-    ) -> MLXArray {
-        
-        // MLX `put` requires indices as Int32 or Int64
-        guard indices.dtype == .int32 || indices.dtype == .int64 else {
-            fatalError("Scatter indices must be Int32 or Int64, but got \(indices.dtype)")
-        }
-        
-        // Basic shape compatibility check (more robust checks might be needed)
-        guard updates.shape == indices.shape else {
-             // Allow broadcasting later if necessary, but for the current use case they match
-             print("Warning: Scatter updates shape \(updates.shape) does not match indices shape \(indices.shape). Assuming broadcast works or shapes are compatible for put.")
-            return source
-        }
-
-        // Create a copy of the source array
-        let result = source + 0 // Adding 0 creates a copy
-        
-        // For 1D case (most common in your use case), we can use advanced indexing
-        if axis == 0 && source.ndim == 1 {
-            // Use advanced indexing to update multiple elements at once
-            // This keeps everything on GPU without CPU round-trips
-            result[indices] = updates
-            return result
-        }
-        
-        // For more complex cases, we could implement other optimizations
-        // For now, fall back to a simpler approach that still avoids .item() calls
-        // This is a placeholder - the 1D case above should handle your repetition penalty use case
-        
-        return result
-    }
-
-    /// Stacks arrays along a new axis.
-    ///
-    /// - Parameters:
-    ///   - arrays: A list of arrays to stack.
-    ///   - axis: The axis in the result array along which the input arrays are stacked. Defaults to 0.
-    /// - Returns: The resulting stacked array.
-    static func stack(_ arrays: [MLXArray], axis: Int = 0) -> MLXArray {
-        // Ensure all arrays have the same shape
-        guard let firstShape = arrays.first?.shape else {
-            return MLXArray([])
-        }
-        
-        for array in arrays {
-            guard array.shape == firstShape else {
-                print("Warning: Array shape \(array.shape) does not match first shape \(firstShape)")
-                return MLXArray([])
-            }
-        }
-        
-        // Create a new shape with an additional dimension
-        var newShape = firstShape
-        // Ensure axis is within valid bounds
-        let validAxis = Swift.max(0, Swift.min(axis, newShape.count))
-        newShape.insert(arrays.count, at: validAxis)
-        
-        // Create a result array with the new shape
-        let dtype = arrays.first?.dtype ?? .float32
-        let result: MLXArray
-        switch dtype {
-        case .float32: result = MLXArray.zeros(newShape, type: Float.self)
-        case .float16: result = MLXArray.zeros(newShape, type: Float16.self)
-        case .int32: result = MLXArray.zeros(newShape, type: Int32.self)
-        case .int64: result = MLXArray.zeros(newShape, type: Int64.self)
-        case .bool: result = MLXArray.zeros(newShape, type: Bool.self)
-        case .uint8: result = MLXArray.zeros(newShape, type: UInt8.self)
-        case .uint16: result = MLXArray.zeros(newShape, type: UInt16.self)
-        case .uint32: result = MLXArray.zeros(newShape, type: UInt32.self)
-        case .uint64: result = MLXArray.zeros(newShape, type: UInt64.self)
-        case .int8: result = MLXArray.zeros(newShape, type: Int8.self)
-        case .int16: result = MLXArray.zeros(newShape, type: Int16.self)
-        case .bfloat16: result = MLXArray.zeros(newShape, type: Float16.self)
-        case .complex64: result = MLXArray.zeros(newShape, type: Float64.self)
-        case .float64: result = MLXArray.zeros(newShape, type: Float64.self)
-        }
-        
-        // Copy each array into the result
-        for (i, array) in arrays.enumerated() {
-            // Create a slice for the current array
-            var indices = Array(repeating: 0..<newShape[0], count: newShape.count)
-            indices[validAxis] = i..<(i+1)
-            
-            // Copy the array into the slice
-            result[indices] = array.expandedDimensions(axis: validAxis)
-        }
-        
-        return result
-    }
-    
-    /// Generates ranges of numbers, similar to Python's `arange`.
-    ///
-    /// Generate numbers in the half-open interval `[start, stop)` with the specified `step`.
-    ///
-    /// - Parameters:
-    ///   - start: Starting value of the sequence. Defaults to 0.
-    ///   - stop: End of the sequence. The sequence does not include this value.
-    ///   - step: Spacing between values. Defaults to 1.
-    ///   - dtype: The desired data type of the output array. Defaults to `.int32` if start, stop, step are Int, otherwise `.float32`.
-    /// - Returns: An array containing the generated range of values.
-    static func arange(start: Int = 0, stop: Int, step: Int = 1, dtype: DType = .int32) -> MLXArray {
-        guard step != 0 else { fatalError("Step cannot be zero.") }
-        guard (step > 0 && start < stop) || (step < 0 && start > stop) else {
-            // Return empty array if range is invalid or empty
-            switch dtype {
-                case .int32: return MLXArray.zeros([0], type: Int32.self)
-                case .float32: return MLXArray.zeros([0], type: Float.self)
-                default: fatalError("Unsupported dtype for empty arange: \(dtype)")
-            }
-        }
-        
-        let _ = (stop - start + step - 1) / step  // count - unused calculation
-        let sequence = Swift.stride(from: start, to: stop, by: step)
-        
-        // Create the array based on the dtype
-        switch dtype {
-        case .int32:
-            let data = sequence.map { Int32($0) }
-            return MLXArray(data)
-        case .float32:
-            let data = sequence.map { Float($0) }
-            return MLXArray(data)
-        default:
-            fatalError("Unsupported dtype for arange: \(dtype)")
-        }
-    }
-    
-    /// Generates ranges of numbers, similar to Python's `arange` (Float version).
-    static func arange(start: Float = 0.0, stop: Float, step: Float = 1.0, dtype: DType = .float32) -> MLXArray {
-        guard step != 0.0 else { fatalError("Step cannot be zero.") }
-        guard (step > 0 && start < stop) || (step < 0 && start > stop) else {
-            // Return empty array if range is invalid or empty
-            switch dtype {
-            case .float32: return MLXArray.zeros([0], type: Float.self)
-            case .float16: return MLXArray.zeros([0], type: Float16.self)
-            default: fatalError("Unsupported float dtype for empty arange: \(dtype)")
-            }
-        }
-        
-        let _ = Int((stop - start) / step)  // count - unused calculation
-        let sequence = Swift.stride(from: start, to: stop, by: step)
-        
-        // Create the array based on the dtype
-        switch dtype {
-        case .float32:
-            let data = sequence.map { Float($0) }
-            return MLXArray(data)
-        case .float16:
-            let data = sequence.map { Float16($0) }
-            return MLXArray(data)
-        default:
-            fatalError("Unsupported float dtype for arange: \(dtype)")
-        }
-    }
-} 
diff --git a/Sources/MLXAudioTTS/Models/Marvis/CSMLlamaModel.swift b/Sources/MLXAudioTTS/Models/Marvis/CSMLlamaModel.swift
index 97ec36f..6182878 100644
--- a/Sources/MLXAudioTTS/Models/Marvis/CSMLlamaModel.swift
+++ b/Sources/MLXAudioTTS/Models/Marvis/CSMLlamaModel.swift
@@ -292,7 +292,7 @@ public class CSMLlamaModel: Module, LLMModel, KVCacheDimensionProvider {
     public func callAsFunction(_ inputs: MLXArray, cache: [KVCacheSimple]?) -> MLXArray {
         var h = inputs
 
-        let mask = createAttentionMask(h: h, cache: cache)
+        let mask = createAttentionMask(h: h, cache: cache?.first)
 
         for (i, layer) in layers.enumerated() {
             h = layer(h, mask: mask, cache: cache?[i])
diff --git a/Sources/MLXAudioTTS/Models/Soprano/SopranoConfig.swift b/Sources/MLXAudioTTS/Models/Soprano/SopranoConfig.swift
index 3b08c5c..1d59ced 100644
--- a/Sources/MLXAudioTTS/Models/Soprano/SopranoConfig.swift
+++ b/Sources/MLXAudioTTS/Models/Soprano/SopranoConfig.swift
@@ -170,7 +170,7 @@ public struct SopranoConfiguration: Codable, Sendable {
         let baseConfig = try? BaseConfiguration(from: decoder)
         let globalQuant = try container.decodeIfPresent(BaseConfiguration.Quantization.self, forKey: .quantization)
         let altGlobalQuant = try container.decodeIfPresent(BaseConfiguration.Quantization.self, forKey: .quantizationConfig)
-        self.quantization = globalQuant ?? altGlobalQuant ?? baseConfig?.quantization
+        self.quantization = globalQuant ?? altGlobalQuant
         self.perLayerQuantization = baseConfig?.perLayerQuantization
     }
 

commit a6aa5a7b095708a10c4a42742af8c369bb500923
Author: Lucas Newman <lucas@future.fit>
Date:   Sat Feb 21 09:19:12 2026 -0800

    Allow configuring the cache location for downloaded models (#60)
    
    * Add Voxtral Realtime STT model.
    
    Co-authored-by: Shreyas Karnik <311217+shreyaskarnik@users.noreply.github.com>
    
    * Update README.
    
    * Fix merge conflicts.
    
    * Review feedback.
    
    * Checkpoint.
    
    * Use factory pattern for TTS like STS.
    
    * Remove unused function.
    
    ---------
    
    Co-authored-by: Shreyas Karnik <311217+shreyaskarnik@users.noreply.github.com>
    Co-authored-by: Prince Canuma <prince.gdt@gmail.com>

diff --git a/Examples/SimpleChat/SimpleChat/SpeechController.swift b/Examples/SimpleChat/SimpleChat/SpeechController.swift
index 624ffc0..3f775bd 100644
--- a/Examples/SimpleChat/SimpleChat/SpeechController.swift
+++ b/Examples/SimpleChat/SimpleChat/SpeechController.swift
@@ -40,7 +40,7 @@ final class SpeechController {
         Task {
             do {
                 print("Loading TTS model: \(ttsRepoId)")
-                self.model = try await TTSModelUtils.loadModel(modelRepo: ttsRepoId)
+                self.model = try await TTS.loadModel(modelRepo: ttsRepoId)
                 print("Loaded TTS model.")
             } catch {
                 print("Error loading model: \(error)")
diff --git a/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift b/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
index 4c7522c..644b803 100644
--- a/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
+++ b/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
@@ -135,7 +135,7 @@ class TTSViewModel {
 
         do {
             defaultGenerationParameters = model?.defaultGenerationParameters ?? defaultGenerationParameters
-            model = try await TTSModelUtils.loadModel(modelRepo: modelId)
+            model = try await TTS.loadModel(modelRepo: modelId)
             loadedModelId = modelId
             generationProgress = "" // Clear progress on success
         } catch {
diff --git a/Sources/MLXAudioCodecs/DACVAE/DACVAE.swift b/Sources/MLXAudioCodecs/DACVAE/DACVAE.swift
index fb915bb..a26bd90 100644
--- a/Sources/MLXAudioCodecs/DACVAE/DACVAE.swift
+++ b/Sources/MLXAudioCodecs/DACVAE/DACVAE.swift
@@ -8,7 +8,8 @@
 import Foundation
 import MLX
 import MLXNN
-import Hub
+import HuggingFace
+import MLXAudioCore
 
 // MARK: - Quantizer Projections
 
@@ -571,12 +572,23 @@ public class DACVAE: Module {
     }
 
     /// Load a pretrained DACVAE model from HuggingFace Hub.
-    public static func fromPretrained(_ repoId: String) async throws -> DACVAE {
-        let hub = HubApi()
-        let repo = Hub.Repo(id: repoId)
+    public static func fromPretrained(
+        _ repoId: String,
+        cache: HubCache = .default
+    ) async throws -> DACVAE {
+        guard let repoID = Repo.ID(rawValue: repoId) else {
+            throw NSError(
+                domain: "DACVAE",
+                code: 1,
+                userInfo: [NSLocalizedDescriptionKey: "Invalid repository ID: \(repoId)"]
+            )
+        }
 
-        // Download model files
-        let modelURL = try await hub.snapshot(from: repo, matching: ["*.json", "*.safetensors"])
+        let modelURL = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: ".safetensors",
+            cache: cache
+        )
 
         // Load config
         let configURL = modelURL.appendingPathComponent("config.json")
diff --git a/Sources/MLXAudioCodecs/Encodec/Encodec.swift b/Sources/MLXAudioCodecs/Encodec/Encodec.swift
index e904cf5..fc92baf 100644
--- a/Sources/MLXAudioCodecs/Encodec/Encodec.swift
+++ b/Sources/MLXAudioCodecs/Encodec/Encodec.swift
@@ -8,7 +8,8 @@
 import Foundation
 import MLX
 import MLXNN
-import Hub
+import HuggingFace
+import MLXAudioCore
 
 // MARK: - Encodec Encoder
 
@@ -398,12 +399,23 @@ public class Encodec: Module {
     // MARK: - Loading
 
     /// Load a pretrained Encodec model from HuggingFace Hub.
-    public static func fromPretrained(_ pathOrRepo: String) async throws -> Encodec {
-        let hub = HubApi()
-        let repo = Hub.Repo(id: pathOrRepo)
+    public static func fromPretrained(
+        _ pathOrRepo: String,
+        cache: HubCache = .default
+    ) async throws -> Encodec {
+        guard let repoID = Repo.ID(rawValue: pathOrRepo) else {
+            throw NSError(
+                domain: "Encodec",
+                code: 1,
+                userInfo: [NSLocalizedDescriptionKey: "Invalid repository ID: \(pathOrRepo)"]
+            )
+        }
 
-        // Download model files
-        let modelURL = try await hub.snapshot(from: repo, matching: ["*.json", "*.safetensors"])
+        let modelURL = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: ".safetensors",
+            cache: cache
+        )
 
         // Load config
         let configURL = modelURL.appendingPathComponent("config.json")
diff --git a/Sources/MLXAudioCodecs/Mimi/Mimi.swift b/Sources/MLXAudioCodecs/Mimi/Mimi.swift
index 1112ce6..a405e1a 100644
--- a/Sources/MLXAudioCodecs/Mimi/Mimi.swift
+++ b/Sources/MLXAudioCodecs/Mimi/Mimi.swift
@@ -1,5 +1,5 @@
 import Foundation
-import Hub
+import HuggingFace
 import MLX
 import MLXAudioCore
 import MLXNN
@@ -233,7 +233,12 @@ public final class MimiStreamingDecoder {
 }
 
 public extension Mimi {
-    static func fromPretrained(repoId: String = "kyutai/moshiko-pytorch-bf16", filename: String = "tokenizer-e351c8d8-checkpoint125.safetensors", progressHandler: @escaping (Progress) -> Void) async throws -> Mimi {
+    static func fromPretrained(
+        repoId: String = "kyutai/moshiko-pytorch-bf16",
+        filename: String = "tokenizer-e351c8d8-checkpoint125.safetensors",
+        cache: HubCache = .default,
+        progressHandler: @escaping (Progress) -> Void
+    ) async throws -> Mimi {
         print("[Mimi] Starting Mimi model loading from \(repoId)")
 
         print("[Mimi] Creating configuration...")
@@ -247,7 +252,40 @@ public extension Mimi {
 
         print("[Mimi] Downloading/snapshotting weights file...")
         let snapshotStart = CFAbsoluteTimeGetCurrent()
-        let weightFileURL = try await Hub.snapshot(from: repoId, matching: filename, progressHandler: progressHandler).appending(path: filename)
+        guard let repoID = Repo.ID(rawValue: repoId) else {
+            throw NSError(
+                domain: "Mimi",
+                code: 1,
+                userInfo: [NSLocalizedDescriptionKey: "Invalid repository ID: \(repoId)"]
+            )
+        }
+        let modelSubdir = repoID.description.replacingOccurrences(of: "/", with: "_")
+        let modelDir = cache.cacheDirectory
+            .appendingPathComponent("mlx-audio")
+            .appendingPathComponent(modelSubdir)
+        let weightFileURL = modelDir.appendingPathComponent(filename)
+
+        if !FileManager.default.fileExists(atPath: weightFileURL.path) {
+            try FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
+
+            let client = HubClient(cache: cache)
+            _ = try await client.downloadSnapshot(
+                of: repoID,
+                kind: .model,
+                to: modelDir,
+                revision: "main",
+                matching: [filename],
+                progressHandler: progressHandler
+            )
+        }
+
+        guard FileManager.default.fileExists(atPath: weightFileURL.path) else {
+            throw NSError(
+                domain: "Mimi",
+                code: 2,
+                userInfo: [NSLocalizedDescriptionKey: "Expected weights file not found at \(weightFileURL.path)"]
+            )
+        }
         let snapshotTime = CFAbsoluteTimeGetCurrent() - snapshotStart
         print(String(format: "[Mimi] Weights file snapshot completed in %.2f seconds", snapshotTime))
 
diff --git a/Sources/MLXAudioCodecs/SNAC/SNACDecoder.swift b/Sources/MLXAudioCodecs/SNAC/SNACDecoder.swift
index ff17680..ec792cf 100644
--- a/Sources/MLXAudioCodecs/SNAC/SNACDecoder.swift
+++ b/Sources/MLXAudioCodecs/SNAC/SNACDecoder.swift
@@ -2,6 +2,7 @@ import Foundation
 import MLX
 import MLXNN
 import HuggingFace
+import MLXAudioCore
 
 
 
@@ -152,19 +153,19 @@ public class SNAC: Module {
         )
     }
 
-    public static func fromPretrained(_ modelRepo: String) async throws -> SNAC {
-        let client = HubClient.default
-        let cache = client.cache ?? HubCache.default
-
+    public static func fromPretrained(
+        _ modelRepo: String,
+        cache: HubCache = .default
+    ) async throws -> SNAC {
         guard let repoID = Repo.ID(rawValue: modelRepo) else {
             throw NSError(domain: "SNAC", code: 1, userInfo: [NSLocalizedDescriptionKey: "Invalid repository ID: \(modelRepo)"])
         }
 
         // Check if model is already fully cached (has weight files)
-        let modelDir = try await resolveOrDownloadSNACModel(
-            client: client,
-            cache: cache,
-            repoID: repoID
+        let modelDir = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: ".safetensors",
+            cache: cache
         )
 
 
@@ -232,47 +233,3 @@ extension MLXArray {
         return MLX.padded(self, widths: paddingArray)
     }
 }
-
-// MARK: - Cache Helper
-
-/// Resolves a SNAC model from cache or downloads it if not cached.
-private func resolveOrDownloadSNACModel(
-    client: HubClient,
-    cache: HubCache,
-    repoID: Repo.ID
-) async throws -> URL {
-    // Use a persistent cache directory based on repo ID
-    let modelSubdir = repoID.description.replacingOccurrences(of: "/", with: "_")
-    let modelDir = FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first!
-        .appendingPathComponent("mlx-audio")
-        .appendingPathComponent(modelSubdir)
-
-    // Check if model already exists with weight files
-    if FileManager.default.fileExists(atPath: modelDir.path) {
-        let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-        let hasWeights = files?.contains { $0.pathExtension == "safetensors" } ?? false
-
-        if hasWeights {
-            print("Using cached SNAC model at: \(modelDir.path)")
-            return modelDir
-        }
-    }
-
-    // Create directory if needed
-    try FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
-
-    print("Downloading SNAC model \(repoID)...")
-    _ = try await client.downloadSnapshot(
-        of: repoID,
-        kind: .model,
-        to: modelDir,
-        revision: "main",
-        matching: ["*.safetensors", "*.json"],
-        progressHandler: { progress in
-            print("\(progress.completedUnitCount)/\(progress.totalUnitCount) files")
-        }
-    )
-
-    print("SNAC model downloaded to: \(modelDir.path)")
-    return modelDir
-}
diff --git a/Sources/MLXAudioCore/ModelUtils.swift b/Sources/MLXAudioCore/ModelUtils.swift
index fc0522f..76b9033 100644
--- a/Sources/MLXAudioCore/ModelUtils.swift
+++ b/Sources/MLXAudioCore/ModelUtils.swift
@@ -2,9 +2,18 @@ import Foundation
 import HuggingFace
 
 public enum ModelUtils {
-    public static func resolveModelType(repoID: Repo.ID, hfToken: String? = nil) async throws -> String? {
+    public static func resolveModelType(
+        repoID: Repo.ID,
+        hfToken: String? = nil,
+        cache: HubCache = .default
+    ) async throws -> String? {
         let modelNameComponents = repoID.name.split(separator: "/").last?.split(separator: "-")
-        let modelURL = try await resolveOrDownloadModel(repoID: repoID, requiredExtension: "safetensors", hfToken: hfToken)
+        let modelURL = try await resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: "safetensors",
+            hfToken: hfToken,
+            cache: cache
+        )
         let configJSON = try JSONSerialization.jsonObject(with: Data(contentsOf: modelURL.appendingPathComponent("config.json")))
         if let config = configJSON as? [String: Any] {
             return (config["model_type"] as? String) ?? (config["architecture"] as? String) ?? modelNameComponents?.first?.lowercased()
@@ -21,17 +30,23 @@ public enum ModelUtils {
     public static func resolveOrDownloadModel(
         repoID: Repo.ID,
         requiredExtension: String,
-        hfToken: String? = nil
+        hfToken: String? = nil,
+        cache: HubCache = .default
     ) async throws -> URL {
         let client: HubClient
         if let token = hfToken, !token.isEmpty {
             print("Using HuggingFace token from configuration")
-            client = HubClient(host: HubClient.defaultHost, bearerToken: token)
+            client = HubClient(host: HubClient.defaultHost, bearerToken: token, cache: cache)
         } else {
-            client = HubClient.default
+            client = HubClient(cache: cache)
         }
-        let cache = client.cache ?? HubCache.default
-        return try await resolveOrDownloadModel(client: client, cache: cache, repoID: repoID, requiredExtension: requiredExtension)
+        let resolvedCache = client.cache ?? cache
+        return try await resolveOrDownloadModel(
+            client: client,
+            cache: resolvedCache,
+            repoID: repoID,
+            requiredExtension: requiredExtension
+        )
     }
 
     /// Resolves a model from cache or downloads it if not cached.
@@ -43,18 +58,24 @@ public enum ModelUtils {
     /// - Returns: The model directory URL
     public static func resolveOrDownloadModel(
         client: HubClient,
-        cache: HubCache,
+        cache: HubCache = .default,
         repoID: Repo.ID,
         requiredExtension: String
     ) async throws -> URL {
-        // Use a persistent cache directory based on repo ID
+        let normalizedRequiredExtension = requiredExtension.hasPrefix(".")
+            ? String(requiredExtension.dropFirst())
+            : requiredExtension
+
+        // Store downloaded model snapshots under the configured Hugging Face cache root.
         let modelSubdir = repoID.description.replacingOccurrences(of: "/", with: "_")
-        let modelDir = URL.cachesDirectory.appendingPathComponent("mlx-audio").appendingPathComponent(modelSubdir)
+        let modelDir = cache.cacheDirectory
+            .appendingPathComponent("mlx-audio")
+            .appendingPathComponent(modelSubdir)
 
         // Check if model already exists with required files
         if FileManager.default.fileExists(atPath: modelDir.path) {
             let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-            let hasRequiredFiles = files?.contains { $0.pathExtension == requiredExtension } ?? false
+            let hasRequiredFiles = files?.contains { $0.pathExtension == normalizedRequiredExtension } ?? false
 
             if hasRequiredFiles {
                 // Validate that config.json is valid JSON
@@ -75,7 +96,7 @@ public enum ModelUtils {
         // Create directory if needed
         try FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
 
-        let allowedExtensions: Set<String> = ["*.\(requiredExtension)", "*.safetensors", "*.json", "*.txt", "*.wav"]
+        let allowedExtensions: Set<String> = ["*.\(normalizedRequiredExtension)", "*.safetensors", "*.json", "*.txt", "*.wav"]
 
         print("Downloading model \(repoID)...")
         _ = try await client.downloadSnapshot(
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift b/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift
index b87ea3e..e5b2703 100644
--- a/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift
@@ -717,13 +717,17 @@ public class LFM2AudioModel: Module, STSModel, @unchecked Sendable {
 
     // MARK: - From Pretrained
 
-    public static func fromPretrained(_ modelNameOrPath: String) async throws -> LFM2AudioModel {
+    public static func fromPretrained(
+        _ modelNameOrPath: String,
+        cache: HubCache = .default
+    ) async throws -> LFM2AudioModel {
         guard let repoID = Repo.ID(rawValue: modelNameOrPath) else {
             throw LFMAudioError.modelNotFound(modelNameOrPath)
         }
         let modelDir = try await ModelUtils.resolveOrDownloadModel(
             repoID: repoID,
-            requiredExtension: "safetensors"
+            requiredExtension: "safetensors",
+            cache: cache
         )
 
         let configURL = modelDir.appendingPathComponent("config.json")
diff --git a/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift b/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
index c95eaf8..272a61f 100644
--- a/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
+++ b/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
@@ -363,12 +363,19 @@ public final class MossFormer2SEModel: STSModel {
         return MossFormer2SEModel(model: model, config: config)
     }
 
-    public static func fromPretrained(_ modelPath: String = defaultRepo) async throws -> MossFormer2SEModel {
+    public static func fromPretrained(
+        _ modelPath: String = defaultRepo,
+        cache: HubCache = .default
+    ) async throws -> MossFormer2SEModel {
         guard let repoID = Repo.ID(rawValue: modelPath) else {
             throw MossFormer2SEError.invalidRepoID(modelPath)
         }
 
-        let modelDir = try await ModelUtils.resolveOrDownloadModel(repoID: repoID, requiredExtension: "safetensors")
+        let modelDir = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: "safetensors",
+            cache: cache
+        )
         let config = try loadConfig(from: modelDir, fallbackPolicy: .fallbackOnReadError)
         let weights = try loadWeights(from: modelDir, duplicateKeyPolicy: .overwrite)
         return try buildModel(config: config, weights: weights)
diff --git a/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudioWeights.swift b/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudioWeights.swift
index 3d58f21..609cbfa 100644
--- a/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudioWeights.swift
+++ b/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudioWeights.swift
@@ -341,7 +341,8 @@ extension SAMAudio {
     public static func fromPretrained(
         _ modelPath: String = defaultRepo,
         hfToken: String? = nil,
-        strict: Bool = false
+        strict: Bool = false,
+        cache: HubCache = .default
     ) async throws -> SAMAudio {
         let fm = FileManager.default
         let modelDir: URL
@@ -355,7 +356,8 @@ extension SAMAudio {
             modelDir = try await ModelUtils.resolveOrDownloadModel(
                 repoID: repoID,
                 requiredExtension: "safetensors",
-                hfToken: hfToken
+                hfToken: hfToken,
+                cache: cache
             )
         }
 
diff --git a/Sources/MLXAudioSTS/STSModel.swift b/Sources/MLXAudioSTS/STSModel.swift
index 1962b97..e0b244e 100644
--- a/Sources/MLXAudioSTS/STSModel.swift
+++ b/Sources/MLXAudioSTS/STSModel.swift
@@ -52,18 +52,24 @@ public enum STS {
     public static func loadModel(
         modelRepo: String,
         hfToken: String? = nil,
-        strict: Bool = false
+        strict: Bool = false,
+        cache: HubCache = .default
     ) async throws -> LoadedSTSModel {
         guard let repoID = Repo.ID(rawValue: modelRepo) else {
             throw STSModelError.invalidRepositoryID(modelRepo)
         }
 
-        let modelType = try await ModelUtils.resolveModelType(repoID: repoID, hfToken: hfToken)
+        let modelType = try await ModelUtils.resolveModelType(
+            repoID: repoID,
+            hfToken: hfToken,
+            cache: cache
+        )
         return try await loadModel(
             modelRepo: modelRepo,
             modelType: modelType,
             hfToken: hfToken,
-            strict: strict
+            strict: strict,
+            cache: cache
         )
     }
 
@@ -71,7 +77,8 @@ public enum STS {
         modelRepo: String,
         modelType: String?,
         hfToken: String? = nil,
-        strict: Bool = false
+        strict: Bool = false,
+        cache: HubCache = .default
     ) async throws -> LoadedSTSModel {
         let resolved = normalizedModelType(modelType) ?? inferModelType(from: modelRepo)
         guard let resolved else {
@@ -80,15 +87,20 @@ public enum STS {
 
         switch resolved {
         case "lfm_audio", "lfm", "lfm2", "lfm2_audio":
-            let model = try await LFM2AudioModel.fromPretrained(modelRepo)
+            let model = try await LFM2AudioModel.fromPretrained(modelRepo, cache: cache)
             return .lfmAudio(model)
 
         case "sam_audio", "sam", "samaudio":
-            let model = try await SAMAudio.fromPretrained(modelRepo, hfToken: hfToken, strict: strict)
+            let model = try await SAMAudio.fromPretrained(
+                modelRepo,
+                hfToken: hfToken,
+                strict: strict,
+                cache: cache
+            )
             return .samAudio(model)
 
         case "mossformer2_se", "mossformer2", "mossformer":
-            let model = try await MossFormer2SEModel.fromPretrained(modelRepo)
+            let model = try await MossFormer2SEModel.fromPretrained(modelRepo, cache: cache)
             return .mossFormer2SE(model)
 
         default:
diff --git a/Sources/MLXAudioSTT/Models/GLMASR/GLMASR.swift b/Sources/MLXAudioSTT/Models/GLMASR/GLMASR.swift
index 336bcd8..884f122 100644
--- a/Sources/MLXAudioSTT/Models/GLMASR/GLMASR.swift
+++ b/Sources/MLXAudioSTT/Models/GLMASR/GLMASR.swift
@@ -587,10 +587,10 @@ public class GLMASRModel: Module {
     }
 
     /// Load model from pretrained weights.
-    public static func fromPretrained(_ modelPath: String) async throws -> GLMASRModel {
-        let client = HubClient.default
-        let cache = client.cache ?? HubCache.default
-
+    public static func fromPretrained(
+        _ modelPath: String,
+        cache: HubCache = .default
+    ) async throws -> GLMASRModel {
         guard let repoID = Repo.ID(rawValue: modelPath) else {
             throw NSError(
                 domain: "GLMASRModel",
@@ -599,10 +599,10 @@ public class GLMASRModel: Module {
             )
         }
 
-        let modelDir = try await resolveOrDownloadModel(
-            client: client,
-            cache: cache,
-            repoID: repoID
+        let modelDir = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: ".safetensors",
+            cache: cache
         )
 
         // Load config
@@ -763,44 +763,4 @@ public class GLMASRModel: Module {
         )
     }
 
-    private static func resolveOrDownloadModel(
-        client: HubClient,
-        cache: HubCache,
-        repoID: Repo.ID
-    ) async throws -> URL {
-        let modelSubdir = repoID.description.replacingOccurrences(of: "/", with: "_")
-        let modelDir = FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first!
-            .appendingPathComponent("mlx-audio")
-            .appendingPathComponent(modelSubdir)
-
-        // Check if model already exists
-        let configPath = modelDir.appendingPathComponent("config.json")
-        if FileManager.default.fileExists(atPath: configPath.path) {
-            let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-            let hasSafetensors = files?.contains { $0.pathExtension == "safetensors" } ?? false
-
-            if hasSafetensors {
-                print("Using cached model at: \(modelDir.path)")
-                return modelDir
-            }
-        }
-
-        // Create directory if needed
-        try FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
-
-        // Download model
-        print("Downloading model \(repoID)...")
-        _ = try await client.downloadSnapshot(
-            of: repoID,
-            kind: .model,
-            to: modelDir,
-            revision: "main",
-            progressHandler: { progress in
-                print("\(progress.completedUnitCount)/\(progress.totalUnitCount) files")
-            }
-        )
-
-        print("Model downloaded to: \(modelDir.path)")
-        return modelDir
-    }
 }
diff --git a/Sources/MLXAudioSTT/Models/Parakeet/ParakeetModel.swift b/Sources/MLXAudioSTT/Models/Parakeet/ParakeetModel.swift
index d07ab9b..c687c49 100644
--- a/Sources/MLXAudioSTT/Models/Parakeet/ParakeetModel.swift
+++ b/Sources/MLXAudioSTT/Models/Parakeet/ParakeetModel.swift
@@ -556,7 +556,10 @@ public extension ParakeetModel {
         return model
     }
 
-    static func fromPretrained(_ modelPath: String) async throws -> ParakeetModel {
+    static func fromPretrained(
+        _ modelPath: String,
+        cache: HubCache = .default
+    ) async throws -> ParakeetModel {
         let hfToken: String? = ProcessInfo.processInfo.environment["HF_TOKEN"]
             ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
 
@@ -571,7 +574,8 @@ public extension ParakeetModel {
         let modelDir = try await ModelUtils.resolveOrDownloadModel(
             repoID: repoID,
             requiredExtension: "safetensors",
-            hfToken: hfToken
+            hfToken: hfToken,
+            cache: cache
         )
         return try fromDirectory(modelDir)
     }
diff --git a/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ASR.swift b/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ASR.swift
index 2c0f4b9..2f0abca 100644
--- a/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ASR.swift
+++ b/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ASR.swift
@@ -1448,7 +1448,10 @@ public class Qwen3ASRModel: Module {
 
     // MARK: - Model Loading
 
-    public static func fromPretrained(_ modelPath: String) async throws -> Qwen3ASRModel {
+    public static func fromPretrained(
+        _ modelPath: String,
+        cache: HubCache = .default
+    ) async throws -> Qwen3ASRModel {
         let hfToken: String? = ProcessInfo.processInfo.environment["HF_TOKEN"]
             ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
 
@@ -1463,7 +1466,8 @@ public class Qwen3ASRModel: Module {
         let modelDir = try await ModelUtils.resolveOrDownloadModel(
             repoID: repoID,
             requiredExtension: "safetensors",
-            hfToken: hfToken
+            hfToken: hfToken,
+            cache: cache
         )
 
         // Load config
diff --git a/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ForcedAligner.swift b/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ForcedAligner.swift
index 126debd..be5e97e 100644
--- a/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ForcedAligner.swift
+++ b/Sources/MLXAudioSTT/Models/Qwen3ASR/Qwen3ForcedAligner.swift
@@ -528,7 +528,10 @@ public class Qwen3ForcedAlignerModel: Module {
 
     // MARK: - Model Loading
 
-    public static func fromPretrained(_ modelPath: String) async throws -> Qwen3ForcedAlignerModel {
+    public static func fromPretrained(
+        _ modelPath: String,
+        cache: HubCache = .default
+    ) async throws -> Qwen3ForcedAlignerModel {
         let hfToken: String? = ProcessInfo.processInfo.environment["HF_TOKEN"]
             ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
 
@@ -543,7 +546,8 @@ public class Qwen3ForcedAlignerModel: Module {
         let modelDir = try await ModelUtils.resolveOrDownloadModel(
             repoID: repoID,
             requiredExtension: "safetensors",
-            hfToken: hfToken
+            hfToken: hfToken,
+            cache: cache
         )
 
         // Load config
diff --git a/Sources/MLXAudioTTS/Models/Llama/LlamaTTS.swift b/Sources/MLXAudioTTS/Models/Llama/LlamaTTS.swift
index 74f1fe0..65abe9b 100644
--- a/Sources/MLXAudioTTS/Models/Llama/LlamaTTS.swift
+++ b/Sources/MLXAudioTTS/Models/Llama/LlamaTTS.swift
@@ -592,12 +592,12 @@ public class LlamaTTSModel: Module, KVCacheDimensionProvider, SpeechGenerationMo
         return weights
     }
 
-    public func post_load_hook(model: LlamaTTSModel, modelDir: URL) async throws {
+    public func post_load_hook(model: LlamaTTSModel, modelDir: URL, cache: HubCache = .default) async throws {
         if model.tokenizer == nil {
             model.tokenizer = try await AutoTokenizer.from(modelFolder: modelDir)
         }
         if model._snacModel == nil {
-            model._snacModel = try await SNAC.fromPretrained("mlx-community/snac_24khz")
+            model._snacModel = try await SNAC.fromPretrained("mlx-community/snac_24khz", cache: cache)
         }
     }
 
@@ -914,10 +914,10 @@ public class LlamaTTSModel: Module, KVCacheDimensionProvider, SpeechGenerationMo
     ///
     /// - Parameter modelRepo: The model repository ID (e.g., "mlx-community/orpheus-3b-0.1-ft-bf16")
     /// - Returns: The loaded model
-    public static func fromPretrained(_ modelRepo: String) async throws -> LlamaTTSModel {
-        let client = HubClient.default
-        let cache = client.cache ?? HubCache.default
-
+    public static func fromPretrained(
+        _ modelRepo: String,
+        cache: HubCache = .default
+    ) async throws -> LlamaTTSModel {
         guard let repoID = Repo.ID(rawValue: modelRepo) else {
             throw NSError(
                 domain: "LlamaTTSModel",
@@ -926,11 +926,10 @@ public class LlamaTTSModel: Module, KVCacheDimensionProvider, SpeechGenerationMo
             )
         }
 
-        let modelDir = try await llamaTTSResolveOrDownloadModel(
-            client: client,
-            cache: cache,
+        let modelDir = try await ModelUtils.resolveOrDownloadModel(
             repoID: repoID,
-            requiredExtension: "safetensors"
+            requiredExtension: ".safetensors",
+            cache: cache
         )
 
         let configPath = modelDir.appendingPathComponent("config.json")
@@ -961,7 +960,7 @@ public class LlamaTTSModel: Module, KVCacheDimensionProvider, SpeechGenerationMo
         try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
         eval(model)
 
-        try await model.post_load_hook(model: model, modelDir: modelDir)
+        try await model.post_load_hook(model: model, modelDir: modelDir, cache: cache)
 
         return model
     }
@@ -981,52 +980,3 @@ private func llamaTTSLoadWeights(from directory: URL) throws -> [String: MLXArra
     }
     return weights
 }
-
-private func llamaTTSResolveOrDownloadModel(
-    client: HubClient,
-    cache: HubCache,
-    repoID: Repo.ID,
-    requiredExtension: String
-) async throws -> URL {
-    let modelSubdir = repoID.description.replacingOccurrences(of: "/", with: "_")
-    let modelDir = FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first!
-        .appendingPathComponent("mlx-audio")
-        .appendingPathComponent(modelSubdir)
-
-    // Check if model already exists with required files (config.json + safetensors)
-    let configPath = modelDir.appendingPathComponent("config.json")
-    if FileManager.default.fileExists(atPath: configPath.path) {
-        let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-        let hasRequiredFiles = files?.contains { $0.pathExtension == requiredExtension } ?? false
-
-        if hasRequiredFiles {
-            print("Using cached model at: \(modelDir.path)")
-            return modelDir
-        }
-    }
-
-    // Create directory if needed
-    try FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
-
-    // Remove any partial downloads to avoid "file exists" errors
-    if FileManager.default.fileExists(atPath: modelDir.path) {
-        let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-        for file in files ?? [] {
-            try? FileManager.default.removeItem(at: file)
-        }
-    }
-
-    print("Downloading model \(repoID)...")
-    _ = try await client.downloadSnapshot(
-        of: repoID,
-        kind: .model,
-        to: modelDir,
-        revision: "main",
-        progressHandler: { progress in
-            print("\(progress.completedUnitCount)/\(progress.totalUnitCount) files")
-        }
-    )
-
-    print("Model downloaded to: \(modelDir.path)")
-    return modelDir
-}
diff --git a/Sources/MLXAudioTTS/Models/Marvis/MarvisTTSModel.swift b/Sources/MLXAudioTTS/Models/Marvis/MarvisTTSModel.swift
index c7650b6..95838d3 100644
--- a/Sources/MLXAudioTTS/Models/Marvis/MarvisTTSModel.swift
+++ b/Sources/MLXAudioTTS/Models/Marvis/MarvisTTSModel.swift
@@ -99,13 +99,6 @@ public final class MarvisTTSModel: Module {
         return (frame, mask)
     }
     
-    private func cacheURL(for audioURL: URL) -> URL {
-        let cacheFilename = audioURL.deletingPathExtension().appendingPathExtension("npy").lastPathComponent
-        let cacheDirectory = FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first!.appending(component: "MarvisTTSModel/prompt_cache")
-        try? FileManager.default.createDirectory(at: cacheDirectory, withIntermediateDirectories: true)
-        return cacheDirectory.appending(component: cacheFilename)
-    }
-    
     private func tokenizeAudio(_ audio: MLXArray, addEOS: Bool = true) throws -> (MLXArray, MLXArray) {
         let K = model.args.audioNumCodebooks
         
@@ -150,6 +143,7 @@ public final class MarvisTTSModel: Module {
 public extension MarvisTTSModel {
     static func fromPretrained(
         _ modelRepo: String = "Marvis-AI/marvis-tts-250m-v0.2-MLX-8bit",
+        cache: HubCache = .default,
         progressHandler: @escaping (Progress) -> Void = { _ in }
     ) async throws -> MarvisTTSModel {
         Memory.cacheLimit = 100 * 1024 * 1024
@@ -168,7 +162,8 @@ public extension MarvisTTSModel {
         let modelDirectoryURL = try await ModelUtils.resolveOrDownloadModel(
             repoID: repoID,
             requiredExtension: "safetensors",
-            hfToken: hfToken
+            hfToken: hfToken,
+            cache: cache
         )
 
         let promptFileURLs = modelDirectoryURL.appendingPathComponent("prompts", isDirectory: true)
@@ -185,7 +180,7 @@ public extension MarvisTTSModel {
         let args = try JSONDecoder().decode(CSMModelArgs.self, from: Data(contentsOf: configFileURL))
 
         let textTokenizer = try await AutoTokenizer.from(modelFolder: modelDirectoryURL)
-        let codec = try await Mimi.fromPretrained(progressHandler: progressHandler)
+        let codec = try await Mimi.fromPretrained(cache: cache, progressHandler: progressHandler)
         let audioTokenizer = MimiTokenizer(codec)
         let model = MarvisTTSModel(
             config: args,
@@ -220,10 +215,11 @@ public extension MarvisTTSModel {
     static func fromPretrained(
         hub: HubApi = .shared,
         repoId: String = "Marvis-AI/marvis-tts-250m-v0.2-MLX-8bit",
+        cache: HubCache = .default,
         progressHandler: @escaping (Progress) -> Void
     ) async throws -> MarvisTTSModel {
         _ = hub
-        return try await fromPretrained(repoId, progressHandler: progressHandler)
+        return try await fromPretrained(repoId, cache: cache, progressHandler: progressHandler)
     }
     
     private static func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
diff --git a/Sources/MLXAudioTTS/Models/PocketTTS/PocketTTSModel.swift b/Sources/MLXAudioTTS/Models/PocketTTS/PocketTTSModel.swift
index 1cf6efd..69efc11 100644
--- a/Sources/MLXAudioTTS/Models/PocketTTS/PocketTTSModel.swift
+++ b/Sources/MLXAudioTTS/Models/PocketTTS/PocketTTSModel.swift
@@ -1,5 +1,4 @@
 import Foundation
-import Hub
 import HuggingFace
 @preconcurrency import MLX
 import MLXAudioCore
@@ -334,22 +333,23 @@ public final class PocketTTSModel: Module, SpeechGenerationModel, @unchecked Sen
 
     // MARK: - Loading
 
-    public static func fromPretrained(_ modelRepo: String) async throws -> PocketTTSModel {
+    public static func fromPretrained(
+        _ modelRepo: String,
+        cache: HubCache = .default
+    ) async throws -> PocketTTSModel {
         let hfToken: String? = ProcessInfo.processInfo.environment["HF_TOKEN"]
             ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
 
-        let client = if let token = hfToken, !token.isEmpty {
-            HubClient(host: HubClient.defaultHost, bearerToken: token)
-        } else {
-            HubClient.default
-        }
-        let cache = client.cache ?? HubCache.default
-
         guard let repoID = Repo.ID(rawValue: modelRepo) else {
             throw NSError(domain: "PocketTTSModel", code: 1, userInfo: [NSLocalizedDescriptionKey: "Invalid repository ID: \(modelRepo)"])
         }
 
-        let modelDir = try await resolveOrDownloadPocketTTSModel(client: client, cache: cache, repoID: repoID)
+        let modelDir = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: ".safetensors",
+            hfToken: hfToken,
+            cache: cache
+        )
         let configURL = modelDir.appendingPathComponent("config.json")
         let config = try PocketTTSModelConfig.load(from: configURL)
 
@@ -362,39 +362,6 @@ public final class PocketTTSModel: Module, SpeechGenerationModel, @unchecked Sen
     }
 }
 
-private func resolveOrDownloadPocketTTSModel(
-    client: HubClient,
-    cache: HubCache,
-    repoID: Repo.ID
-) async throws -> URL {
-    let modelSubdir = repoID.description.replacingOccurrences(of: "/", with: "_")
-    let modelDir = FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first!
-        .appendingPathComponent("mlx-audio")
-        .appendingPathComponent(modelSubdir)
-
-    if FileManager.default.fileExists(atPath: modelDir.path) {
-        let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-        let hasConfig = files?.contains { $0.lastPathComponent == "config.json" } ?? false
-        let hasWeights = files?.contains { $0.pathExtension == "safetensors" } ?? false
-        if hasConfig, hasWeights {
-            return modelDir
-        }
-    }
-
-    try FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
-    _ = try await client.downloadSnapshot(
-        of: repoID,
-        kind: .model,
-        to: modelDir,
-        revision: "main",
-        matching: ["*.json", "*.safetensors", "embeddings/*.safetensors"],
-        progressHandler: { progress in
-            print("\(progress.completedUnitCount)/\(progress.totalUnitCount) files")
-        }
-    )
-    return modelDir
-}
-
 private func loadPocketTTSWeights(modelDir: URL) async throws -> [String: MLXArray] {
     let weightsURL = modelDir.appendingPathComponent("model.safetensors")
     if !FileManager.default.fileExists(atPath: weightsURL.path) {
diff --git a/Sources/MLXAudioTTS/Models/Qwen3/Qwen3.swift b/Sources/MLXAudioTTS/Models/Qwen3/Qwen3.swift
index c5c3d41..64c7986 100644
--- a/Sources/MLXAudioTTS/Models/Qwen3/Qwen3.swift
+++ b/Sources/MLXAudioTTS/Models/Qwen3/Qwen3.swift
@@ -527,12 +527,12 @@ public class Qwen3Model: Module, KVCacheDimensionProvider, SpeechGenerationModel
         return weights
     }
 
-    public func post_load_hook(model: Qwen3Model, modelDir: URL) async throws {
+    public func post_load_hook(model: Qwen3Model, modelDir: URL, cache: HubCache = .default) async throws {
         if model.tokenizer == nil {
             model.tokenizer = try await AutoTokenizer.from(modelFolder: modelDir)
         }
         if model._snacModel == nil {
-            model._snacModel = try await SNAC.fromPretrained("mlx-community/snac_24khz")
+            model._snacModel = try await SNAC.fromPretrained("mlx-community/snac_24khz", cache: cache)
         }
     }
 
@@ -864,30 +864,24 @@ public class Qwen3Model: Module, KVCacheDimensionProvider, SpeechGenerationModel
         return stream
     }
 
-    public static func fromPretrained(_ modelRepo: String) async throws -> Qwen3Model {
+    public static func fromPretrained(
+        _ modelRepo: String,
+        cache: HubCache = .default
+    ) async throws -> Qwen3Model {
         // Check for HF token in environment (macOS) or Info.plist (iOS)
         let hfToken: String? = ProcessInfo.processInfo.environment["HF_TOKEN"]
             ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
 
-        let client: HubClient
-        if let token = hfToken, !token.isEmpty {
-            print("Using HuggingFace token from configuration")
-            client = HubClient(host: HubClient.defaultHost, bearerToken: token)
-        } else {
-            client = HubClient.default
-        }
-        let cache = client.cache ?? HubCache.default
-
         guard let repoID = Repo.ID(rawValue: modelRepo) else {
             throw NSError(domain: "Qwen3Model", code: 1, userInfo: [NSLocalizedDescriptionKey: "Invalid repository ID: \(modelRepo)"])
         }
 
         // Check if model is already fully cached (has weight files)
-        let modelDir = try await resolveOrDownloadModel(
-            client: client,
-            cache: cache,
+        let modelDir = try await ModelUtils.resolveOrDownloadModel(
             repoID: repoID,
-            requiredExtension: "safetensors"
+            requiredExtension: ".safetensors",
+            hfToken: hfToken,
+            cache: cache
         )
 
 
@@ -929,7 +923,7 @@ public class Qwen3Model: Module, KVCacheDimensionProvider, SpeechGenerationModel
         try model.update(parameters: ModuleParameters.unflattened(sanitizedWeights), verify: [.all])
         eval(model)
 
-        try await model.post_load_hook(model: model, modelDir: modelDir)
+        try await model.post_load_hook(model: model, modelDir: modelDir, cache: cache)
 
         return model
     }
@@ -947,61 +941,3 @@ func loadWeights(from directory: URL) throws -> [String: MLXArray] {
     }
     return weights
 }
-
-/// Resolves a model from cache or downloads it if not cached.
-/// - Parameters:
-///   - client: The HuggingFace Hub client
-///   - cache: The HuggingFace cache
-///   - repoID: The repository ID
-///   - requiredExtension: File extension that must exist for cache to be considered complete (e.g., "safetensors")
-/// - Returns: The model directory URL
-func resolveOrDownloadModel(
-    client: HubClient,
-    cache: HubCache,
-    repoID: Repo.ID,
-    requiredExtension: String
-) async throws -> URL {
-    // Use a persistent cache directory based on repo ID
-    let modelSubdir = repoID.description.replacingOccurrences(of: "/", with: "_")
-    let modelDir = FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first!
-        .appendingPathComponent("mlx-audio")
-        .appendingPathComponent(modelSubdir)
-
-    // Check if model already exists with required files
-    if FileManager.default.fileExists(atPath: modelDir.path) {
-        let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-        let hasRequiredFiles = files?.contains { $0.pathExtension == requiredExtension } ?? false
-
-        if hasRequiredFiles {
-            // Validate that config.json is valid JSON
-            let configPath = modelDir.appendingPathComponent("config.json")
-            if FileManager.default.fileExists(atPath: configPath.path) {
-                if let configData = try? Data(contentsOf: configPath),
-                   let _ = try? JSONSerialization.jsonObject(with: configData) {
-                    print("Using cached model at: \(modelDir.path)")
-                    return modelDir
-                } else {
-                    print("Cached config.json is invalid, clearing cache...")
-                    try? FileManager.default.removeItem(at: modelDir)
-                }
-            }
-        }
-    }
-
-    // Create directory if needed
-    try FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
-
-    print("Downloading model \(repoID)...")
-    _ = try await client.downloadSnapshot(
-        of: repoID,
-        kind: .model,
-        to: modelDir,
-        revision: "main",
-        progressHandler: { progress in
-            print("\(progress.completedUnitCount)/\(progress.totalUnitCount) files")
-        }
-    )
-
-    print("Model downloaded to: \(modelDir.path)")
-    return modelDir
-}
diff --git a/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift b/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
index f144b33..7ef7f96 100644
--- a/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
+++ b/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
@@ -719,10 +719,15 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
 
     // MARK: - fromPretrained
 
-    public static func fromPretrained(_ modelRepo: String) async throws -> Qwen3TTSModel {
+    public static func fromPretrained(
+        _ modelRepo: String,
+        cache: HubCache = .default
+    ) async throws -> Qwen3TTSModel {
         let repoID = Repo.ID(rawValue: modelRepo)!
         let modelDir = try await ModelUtils.resolveOrDownloadModel(
-            repoID: repoID, requiredExtension: "safetensors"
+            repoID: repoID,
+            requiredExtension: "safetensors",
+            cache: cache
         )
 
         // Load main config
diff --git a/Sources/MLXAudioTTS/Models/Soprano/Soprano.swift b/Sources/MLXAudioTTS/Models/Soprano/Soprano.swift
index 8340e9e..9d89e8f 100644
--- a/Sources/MLXAudioTTS/Models/Soprano/Soprano.swift
+++ b/Sources/MLXAudioTTS/Models/Soprano/Soprano.swift
@@ -891,28 +891,24 @@ public class SopranoModel: Module, KVCacheDimensionProvider, SpeechGenerationMod
 
     // MARK: - Loading
 
-    public static func fromPretrained(_ modelRepo: String) async throws -> SopranoModel {
+    public static func fromPretrained(
+        _ modelRepo: String,
+        cache: HubCache = .default
+    ) async throws -> SopranoModel {
         let hfToken: String? = ProcessInfo.processInfo.environment["HF_TOKEN"]
             ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
 
-        let client: HubClient
-        if let token = hfToken, !token.isEmpty {
-            client = HubClient(host: HubClient.defaultHost, bearerToken: token)
-        } else {
-            client = HubClient.default
-        }
-        let cache = client.cache ?? HubCache.default
-
         guard let repoID = Repo.ID(rawValue: modelRepo) else {
             throw NSError(domain: "SopranoModel", code: 1, userInfo: [
                 NSLocalizedDescriptionKey: "Invalid repository ID: \(modelRepo)"
             ])
         }
 
-        let modelDir = try await resolveOrDownloadSopranoModel(
-            client: client,
-            cache: cache,
-            repoID: repoID
+        let modelDir = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: ".safetensors",
+            hfToken: hfToken,
+            cache: cache
         )
 
         // Load config
@@ -980,45 +976,6 @@ private func loadSopranoWeights(from directory: URL) throws -> [String: MLXArray
     return weights
 }
 
-private func resolveOrDownloadSopranoModel(
-    client: HubClient,
-    cache: HubCache,
-    repoID: Repo.ID
-) async throws -> URL {
-    let modelSubdir = repoID.description.replacingOccurrences(of: "/", with: "_")
-    let modelDir = FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first!
-        .appendingPathComponent("mlx-audio")
-        .appendingPathComponent(modelSubdir)
-
-    if FileManager.default.fileExists(atPath: modelDir.path) {
-        let files = try? FileManager.default.contentsOfDirectory(at: modelDir, includingPropertiesForKeys: nil)
-        let hasWeights = files?.contains { $0.pathExtension == "safetensors" } ?? false
-
-        if hasWeights {
-            let configPath = modelDir.appendingPathComponent("config.json")
-            if FileManager.default.fileExists(atPath: configPath.path),
-               let configData = try? Data(contentsOf: configPath),
-               (try? JSONSerialization.jsonObject(with: configData)) != nil {
-                return modelDir
-            }
-        }
-    }
-
-    try FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
-
-    _ = try await client.downloadSnapshot(
-        of: repoID,
-        kind: .model,
-        to: modelDir,
-        revision: "main",
-        progressHandler: { progress in
-            print("\(progress.completedUnitCount)/\(progress.totalUnitCount) files")
-        }
-    )
-
-    return modelDir
-}
-
 // MARK: - TopP Sampler (matching Python's mlx_lm implementation)
 
 private struct TopPSampler {
diff --git a/Sources/MLXAudioTTS/TTSModelUtils.swift b/Sources/MLXAudioTTS/TTSModel.swift
similarity index 74%
rename from Sources/MLXAudioTTS/TTSModelUtils.swift
rename to Sources/MLXAudioTTS/TTSModel.swift
index 0bb30bf..e474595 100644
--- a/Sources/MLXAudioTTS/TTSModelUtils.swift
+++ b/Sources/MLXAudioTTS/TTSModel.swift
@@ -2,7 +2,7 @@ import Foundation
 import HuggingFace
 import MLXAudioCore
 
-public enum TTSModelUtilsError: Error, LocalizedError, CustomStringConvertible {
+public enum TTSModelError: Error, LocalizedError, CustomStringConvertible {
     case invalidRepositoryID(String)
     case unsupportedModelType(String?)
 
@@ -20,43 +20,49 @@ public enum TTSModelUtilsError: Error, LocalizedError, CustomStringConvertible {
     }
 }
 
-public enum TTSModelUtils {
+public enum TTS {
     public static func loadModel(
         modelRepo: String,
-        hfToken: String? = nil
+        hfToken: String? = nil,
+        cache: HubCache = .default
     ) async throws -> SpeechGenerationModel {
         guard let repoID = Repo.ID(rawValue: modelRepo) else {
-            throw TTSModelUtilsError.invalidRepositoryID(modelRepo)
+            throw TTSModelError.invalidRepositoryID(modelRepo)
         }
 
-        let modelType = try await ModelUtils.resolveModelType(repoID: repoID, hfToken: hfToken)
-        return try await loadModel(modelRepo: modelRepo, modelType: modelType)
+        let modelType = try await ModelUtils.resolveModelType(
+            repoID: repoID,
+            hfToken: hfToken,
+            cache: cache
+        )
+        return try await loadModel(modelRepo: modelRepo, modelType: modelType, cache: cache)
     }
 
     public static func loadModel(
         modelRepo: String,
-        modelType: String?
+        modelType: String?,
+        cache: HubCache = .default
     ) async throws -> SpeechGenerationModel {
         let resolvedType = normalizedModelType(modelType) ?? inferModelType(from: modelRepo)
         guard let resolvedType else {
-            throw TTSModelUtilsError.unsupportedModelType(modelType)
+            throw TTSModelError.unsupportedModelType(modelType)
         }
 
         switch resolvedType {
         case "qwen3_tts":
-            return try await Qwen3TTSModel.fromPretrained(modelRepo)
+            return try await Qwen3TTSModel.fromPretrained(modelRepo, cache: cache)
         case "qwen3", "qwen":
-            return try await Qwen3Model.fromPretrained(modelRepo)
+            return try await Qwen3Model.fromPretrained(modelRepo, cache: cache)
         case "llama_tts", "llama3_tts", "llama3", "llama", "orpheus", "orpheus_tts":
-            return try await LlamaTTSModel.fromPretrained(modelRepo)
+            return try await LlamaTTSModel.fromPretrained(modelRepo, cache: cache)
         case "csm", "sesame":
-            return try await MarvisTTSModel.fromPretrained(modelRepo)
+            return try await MarvisTTSModel.fromPretrained(modelRepo, cache: cache)
         case "soprano_tts", "soprano":
-            return try await SopranoModel.fromPretrained(modelRepo)
+            return try await SopranoModel.fromPretrained(modelRepo, cache: cache)
         case "pocket_tts":
-            return try await PocketTTSModel.fromPretrained(modelRepo)
+            return try await PocketTTSModel.fromPretrained(modelRepo, cache: cache)
         default:
-            throw TTSModelUtilsError.unsupportedModelType(modelType ?? resolvedType)
+            throw TTSModelError.unsupportedModelType(modelType ?? resolvedType)
         }
     }
 
@@ -90,3 +96,9 @@ public enum TTSModelUtils {
         return nil
     }
 }
+
+@available(*, deprecated, renamed: "TTSModelError")
+public typealias TTSModelUtilsError = TTSModelError
+
+@available(*, deprecated, renamed: "TTS")
+public typealias TTSModelUtils = TTS
diff --git a/Sources/MLXAudioVAD/Models/Sortformer/Sortformer.swift b/Sources/MLXAudioVAD/Models/Sortformer/Sortformer.swift
index ddf04a5..147025d 100644
--- a/Sources/MLXAudioVAD/Models/Sortformer/Sortformer.swift
+++ b/Sources/MLXAudioVAD/Models/Sortformer/Sortformer.swift
@@ -3,7 +3,7 @@ import MLX
 import MLXAudioCore
 import MLXNN
 import MLXLMCommon
-import Hub
+import HuggingFace
 
 private struct UncheckedSendableBox<T>: @unchecked Sendable {
     let value: T
@@ -1385,11 +1385,23 @@ public class SortformerModel: Module {
     }
 
     /// Load model from a HuggingFace repository.
-    public static func fromPretrained(_ repoId: String) async throws -> SortformerModel {
-        let hub = HubApi()
-        let repo = Hub.Repo(id: repoId)
+    public static func fromPretrained(
+        _ repoId: String,
+        cache: HubCache = .default
+    ) async throws -> SortformerModel {
+        guard let repoID = Repo.ID(rawValue: repoId) else {
+            throw NSError(
+                domain: "SortformerModel",
+                code: 1,
+                userInfo: [NSLocalizedDescriptionKey: "Invalid repository ID: \(repoId)"]
+            )
+        }
 
-        let modelURL = try await hub.snapshot(from: repo, matching: ["*.json", "*.safetensors"])
+        let modelURL = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: ".safetensors",
+            cache: cache
+        )
 
         // Load config
         let configURL = modelURL.appendingPathComponent("config.json")
diff --git a/Sources/Tools/mlx-audio-swift-tts/App.swift b/Sources/Tools/mlx-audio-swift-tts/App.swift
index 42b303e..6dafd02 100644
--- a/Sources/Tools/mlx-audio-swift-tts/App.swift
+++ b/Sources/Tools/mlx-audio-swift-tts/App.swift
@@ -73,8 +73,8 @@ enum App {
 
         let loadedModel: SpeechGenerationModel
         do {
-            loadedModel = try await TTSModelUtils.loadModel(modelRepo: model, hfToken: hfToken)
-        } catch let error as TTSModelUtilsError {
+            loadedModel = try await TTS.loadModel(modelRepo: model, hfToken: hfToken)
+        } catch let error as TTSModelError {
             switch error {
             case .invalidRepositoryID(let modelRepo):
                 throw AppError.invalidRepositoryID(modelRepo)

commit 711a9b745db32504876b56d083eeb0f12c8c3264
Author: Ken Woo <ikenwoo@gmail.com>
Date:   Sat Feb 21 09:04:30 2026 -0800

    Fix Voices App (#59)
    
    Co-authored-by: Prince Canuma <prince.gdt@gmail.com>

diff --git a/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift b/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
index 55f726d..4c7522c 100644
--- a/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
+++ b/Examples/VoicesApp/VoicesApp/ViewModels/TTSViewModel.swift
@@ -31,6 +31,7 @@ class TTSViewModel {
     private var maxTokensOverride: Int?
     private var temperatureOverride: Float?
     private var topPOverride: Float?
+    private var repetitionPenaltyOverride: Float?
 
     var maxTokens: Int {
         get { maxTokensOverride ?? defaultMaxTokens }
@@ -54,7 +55,15 @@ class TTSViewModel {
             topPOverride = abs(newValue - defaultValue) < 0.0001 ? nil : newValue
         }
     }
-
+    
+    var repetitionPenalty: Float {
+        get { repetitionPenaltyOverride ?? defaultGenerationParameters.repetitionPenalty ?? 1.3 }
+        set {
+            let defaultValue = defaultGenerationParameters.repetitionPenalty ?? 1.3
+            repetitionPenaltyOverride = abs(newValue - defaultValue) < 0.0001 ? nil : newValue
+        }
+    }
+    
     // Voice Design (for Qwen3-TTS VoiceDesign models)
     var voiceDescription: String = ""
     var useVoiceDesign: Bool = false
@@ -325,8 +334,8 @@ class TTSViewModel {
                     voice: voiceParam,
                     refAudio: refAudio,
                     refText: refText,
-                    cache: nil,
-                    parameters: generationParameters
+                    language: nil,
+                    generationParameters: generationParameters
                 ) {
                     // Throw if cancelled - this will exit the loop and be caught below
                     try Task.checkCancellation()

commit 64ac41e17e642fec49711738e1442d04bcf8f41b
Author: Lucas Newman <lucas@future.fit>
Date:   Fri Feb 20 11:16:27 2026 -0800

    Fix quantization for Qwen3 TTS models. (#61)

diff --git a/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift b/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
index c28635a..f144b33 100644
--- a/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
+++ b/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTS.swift
@@ -743,6 +743,24 @@ public final class Qwen3TTSModel: Module, SpeechGenerationModel, @unchecked Send
         // Sanitize and load talker weights
         let talkerWeights = Qwen3TTSTalkerForConditionalGeneration.sanitize(weights: allWeights)
         let talkerPairs = talkerWeights.map { ($0.key, $0.value) }
+
+        // Quantized checkpoints store packed weights and companion .scales tensors.
+        // Convert talker Linear layers before loading those tensors.
+        if config.quantization != nil || config.perLayerQuantization != nil {
+            quantize(model: model.talker) { path, _ in
+                guard talkerWeights["\(path).scales"] != nil else {
+                    return nil
+                }
+
+                if let perLayerQuant = config.perLayerQuantization,
+                   let layerQuant = perLayerQuant.quantization(layer: path) {
+                    return layerQuant.asTuple
+                }
+
+                return config.quantization?.asTuple
+            }
+        }
+
         try model.talker.update(parameters: ModuleParameters.unflattened(talkerPairs), verify: .all)
         eval(model.talker.parameters())
 
diff --git a/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTSConfig.swift b/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTSConfig.swift
index 50ee8dc..6245ac2 100644
--- a/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTSConfig.swift
+++ b/Sources/MLXAudioTTS/Models/Qwen3TTS/Qwen3TTSConfig.swift
@@ -447,11 +447,13 @@ public struct Qwen3TTSTokenizerConfig: Codable, Sendable {
 
 // MARK: - Top-level Model Config
 
-public struct Qwen3TTSModelConfig: Codable, Sendable {
+public struct Qwen3TTSModelConfig: Decodable, Sendable {
     var modelType: String
     var talkerConfig: Qwen3TTSTalkerConfig?
     var speakerEncoderConfig: Qwen3TTSSpeakerEncoderConfig
     var tokenizerConfig: Qwen3TTSTokenizerConfig?
+    var quantization: BaseConfiguration.Quantization?
+    var perLayerQuantization: BaseConfiguration.PerLayerQuantization?
     var tokenizerType: String
     var ttsModelSize: String
     var ttsModelType: String
@@ -467,6 +469,8 @@ public struct Qwen3TTSModelConfig: Codable, Sendable {
         case talkerConfig = "talker_config"
         case speakerEncoderConfig = "speaker_encoder_config"
         case tokenizerConfig = "tokenizer_config"
+        case quantization
+        case quantizationConfig = "quantization_config"
         case tokenizerType = "tokenizer_type"
         case ttsModelSize = "tts_model_size"
         case ttsModelType = "tts_model_type"
@@ -480,11 +484,16 @@ public struct Qwen3TTSModelConfig: Codable, Sendable {
 
     public init(from decoder: Swift.Decoder) throws {
         let c = try decoder.container(keyedBy: CodingKeys.self)
+        let baseConfig = try? BaseConfiguration(from: decoder)
         modelType = try c.decodeIfPresent(String.self, forKey: .modelType) ?? "qwen3_tts"
         talkerConfig = try c.decodeIfPresent(Qwen3TTSTalkerConfig.self, forKey: .talkerConfig)
         speakerEncoderConfig = try c.decodeIfPresent(Qwen3TTSSpeakerEncoderConfig.self, forKey: .speakerEncoderConfig)
             ?? Qwen3TTSSpeakerEncoderConfig()
         tokenizerConfig = try c.decodeIfPresent(Qwen3TTSTokenizerConfig.self, forKey: .tokenizerConfig)
+        let globalQuant = try c.decodeIfPresent(BaseConfiguration.Quantization.self, forKey: .quantization)
+        let altGlobalQuant = try c.decodeIfPresent(BaseConfiguration.Quantization.self, forKey: .quantizationConfig)
+        quantization = globalQuant ?? altGlobalQuant
+        perLayerQuantization = baseConfig?.perLayerQuantization
         tokenizerType = try c.decodeIfPresent(String.self, forKey: .tokenizerType) ?? "qwen3_tts_tokenizer_12hz"
         ttsModelSize = try c.decodeIfPresent(String.self, forKey: .ttsModelSize) ?? "0b6"
         ttsModelType = try c.decodeIfPresent(String.self, forKey: .ttsModelType) ?? "base"

commit 3ebaac3daeb2adfdde4d9d716978f6dd36dd1bc8
Author: Prince Canuma <prince.gdt@gmail.com>
Date:   Fri Feb 20 17:55:48 2026 +0100

    Refactor sts app (#56)

diff --git a/README.md b/README.md
index 65f26c1..63e484c 100644
--- a/README.md
+++ b/README.md
@@ -15,7 +15,7 @@ MLXAudio follows a modular design allowing you to import only what you need:
 - **MLXAudioTTS**: Text-to-Speech models (Soprano, VyvoTTS, Orpheus, Marvis TTS, Pocket TTS)
 - **MLXAudioSTT**: Speech-to-Text models (GLMASR)
 - **MLXAudioVAD**: Voice Activity Detection & Speaker Diarization (Sortformer)
-- **MLXAudioSTS**: Speech-to-Speech (future)
+- **MLXAudioSTS**: Speech-to-Speech models (LFM2.5-Audio)
 - **MLXAudioUI**: SwiftUI components for audio interfaces
 
 ## Installation
@@ -128,6 +128,12 @@ for try await event in model.generateStream(text: text, parameters: parameters)
 |-------|--------------|------------------|
 | GLMASR | [GLMASR README](Sources/MLXAudioSTT/Models/GLMASR/README.md) | [mlx-community/GLM-ASR-Nano-2512-4bit](https://huggingface.co/mlx-community/GLM-ASR-Nano-2512-4bit) |
 
+### STS Models
+
+| Model | Model README | HuggingFace Repo |
+|-------|--------------|------------------|
+| LFM2.5-Audio | [LFM Audio README](Sources/MLXAudioSTS/Models/LFMAudio/README.md) | [mlx-community/LFM2.5-Audio-1.5B-6bit](https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-6bit) |
+
 ### VAD / Speaker Diarization Models
 
 | Model | Model README | HuggingFace Repo |
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift b/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift
index 3476517..b87ea3e 100644
--- a/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift
@@ -151,7 +151,7 @@ class AudioHead: Module {
 
 // MARK: - LFM2 Audio Model
 
-public class LFM2AudioModel: Module {
+public class LFM2AudioModel: Module, STSModel, @unchecked Sendable {
     public let config: LFM2AudioConfig
     public var processor: LFM2AudioProcessor?
     public var modelDirectory: URL?
@@ -225,6 +225,7 @@ public class LFM2AudioModel: Module {
         modalities: MLXArray? = nil,
         cache: [KVCache]? = nil
     ) -> (MLXArray, [KVCache]) {
+
         let inputEmbeddings: MLXArray
 
         if let modalities = modalities {
@@ -448,10 +449,13 @@ public class LFM2AudioModel: Module {
                         let eosFrame = MLX.full(audioFrame.shape, values: MLXArray(Int32(lfmAudioEOSToken)), type: Int32.self)
                         continuation.yield((eosFrame.squeezed(axis: 0), .audioOut))
 
+                        // Embed EOS back into the model
+                        let nextEmb = embedAudioOut(eosFrame).expandedDimensions(axis: 1)
+                        lastHidden = lfm(inputEmbeddings: nextEmb, cache: cache)
+
                         generated += 1
                         currentModality = .text
                         if textDone { break }
-                        modalityLeft = nText
                         continue
                     }
 
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/README.md b/Sources/MLXAudioSTS/Models/LFMAudio/README.md
new file mode 100644
index 0000000..d5bc4a0
--- /dev/null
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/README.md
@@ -0,0 +1,345 @@
+# LFM2.5-Audio for MLX Swift
+
+MLX Swift implementation of [LiquidAI's LFM2.5-Audio](https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B), a multimodal foundation model for audio understanding and generation.
+
+## Features
+
+- **Text-to-Speech (TTS)**: Generate natural speech from text
+- **Speech-to-Text (STT)**: Transcribe audio to text
+- **Speech-to-Speech (STS)**: Voice conversations with audio input and output
+- **Text-to-Text (T2T)**: Standard text chat
+- **Interleaved Generation**: Mixed text and audio responses in a single turn
+- **Streaming**: Real-time token-by-token generation via `AsyncThrowingStream`
+
+## Quick Start
+
+### Text-to-Speech (TTS)
+
+```swift
+import MLXAudioSTS
+import MLX
+
+// Load model (downloads from HuggingFace Hub)
+let model = try await LFM2AudioModel.fromPretrained("mlx-community/LFM2.5-Audio-1.5B-6bit")
+let processor = model.processor!
+
+// Build chat prompt
+let chat = ChatState(processor: processor)
+chat.newTurn(role: "system")
+chat.addText("Perform TTS. Use a UK male voice.")
+chat.endTurn()
+chat.newTurn(role: "user")
+chat.addText("Hello, welcome to MLX Audio Swift!")
+chat.endTurn()
+chat.newTurn(role: "assistant")
+chat.addAudioStartToken()
+
+// Generate audio sequentially
+let genConfig = LFMGenerationConfig(
+    maxNewTokens: 2048,
+    audioTemperature: 0.8,
+    audioTopK: 4
+)
+
+var audioCodes: [MLXArray] = []
+for try await (token, modality) in model.generateSequential(
+    textTokens: chat.getTextTokens(),
+    audioFeatures: chat.getAudioFeatures(),
+    modalities: chat.getModalities(),
+    config: genConfig
+) {
+    eval(token)
+    if modality == .audioOut {
+        if token[0].item(Int.self) == lfmAudioEOSToken { break }
+        audioCodes.append(token)
+    }
+}
+
+// Decode audio with the detokenizer
+let stacked = MLX.stacked(audioCodes, axis: 0)
+let codesInput = stacked.transposed(1, 0).expandedDimensions(axis: 0)
+let detokenizer = try LFM2AudioDetokenizer.fromPretrained(modelPath: model.modelDirectory!)
+let waveform = detokenizer(codesInput) // 24kHz sample rate
+```
+
+### Speech-to-Text (STT)
+
+```swift
+import MLXAudioSTS
+import MLX
+
+let model = try await LFM2AudioModel.fromPretrained("mlx-community/LFM2.5-Audio-1.5B-6bit")
+let processor = model.processor!
+
+// Load audio as MLXArray (16kHz for input)
+let audioData: MLXArray = ...
+
+let chat = ChatState(processor: processor)
+chat.newTurn(role: "system")
+chat.addText("You are a helpful assistant that transcribes audio.")
+chat.endTurn()
+chat.newTurn(role: "user")
+chat.addAudio(audioData, sampleRate: 16000)
+chat.addText("Transcribe the audio.")
+chat.endTurn()
+chat.newTurn(role: "assistant")
+
+var textTokens: [Int] = []
+for try await (token, modality) in model.generateInterleaved(
+    textTokens: chat.getTextTokens(),
+    audioFeatures: chat.getAudioFeatures(),
+    modalities: chat.getModalities()
+) {
+    eval(token)
+    if modality == .text {
+        textTokens.append(token.item(Int.self))
+    }
+}
+
+let transcription = processor.decodeText(textTokens)
+```
+
+### Speech-to-Speech (STS)
+
+```swift
+import MLXAudioSTS
+import MLX
+
+let model = try await LFM2AudioModel.fromPretrained("mlx-community/LFM2.5-Audio-1.5B-6bit")
+let processor = model.processor!
+
+let audioData: MLXArray = ... // 16kHz input audio
+
+let chat = ChatState(processor: processor)
+chat.newTurn(role: "system")
+chat.addText("Respond with interleaved text and speech audio. Use a UK male voice.")
+chat.endTurn()
+chat.newTurn(role: "user")
+chat.addAudio(audioData, sampleRate: 16000)
+chat.endTurn()
+chat.newTurn(role: "assistant")
+
+let genConfig = LFMGenerationConfig(maxNewTokens: 2048)
+
+var textTokens: [Int] = []
+var audioCodes: [MLXArray] = []
+
+for try await (token, modality) in model.generateInterleaved(
+    textTokens: chat.getTextTokens(),
+    audioFeatures: chat.getAudioFeatures(),
+    modalities: chat.getModalities(),
+    config: genConfig
+) {
+    eval(token)
+    if modality == .text {
+        textTokens.append(token.item(Int.self))
+    } else if modality == .audioOut {
+        if token[0].item(Int.self) != lfmAudioEOSToken {
+            audioCodes.append(token)
+        }
+    }
+}
+
+let text = processor.decodeText(textTokens)
+
+// Decode audio
+let stacked = MLX.stacked(audioCodes, axis: 0)
+let codesInput = stacked.transposed(1, 0).expandedDimensions(axis: 0)
+let detokenizer = try LFM2AudioDetokenizer.fromPretrained(modelPath: model.modelDirectory!)
+let waveform = detokenizer(codesInput)
+```
+
+## Generation Modes
+
+### Interleaved (`generateInterleaved`)
+
+Produces alternating text and audio tokens. The model alternates between generating `interleavedNText` (default 6) text tokens and `interleavedNAudio` (default 12) audio tokens per cycle. Best for STT, STS, and T2T.
+
+Each audio token is a frame of shape `(8,)` containing all 8 codebook values.
+
+### Sequential (`generateSequential`)
+
+Generates all text first, then all audio. Best for TTS where the model transitions to audio mode after emitting the audio start token (128).
+
+Requires calling `chat.addAudioStartToken()` before generation to trigger audio output.
+
+## Generation Configuration
+
+```swift
+let config = LFMGenerationConfig(
+    maxNewTokens: 2048,      // Maximum tokens to generate
+    temperature: 0.7,         // Text sampling temperature
+    topK: 50,                 // Text top-K sampling
+    topP: 1.0,                // Text nucleus sampling
+    audioTemperature: 0.8,    // Audio sampling temperature
+    audioTopK: 4              // Audio top-K sampling
+)
+```
+
+## Audio Decoding
+
+LFM2.5-Audio generates 8-codebook audio tokens that must be decoded to waveforms using the neural detokenizer (ISTFT-based vocoder):
+
+```swift
+// Stack audio frames: list of (8,) -> (T, 8) -> (8, T) -> (1, 8, T)
+let stacked = MLX.stacked(audioCodes, axis: 0)
+let codesInput = stacked.transposed(1, 0).expandedDimensions(axis: 0)
+
+// Decode using detokenizer
+let detokenizer = try LFM2AudioDetokenizer.fromPretrained(modelPath: model.modelDirectory!)
+let waveform = detokenizer(codesInput)  // (1, T_audio) at 24kHz
+```
+
+## CLI Tool
+
+The `mlx-audio-swift-sts` CLI tool provides a command-line interface:
+
+```bash
+# Text-to-Speech
+swift run mlx-audio-swift-sts \
+    --model mlx-community/LFM2.5-Audio-1.5B-6bit \
+    --mode tts \
+    --text "Hello, welcome to MLX Audio!" \
+    -o output.wav
+
+# Speech-to-Text
+swift run mlx-audio-swift-sts \
+    --model mlx-community/LFM2.5-Audio-1.5B-6bit \
+    --mode stt \
+    --audio input.wav \
+    --stream
+
+# Speech-to-Speech
+swift run mlx-audio-swift-sts \
+    --model mlx-community/LFM2.5-Audio-1.5B-6bit \
+    --mode sts \
+    --audio input.wav \
+    --stream \
+    -o response.wav
+
+# Text-to-Text
+swift run mlx-audio-swift-sts \
+    --model mlx-community/LFM2.5-Audio-1.5B-6bit \
+    --mode t2t \
+    --text "What is machine learning?" \
+    --stream
+```
+
+### CLI Options
+
+| Option | Description | Default |
+|---|---|---|
+| `--model <repo>` | HuggingFace model repo | - |
+| `--mode <t2t\|tts\|stt\|sts>` | Generation mode | `sts` |
+| `-t, --text <string>` | Input text | - |
+| `-i, --audio <path>` | Input audio file | - |
+| `--system <string>` | System prompt | Per-mode default |
+| `--max-new-tokens <int>` | Max tokens | `512` |
+| `--temperature <float>` | Text temperature | `0.7` |
+| `--top-k <int>` | Text top-K | `50` |
+| `--audio-temperature <float>` | Audio temperature | `0.8` |
+| `--audio-top-k <int>` | Audio top-K | `4` |
+| `--stream` | Stream text output | `false` |
+| `-o, --output-target <path>` | Audio output path | `lfm_output.wav` |
+| `--output-text <path>` | Text output path | - |
+
+## Model Architecture
+
+LFM2.5-Audio consists of:
+
+- **Audio Encoder**: Conformer-based encoder (17 layers, 512d) for processing 16kHz input audio
+- **Audio Adapter**: MLP projecting encoder output to backbone dimensions
+- **LFM Backbone**: 1.5B parameter Liquid Foundation Model (16 layers, mix of conv and attention) for multimodal reasoning
+- **Audio Head**: Depthformer (6 layers) for generating 8-codebook audio tokens
+- **Detokenizer**: ISTFT-based neural vocoder for reconstructing 24kHz waveforms from audio codes
+
+## API Reference
+
+### LFM2AudioModel
+
+```swift
+public class LFM2AudioModel: Module, STSModel {
+    /// Load pretrained model from HuggingFace Hub
+    public static func fromPretrained(_ modelNameOrPath: String) async throws -> LFM2AudioModel
+
+    /// Generate interleaved text and audio tokens
+    /// Yields (token, modality) tuples:
+    ///   - TEXT: token is scalar Int
+    ///   - AUDIO_OUT: token is (8,) array with codebook values
+    public func generateInterleaved(
+        textTokens: MLXArray?, audioFeatures: MLXArray?,
+        audioCodes: MLXArray?, modalities: MLXArray?,
+        config: LFMGenerationConfig
+    ) -> AsyncThrowingStream<(MLXArray, LFMModality), Error>
+
+    /// Generate text then audio sequentially (best for TTS)
+    public func generateSequential(
+        textTokens: MLXArray?, audioFeatures: MLXArray?,
+        audioCodes: MLXArray?, modalities: MLXArray?,
+        config: LFMGenerationConfig
+    ) -> AsyncThrowingStream<(MLXArray, LFMModality), Error>
+
+    /// Output sample rate (24kHz)
+    public var sampleRate: Int { get }
+}
+```
+
+### ChatState
+
+```swift
+public class ChatState {
+    public init(processor: LFM2AudioProcessor, addBos: Bool = true)
+
+    public func newTurn(role: String)        // Start turn: "system", "user", or "assistant"
+    public func endTurn()                     // End current turn
+    public func addText(_ text: String)       // Add text to current turn
+    public func addAudio(_ audio: MLXArray, sampleRate: Int = 16000) // Add audio input
+    public func addAudioStartToken()          // Required before TTS sequential generation
+
+    public func getTextTokens() -> MLXArray   // (1, T) Int32
+    public func getAudioFeatures() -> MLXArray? // (1, frames, features)
+    public func getModalities() -> MLXArray   // (1, T) Int32
+}
+```
+
+### LFM2AudioProcessor
+
+```swift
+public class LFM2AudioProcessor {
+    public func tokenize(_ text: String) -> [Int]
+    public func decodeText(_ tokens: [Int]) -> String
+    public func preprocessAudio(_ audio: MLXArray, sampleRate: Int) -> MLXArray
+}
+```
+
+### LFMModality
+
+```swift
+public enum LFMModality: Int {
+    case text = 1
+    case audioIn = 2
+    case audioOut = 3
+}
+```
+
+## Supported Models
+
+| Model | Size | Quantization |
+|---|---|---|
+| `mlx-community/LFM2.5-Audio-1.5B-bf16` | ~3 GB |  bfloat16 |
+| `mlx-community/LFM2.5-Audio-1.5B-8bit` | ~1.7 GB | 8-bit |
+| `mlx-community/LFM2.5-Audio-1.5B-6bit` | ~1.3 GB | 6-bit |
+| `mlx-community/LFM2.5-Audio-1.5B-5bit` | ~1.1 GB | 5-bit |
+| `mlx-community/LFM2.5-Audio-1.5B-4bit` | ~0.9 GB | 4-bit |
+
+
+## License
+
+This implementation follows the license terms of the original LFM2.5-Audio model.
+See [LiquidAI/LFM2.5-Audio-1.5B](https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B) for details.
+
+## Acknowledgements
+
+- [LiquidAI](https://liquid.ai/) for the LFM2.5-Audio model
+- [MLX](https://github.com/ml-explore/mlx) team for the framework
+- [mlx-audio](https://github.com/Blaizzy/mlx-audio) for the Python reference implementation
diff --git a/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift b/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
index 098c6cd..c95eaf8 100644
--- a/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
+++ b/Sources/MLXAudioSTS/Models/MossFormer2SE/MossFormer2Model.swift
@@ -252,11 +252,12 @@ public enum MossFormer2SEError: Error, LocalizedError {
     }
 }
 
-public final class MossFormer2SEModel {
+public final class MossFormer2SEModel: STSModel {
     public static let defaultRepo = "starkdmi/MossFormer2-SE-fp16"
 
     public let model: MossFormer2SE
     public let config: MossFormer2SEConfig
+    public var sampleRate: Int { config.sampleRate }
 
     public init(model: MossFormer2SE, config: MossFormer2SEConfig) {
         self.model = model
diff --git a/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudio.swift b/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudio.swift
index d87f6fd..0c06f98 100644
--- a/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudio.swift
+++ b/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudio.swift
@@ -47,7 +47,7 @@ public enum SAMAudioError: Error, LocalizedError {
     }
 }
 
-public final class SAMAudio: Module, @unchecked Sendable {
+public final class SAMAudio: Module, STSModel, @unchecked Sendable {
     public let config: SAMAudioConfig
     public let textEncoder: T5TextEncoder
     public let processor: SAMAudioProcessor
diff --git a/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudioTextEncoder.swift b/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudioTextEncoder.swift
index 4658d9b..b3f87f9 100644
--- a/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudioTextEncoder.swift
+++ b/Sources/MLXAudioSTS/Models/SAMAudio/SAMAudioTextEncoder.swift
@@ -45,8 +45,8 @@ private struct T5ModelConfig: Codable {
         relativeAttentionMaxDistance: Int = 128,
         dropoutRate: Float = 0.1,
         layerNormEpsilon: Float = 1e-6,
-        isGatedAct: Bool = true,
-        denseActFn: String = "gelu_new"
+        isGatedAct: Bool = false,
+        denseActFn: String = "relu"
     ) {
         self.vocabSize = vocabSize
         self.dModel = dModel
@@ -74,8 +74,8 @@ private struct T5ModelConfig: Codable {
         relativeAttentionMaxDistance = try c.decodeIfPresent(Int.self, forKey: .relativeAttentionMaxDistance) ?? 128
         dropoutRate = try c.decodeIfPresent(Float.self, forKey: .dropoutRate) ?? 0.1
         layerNormEpsilon = try c.decodeIfPresent(Float.self, forKey: .layerNormEpsilon) ?? 1e-6
-        isGatedAct = try c.decodeIfPresent(Bool.self, forKey: .isGatedAct) ?? true
-        denseActFn = try c.decodeIfPresent(String.self, forKey: .denseActFn) ?? "gelu_new"
+        isGatedAct = try c.decodeIfPresent(Bool.self, forKey: .isGatedAct) ?? false
+        denseActFn = try c.decodeIfPresent(String.self, forKey: .denseActFn) ?? "relu"
     }
 }
 
diff --git a/Sources/MLXAudioSTS/STSModel.swift b/Sources/MLXAudioSTS/STSModel.swift
new file mode 100644
index 0000000..1962b97
--- /dev/null
+++ b/Sources/MLXAudioSTS/STSModel.swift
@@ -0,0 +1,121 @@
+import Foundation
+import HuggingFace
+import MLX
+import MLXAudioCore
+
+// MARK: - STSModel Protocol
+
+public protocol STSModel: AnyObject {
+    var sampleRate: Int { get }
+}
+
+// MARK: - Loaded Model Container
+
+public enum LoadedSTSModel {
+    case samAudio(SAMAudio)
+    case lfmAudio(LFM2AudioModel)
+    case mossFormer2SE(MossFormer2SEModel)
+
+    public var model: any STSModel {
+        switch self {
+        case .samAudio(let m): return m
+        case .lfmAudio(let m): return m
+        case .mossFormer2SE(let m): return m
+        }
+    }
+
+    public var sampleRate: Int { model.sampleRate }
+}
+
+// MARK: - Errors
+
+public enum STSModelError: Error, LocalizedError, CustomStringConvertible {
+    case invalidRepositoryID(String)
+    case unsupportedModelType(String?)
+
+    public var errorDescription: String? { description }
+
+    public var description: String {
+        switch self {
+        case .invalidRepositoryID(let repo):
+            return "Invalid repository ID: \(repo)"
+        case .unsupportedModelType(let modelType):
+            return "Unsupported STS model type: \(String(describing: modelType))"
+        }
+    }
+}
+
+// MARK: - Factory
+
+public enum STS {
+
+    public static func loadModel(
+        modelRepo: String,
+        hfToken: String? = nil,
+        strict: Bool = false
+    ) async throws -> LoadedSTSModel {
+        guard let repoID = Repo.ID(rawValue: modelRepo) else {
+            throw STSModelError.invalidRepositoryID(modelRepo)
+        }
+
+        let modelType = try await ModelUtils.resolveModelType(repoID: repoID, hfToken: hfToken)
+        return try await loadModel(
+            modelRepo: modelRepo,
+            modelType: modelType,
+            hfToken: hfToken,
+            strict: strict
+        )
+    }
+
+    public static func loadModel(
+        modelRepo: String,
+        modelType: String?,
+        hfToken: String? = nil,
+        strict: Bool = false
+    ) async throws -> LoadedSTSModel {
+        let resolved = normalizedModelType(modelType) ?? inferModelType(from: modelRepo)
+        guard let resolved else {
+            throw STSModelError.unsupportedModelType(modelType)
+        }
+
+        switch resolved {
+        case "lfm_audio", "lfm", "lfm2", "lfm2_audio":
+            let model = try await LFM2AudioModel.fromPretrained(modelRepo)
+            return .lfmAudio(model)
+
+        case "sam_audio", "sam", "samaudio":
+            let model = try await SAMAudio.fromPretrained(modelRepo, hfToken: hfToken, strict: strict)
+            return .samAudio(model)
+
+        case "mossformer2_se", "mossformer2", "mossformer":
+            let model = try await MossFormer2SEModel.fromPretrained(modelRepo)
+            return .mossFormer2SE(model)
+
+        default:
+            throw STSModelError.unsupportedModelType(resolved)
+        }
+    }
+
+    // MARK: - Private
+
+    private static func normalizedModelType(_ modelType: String?) -> String? {
+        guard let modelType else { return nil }
+        let trimmed = modelType.trimmingCharacters(in: .whitespacesAndNewlines)
+        guard !trimmed.isEmpty else { return nil }
+        return trimmed.lowercased()
+    }
+
+    private static func inferModelType(from modelRepo: String) -> String? {
+        let lower = modelRepo.lowercased()
+        if lower.contains("lfm") {
+            return "lfm_audio"
+        }
+        if lower.contains("mossformer") {
+            return "mossformer2_se"
+        }
+        if lower.contains("sam") || lower.contains("source-separation") {
+            return "sam_audio"
+        }
+        return nil
+    }
+}
diff --git a/Sources/Tools/mlx-audio-swift-sts/App.swift b/Sources/Tools/mlx-audio-swift-sts/App.swift
index 201ad24..6136774 100644
--- a/Sources/Tools/mlx-audio-swift-sts/App.swift
+++ b/Sources/Tools/mlx-audio-swift-sts/App.swift
@@ -9,9 +9,10 @@ enum AppError: Error, LocalizedError, CustomStringConvertible {
     case anchorsUnsupportedForMode(SeparationMode)
     case failedToCreateAudioBuffer
     case failedToAccessAudioBufferData
-    case unsupportedModelRepo(String)
     case lfmRequiresText
     case lfmRequiresAudioForMode(LFMMode)
+    case enhanceRequiresAudio
+    case audioResampleFailed(String)
 
     var errorDescription: String? { description }
 
@@ -25,12 +26,14 @@ enum AppError: Error, LocalizedError, CustomStringConvertible {
             "Failed to create audio buffer"
         case .failedToAccessAudioBufferData:
             "Failed to access audio buffer data"
-        case .unsupportedModelRepo(let repo):
-            "Unsupported STS model repo: \(repo). Expected SAMAudio or LFM model."
         case .lfmRequiresText:
             "--text is required for LFM text-to-text and text-to-speech modes."
         case .lfmRequiresAudioForMode(let mode):
             "--audio is required for LFM \(mode.rawValue) mode."
+        case .enhanceRequiresAudio:
+            "--audio is required for speech enhancement."
+        case .audioResampleFailed(let message):
+            "Failed to resample audio: \(message)"
         }
     }
 }
@@ -54,10 +57,24 @@ enum App {
         do {
             let args = try CLI.parse()
 
-            if isLFMModel(args.model) {
-                try await runLFM(args: args)
-            } else {
-                try await runSAMAudio(args: args)
+            let hfToken = args.hfToken
+                ?? ProcessInfo.processInfo.environment["HF_TOKEN"]
+                ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
+
+            print("Loading model (\(args.model))")
+            let loaded = try await STS.loadModel(
+                modelRepo: args.model,
+                hfToken: hfToken,
+                strict: args.strict
+            )
+
+            switch loaded {
+            case .lfmAudio(let model):
+                try await runLFM(model: model, args: args)
+            case .samAudio(let model):
+                try await runSAMAudio(model: model, args: args)
+            case .mossFormer2SE(let model):
+                try await runMossFormer2SE(model: model, args: args)
             }
         } catch {
             fputs("Error: \(error)\n", stderr)
@@ -66,14 +83,9 @@ enum App {
         }
     }
 
-    private static func isLFMModel(_ model: String) -> Bool {
-        let lower = model.lowercased()
-        return lower.contains("lfm") || lower.contains("lfm2")
-    }
-
     // MARK: - LFM2.5-Audio
 
-    private static func runLFM(args: CLI) async throws {
+    private static func runLFM(model: LFM2AudioModel, args: CLI) async throws {
         let lfmMode = args.lfmMode ?? .sts
 
         switch lfmMode {
@@ -87,10 +99,7 @@ enum App {
             }
         }
 
-        print("Loading LFM2.5-Audio model (\(args.model))")
-        let model = try await LFM2AudioModel.fromPretrained(args.model)
         let processor = model.processor!
-
         let chat = ChatState(processor: processor)
 
         let defaultSystemPrompts: [LFMMode: String] = [
@@ -119,7 +128,6 @@ enum App {
             chat.addText(args.text!)
             chat.endTurn()
             chat.newTurn(role: "assistant")
-            chat.addAudioStartToken()
 
         case .stt:
             let inputURL = resolveURL(path: args.audioPath!)
@@ -259,7 +267,7 @@ enum App {
 
     // MARK: - SAM Audio
 
-    private static func runSAMAudio(args: CLI) async throws {
+    private static func runSAMAudio(model: SAMAudio, args: CLI) async throws {
         let mode = args.mode
 
         guard let audioPath = args.audioPath else {
@@ -275,17 +283,6 @@ enum App {
             throw AppError.anchorsUnsupportedForMode(mode)
         }
 
-        let resolvedHFToken = args.hfToken
-            ?? ProcessInfo.processInfo.environment["HF_TOKEN"]
-            ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
-
-        print("Loading SAM Audio model (\(args.model))")
-        let model = try await SAMAudio.fromPretrained(
-            args.model,
-            hfToken: resolvedHFToken,
-            strict: args.strict
-        )
-
         let targetOutputURL = makeOutputURL(
             outputPath: args.outputTargetPath,
             inputURL: inputURL,
@@ -421,6 +418,47 @@ enum App {
         print("Streamed \(chunks) chunk(s)")
     }
 
+    // MARK: - MossFormer2 Speech Enhancement
+
+    private static func runMossFormer2SE(model: MossFormer2SEModel, args: CLI) async throws {
+        guard let audioPath = args.audioPath else {
+            throw AppError.enhanceRequiresAudio
+        }
+
+        let inputURL = resolveURL(path: audioPath)
+        guard FileManager.default.fileExists(atPath: inputURL.path) else {
+            throw AppError.inputFileNotFound(inputURL.path)
+        }
+        // TODO: Handle loading and resamping inside enhance()
+        let (inputSampleRate, rawAudio) = try loadAudioArray(from: inputURL)
+        let audioData = try resampleIfNeeded(rawAudio, from: inputSampleRate, to: model.sampleRate)
+
+        print("Enhancing audio")
+        let started = CFAbsoluteTimeGetCurrent()
+
+        let enhanced = try model.enhance(audioData)
+        eval(enhanced)
+        let samples = enhanced.asArray(Float.self)
+
+        let duration = Double(samples.count) / Double(model.sampleRate)
+        print(String(format: "Enhanced %d samples (%.1fs at %dHz)", samples.count, duration, model.sampleRate))
+
+        let outputURL: URL
+        if let path = args.outputTargetPath {
+            outputURL = resolveURL(path: path)
+        } else {
+            let stem = inputURL.deletingPathExtension().lastPathComponent
+            outputURL = inputURL.deletingLastPathComponent()
+                .appendingPathComponent("\(stem).enhanced.wav")
+        }
+
+        try AudioUtils.writeWavFile(samples: samples, sampleRate: Double(model.sampleRate), fileURL: outputURL)
+        print("Wrote WAV to \(outputURL.path)")
+
+        let elapsed = CFAbsoluteTimeGetCurrent() - started
+        print(String(format: "Done. Elapsed: %.2fs", elapsed))
+    }
+
     // MARK: - Helpers
 
     private static func resolveURL(path: String) -> URL {
@@ -465,6 +503,86 @@ enum App {
         let audioFile = try AVAudioFile(forWriting: outputURL, settings: format.settings)
         try audioFile.write(from: buffer)
     }
+
+    private static func resampleIfNeeded(_ audio: MLXArray, from inputSampleRate: Int, to targetSampleRate: Int) throws -> MLXArray {
+        let mono = audio.ndim > 1 ? audio.mean(axis: -1) : audio
+        guard inputSampleRate != targetSampleRate else { return mono }
+
+        print("Resampling \(inputSampleRate)Hz → \(targetSampleRate)Hz")
+        let samples = mono.asArray(Float.self)
+        let resampled = try resampleAudio(samples, from: Double(inputSampleRate), to: Double(targetSampleRate))
+        return MLXArray(resampled)
+    }
+
+    private static func resampleAudio(_ samples: [Float], from sourceRate: Double, to targetRate: Double) throws -> [Float] {
+        guard !samples.isEmpty else { return samples }
+
+        guard let inputFormat = AVAudioFormat(
+            commonFormat: .pcmFormatFloat32, sampleRate: sourceRate, channels: 1, interleaved: false
+        ) else {
+            throw AppError.audioResampleFailed("unable to create input format")
+        }
+
+        guard let outputFormat = AVAudioFormat(
+            commonFormat: .pcmFormatFloat32, sampleRate: targetRate, channels: 1, interleaved: false
+        ) else {
+            throw AppError.audioResampleFailed("unable to create output format")
+        }
+
+        guard let converter = AVAudioConverter(from: inputFormat, to: outputFormat) else {
+            throw AppError.audioResampleFailed("unable to create AVAudioConverter")
+        }
+
+        let inputFrameCount = AVAudioFrameCount(samples.count)
+        guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: inputFormat, frameCapacity: inputFrameCount) else {
+            throw AppError.audioResampleFailed("unable to allocate input buffer")
+        }
+        inputBuffer.frameLength = inputFrameCount
+        if let channelData = inputBuffer.floatChannelData {
+            for (i, sample) in samples.enumerated() {
+                channelData[0][i] = sample
+            }
+        }
+
+        let ratio = targetRate / sourceRate
+        let outputCapacity = AVAudioFrameCount(ceil(Double(samples.count) * ratio)) + 32
+        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat, frameCapacity: outputCapacity) else {
+            throw AppError.audioResampleFailed("unable to allocate output buffer")
+        }
+
+        final class AudioInputFeed: @unchecked Sendable {
+            let buffer: AVAudioPCMBuffer
+            var consumed = false
+            init(buffer: AVAudioPCMBuffer) { self.buffer = buffer }
+        }
+        let inputFeed = AudioInputFeed(buffer: inputBuffer)
+
+        var conversionError: NSError?
+        let status = converter.convert(to: outputBuffer, error: &conversionError) { _, outStatus in
+            if inputFeed.consumed {
+                outStatus.pointee = .noDataNow
+                return nil
+            }
+            inputFeed.consumed = true
+            outStatus.pointee = .haveData
+            return inputFeed.buffer
+        }
+
+        if let conversionError {
+            throw AppError.audioResampleFailed(conversionError.localizedDescription)
+        }
+
+        guard status == .haveData || status == .inputRanDry || status == .endOfStream else {
+            throw AppError.audioResampleFailed("unexpected converter status: \(status.rawValue)")
+        }
+
+        let frameLength = Int(outputBuffer.frameLength)
+        guard frameLength > 0, let outputChannel = outputBuffer.floatChannelData?[0] else {
+            throw AppError.audioResampleFailed("converter produced empty output")
+        }
+
+        return Array(UnsafeBufferPointer(start: outputChannel, count: frameLength))
+    }
 }
 
 // MARK: - CLI
@@ -538,10 +656,10 @@ struct CLI {
         var lfmMode: LFMMode?
         var systemPrompt: String?
         var maxNewTokens = 512
-        var temperature: Float = 0.8
+        var temperature: Float = 0.7
         var topK = 50
-        var audioTemperature: Float = 0.7
-        var audioTopK = 30
+        var audioTemperature: Float = 0.8
+        var audioTopK = 4
 
         var iterator = CommandLine.arguments.dropFirst().makeIterator()
         while let arg = iterator.next() {
@@ -700,14 +818,17 @@ struct CLI {
               \(executable) [--model <repo>] [--mode <mode>] [options]
 
             Description:
-              Runs STS (Speech-to-Speech) models. Supports SAM Audio source separation
-              and LFM2.5-Audio multimodal generation (text-to-text, text-to-speech,
-              speech-to-text, speech-to-speech).
+              Runs STS (Speech-to-Speech) models. Model type is auto-detected from
+              config.json or repo name. Supports:
+                - LFM2.5-Audio: multimodal generation (t2t, tts, stt, sts)
+                - SAM Audio: source separation
+                - MossFormer2-SE: speech enhancement
 
             Model Selection:
-              --model <repo>               Model repo or local path.
+              --model <repo>               Model repo or local path (auto-detected).
                                            SAM Audio default: \(SAMAudio.defaultRepo)
                                            LFM example: mlx-community/LFM2.5-Audio-1.5B-6bit
+                                           MossFormer2 example: starkdmi/MossFormer2-SE-fp16
 
             LFM2.5-Audio Options:
               --mode <t2t|tts|stt|sts>     LFM generation mode.
@@ -719,10 +840,10 @@ struct CLI {
               -i, --audio <path>           Input audio file (required for stt/sts)
               --system <string>            System prompt (overrides per-mode default)
               --max-new-tokens <int>       Max tokens to generate. Default: 512
-              --temperature <float>        Text sampling temperature. Default: 0.8
+              --temperature <float>        Text sampling temperature. Default: 0.7
               --top-k <int>                Text top-K. Default: 50
-              --audio-temperature <float>  Audio sampling temperature. Default: 0.7
-              --audio-top-k <int>          Audio top-K. Default: 30
+              --audio-temperature <float>  Audio sampling temperature. Default: 0.8
+              --audio-top-k <int>          Audio top-K. Default: 4
               --stream                     Stream text output to stdout
               -o, --output-target <path>   Audio WAV output path. Default: lfm_output.wav
               --output-text <path>         Text output path (optional)
@@ -742,6 +863,10 @@ struct CLI {
               --anchor <tok:start:end>     Anchor (short mode only, repeatable)
               --strict                     Strict weight loading
 
+            MossFormer2-SE Options:
+              -i, --audio <path>           Input audio file (required)
+              -o, --output-target <path>   Enhanced WAV output. Default: <input>.enhanced.wav
+
             Common:
               --hf-token <token>           Hugging Face token (or set HF_TOKEN env var)
               -h, --help                   Show this help

commit 018dad89b9df1986f3cab3bc06a280a29f507698
Author: Prince Canuma <prince.gdt@gmail.com>
Date:   Thu Feb 19 22:20:01 2026 +0100

    Add LFM-2.5-Audio (#53)
    
    * add lfm-2.5-Audio
    
    * fix quant loading
    
    * patch dsp and processor
    
    * Refactor audio window padding logic in Processor.swift
    
    - Simplified the calculation for effective window padding when the window length is less than the FFT size.
    - Improved code clarity by reducing the number of variables and streamlining the concatenation of arrays.
    
    * Add STS smoke tests to MLXAudioSmokeTests.swift
    
    * fix tests
    
    * add to mlx-audio-swift-sts with LFM2.5-Audio support
    
    * improve AudioUtils and App functionality
    
    * Enhance LFM2AudioModel to handle end-of-sequence frames more effectively by embedding audio output and updating modality state.
    
    * fix sts
    
    * format

diff --git a/Package.swift b/Package.swift
index 08f0ba2..ce3c15b 100644
--- a/Package.swift
+++ b/Package.swift
@@ -140,6 +140,7 @@ let package = Package(
                 .product(name: "MLXNN", package: "mlx-swift"),
                 .product(name: "MLXFast", package: "mlx-swift"),
                 .product(name: "MLXLMCommon", package: "mlx-swift-lm"),
+                .product(name: "MLXLLM", package: "mlx-swift-lm"),
                 .product(name: "HuggingFace", package: "swift-huggingface"),
                 .product(name: "Transformers", package: "swift-transformers"),
             ],
diff --git a/Sources/MLXAudioCore/AudioUtils.swift b/Sources/MLXAudioCore/AudioUtils.swift
index be3be3c..d395f58 100644
--- a/Sources/MLXAudioCore/AudioUtils.swift
+++ b/Sources/MLXAudioCore/AudioUtils.swift
@@ -10,7 +10,7 @@ public class AudioUtils {
   private init() {}
 
   // Debug method to write output to .wav file for checking the speech generation
-  static func writeWavFile(samples: [Float], sampleRate: Double, fileURL: URL) throws {
+  public static func writeWavFile(samples: [Float], sampleRate: Double, fileURL: URL) throws {
     let frameCount = AVAudioFrameCount(samples.count)
 
     guard let format = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: sampleRate, channels: 1, interleaved: false),
diff --git a/Sources/MLXAudioCore/DSP.swift b/Sources/MLXAudioCore/DSP.swift
index 95c0885..8711ef5 100644
--- a/Sources/MLXAudioCore/DSP.swift
+++ b/Sources/MLXAudioCore/DSP.swift
@@ -48,6 +48,7 @@ public func melFilters(
     melScale: MelScale = .htk
 ) -> MLXArray {
     let fMaxVal = fMax ?? Float(sampleRate) / 2.0
+
     let nFreqs = nFft / 2 + 1
 
     // Generate frequency points
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/Conformer.swift b/Sources/MLXAudioSTS/Models/LFMAudio/Conformer.swift
new file mode 100644
index 0000000..49c9b99
--- /dev/null
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/Conformer.swift
@@ -0,0 +1,386 @@
+import Foundation
+import MLX
+import MLXNN
+
+// MARK: - Relative Positional Encoding
+
+class RelativePositionalEncoding: Module {
+    let dModel: Int
+    let maxLen: Int
+    let xscale: Float?
+    let divTerm: MLXArray
+    var pe: MLXArray?
+
+    init(dModel: Int, maxLen: Int = 5000, xscale: Bool = true) {
+        self.dModel = dModel
+        self.maxLen = maxLen
+        self.xscale = xscale ? sqrt(Float(dModel)) : nil
+
+        self.divTerm = MLX.exp(
+            MLXArray(stride(from: 0, to: dModel, by: 2).map { Float($0) })
+                * MLXArray(Float(-log(10000.0) / Float(dModel)))
+        )
+    }
+
+    func extendPE(length: Int) {
+        let neededSize = 2 * length - 1
+        if let pe = pe, pe.shape[0] >= neededSize { return }
+
+        let positions = MLXArray(
+            stride(from: Float(length - 1), through: Float(-(length - 1)), by: -1).map { $0 }
+        ).expandedDimensions(axis: 1)
+
+        var newPE = MLXArray.zeros([neededSize, dModel])
+        let sinVals = MLX.sin(positions * divTerm)
+        let cosVals = MLX.cos(positions * divTerm)
+
+        newPE = newPE.at[0..., .stride(by: 2)].add(sinVals)
+        newPE = newPE.at[0..., .stride(from: 1, by: 2)].add(cosVals)
+
+        self.pe = newPE
+    }
+
+    func callAsFunction(_ x: MLXArray) -> (MLXArray, MLXArray) {
+        let seqLen = x.shape[1]
+        extendPE(length: seqLen)
+
+        var result = x
+        if let scale = xscale {
+            result = result * MLXArray(scale)
+        }
+
+        let center = pe!.shape[0] / 2
+        let start = center - seqLen + 1
+        let end = center + seqLen
+        let posEmb = pe![start..<end]
+
+        return (result, posEmb)
+    }
+}
+
+// MARK: - Conformer Feed Forward
+
+class ConformerFeedForward: Module {
+    @ModuleInfo(key: "linear1") var linear1: Linear
+    @ModuleInfo(key: "linear2") var linear2: Linear
+    @ModuleInfo(key: "dropout") var dropout: Dropout
+
+    init(dModel: Int, dFF: Int, dropoutRate: Float = 0.1) {
+        self._linear1.wrappedValue = Linear(dModel, dFF)
+        self._linear2.wrappedValue = Linear(dFF, dModel)
+        self._dropout.wrappedValue = Dropout(p: dropoutRate)
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        var h = linear1(x)
+        h = silu(h)
+        h = dropout(h)
+        h = linear2(h)
+        h = dropout(h)
+        return h
+    }
+}
+
+// MARK: - Conformer Convolution
+
+class ConformerConvolution: Module {
+    @ModuleInfo(key: "pointwise_conv1") var pointwiseConv1: Linear
+    @ModuleInfo(key: "depthwise_conv") var depthwiseConv: Conv1d
+    @ModuleInfo(key: "norm") var norm: BatchNorm
+    @ModuleInfo(key: "pointwise_conv2") var pointwiseConv2: Linear
+    @ModuleInfo(key: "dropout") var dropout: Dropout
+
+    init(dModel: Int, kernelSize: Int = 31, normType: String = "batch_norm", dropoutRate: Float = 0.1) {
+        self._pointwiseConv1.wrappedValue = Linear(dModel, 2 * dModel)
+        self._depthwiseConv.wrappedValue = Conv1d(
+            inputChannels: dModel,
+            outputChannels: dModel,
+            kernelSize: kernelSize,
+            padding: (kernelSize - 1) / 2,
+            groups: dModel
+        )
+        self._norm.wrappedValue = BatchNorm(featureCount: dModel)
+        self._pointwiseConv2.wrappedValue = Linear(dModel, dModel)
+        self._dropout.wrappedValue = Dropout(p: dropoutRate)
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        var h = pointwiseConv1(x)
+
+        let parts = h.split(parts: 2, axis: -1)
+        h = parts[0] * sigmoid(parts[1])
+
+        h = depthwiseConv(h)
+        h = norm(h)
+        h = silu(h)
+        h = pointwiseConv2(h)
+        h = dropout(h)
+        return h
+    }
+}
+
+// MARK: - Relative Multi-Head Attention
+
+class RelativeMultiHeadAttention: Module {
+    let dModel: Int
+    let numHeads: Int
+    let headDim: Int
+    let scale: Float
+
+    @ModuleInfo(key: "q_proj") var qProj: Linear
+    @ModuleInfo(key: "k_proj") var kProj: Linear
+    @ModuleInfo(key: "v_proj") var vProj: Linear
+    @ModuleInfo(key: "out_proj") var outProj: Linear
+    @ModuleInfo(key: "pos_proj") var posProj: Linear
+    @ModuleInfo(key: "dropout") var dropout: Dropout
+
+    @ParameterInfo(key: "pos_bias_u") var posBiasU: MLXArray
+    @ParameterInfo(key: "pos_bias_v") var posBiasV: MLXArray
+
+    init(dModel: Int, numHeads: Int, dropoutRate: Float = 0.1) {
+        self.dModel = dModel
+        self.numHeads = numHeads
+        self.headDim = dModel / numHeads
+        self.scale = 1.0 / sqrt(Float(self.headDim))
+
+        self._qProj.wrappedValue = Linear(dModel, dModel)
+        self._kProj.wrappedValue = Linear(dModel, dModel)
+        self._vProj.wrappedValue = Linear(dModel, dModel)
+        self._outProj.wrappedValue = Linear(dModel, dModel)
+        self._posProj.wrappedValue = Linear(dModel, dModel, bias: false)
+        self._dropout.wrappedValue = Dropout(p: dropoutRate)
+        self._posBiasU.wrappedValue = MLXArray.zeros([numHeads, self.headDim])
+        self._posBiasV.wrappedValue = MLXArray.zeros([numHeads, self.headDim])
+    }
+
+    private func relShift(_ x: MLXArray) -> MLXArray {
+        let (B, H, T, posLen) = (x.dim(0), x.dim(1), x.dim(2), x.dim(3))
+        var shifted = MLX.padded(x, widths: [
+            IntOrPair((0, 0)), IntOrPair((0, 0)),
+            IntOrPair((0, 0)), IntOrPair((1, 0)),
+        ])
+        shifted = shifted.reshaped(B, H, posLen + 1, T)
+        shifted = shifted[0..., 0..., 1..., 0...]
+        shifted = shifted.reshaped(B, H, T, posLen)
+        return shifted[0..., 0..., 0..., ..<T]
+    }
+
+    func callAsFunction(_ x: MLXArray, posEmb: MLXArray, mask: MLXArray? = nil) -> MLXArray {
+        let (B, T, _) = (x.dim(0), x.dim(1), x.dim(2))
+
+        var q = qProj(x).reshaped(B, T, numHeads, headDim)
+        let k = kProj(x).reshaped(B, T, numHeads, headDim)
+        let v = vProj(x).reshaped(B, T, numHeads, headDim)
+
+        let pInput = posEmb.ndim == 2 ? posEmb.expandedDimensions(axis: 0) : posEmb
+        let p = posProj(pInput).reshaped(1, -1, numHeads, headDim)
+
+        let qWithBiasU = (q + posBiasU.expandedDimensions(axes: [0, 1])).transposed(0, 2, 1, 3)
+        let qWithBiasV = (q + posBiasV.expandedDimensions(axes: [0, 1])).transposed(0, 2, 1, 3)
+
+        let kT = k.transposed(0, 2, 1, 3)
+        let vT = v.transposed(0, 2, 1, 3)
+        let pT = p.transposed(0, 2, 1, 3)
+
+        let matrixAC = MLX.matmul(qWithBiasU, kT.transposed(0, 1, 3, 2))
+        var matrixBD = MLX.matmul(qWithBiasV, pT.transposed(0, 1, 3, 2))
+        matrixBD = relShift(matrixBD)
+
+        var scores = (matrixAC + matrixBD) * MLXArray(scale)
+
+        if let mask = mask {
+            scores = scores + mask
+        }
+
+        var attn = softmax(scores, axis: -1)
+        attn = dropout(attn)
+
+        let out = MLX.matmul(attn, vT).transposed(0, 2, 1, 3).reshaped(B, T, -1)
+        return outProj(out)
+    }
+}
+
+// MARK: - Conformer Layer
+
+class ConformerLayer: Module {
+    @ModuleInfo(key: "ff1_norm") var ff1Norm: LayerNorm
+    @ModuleInfo(key: "ff1") var ff1: ConformerFeedForward
+    @ModuleInfo(key: "attn_norm") var attnNorm: LayerNorm
+    @ModuleInfo(key: "attn") var attn: RelativeMultiHeadAttention
+    @ModuleInfo(key: "conv_norm") var convNorm: LayerNorm
+    @ModuleInfo(key: "conv") var conv: ConformerConvolution
+    @ModuleInfo(key: "ff2_norm") var ff2Norm: LayerNorm
+    @ModuleInfo(key: "ff2") var ff2: ConformerFeedForward
+    @ModuleInfo(key: "final_norm") var finalNorm: LayerNorm
+
+    init(
+        dModel: Int, numHeads: Int, ffExpansionFactor: Int = 4,
+        convKernelSize: Int = 31, convNormType: String = "batch_norm",
+        dropout: Float = 0.1, dropoutAtt: Float = 0.1
+    ) {
+        let dFF = dModel * ffExpansionFactor
+        self._ff1Norm.wrappedValue = LayerNorm(dimensions: dModel)
+        self._ff1.wrappedValue = ConformerFeedForward(dModel: dModel, dFF: dFF, dropoutRate: dropout)
+        self._attnNorm.wrappedValue = LayerNorm(dimensions: dModel)
+        self._attn.wrappedValue = RelativeMultiHeadAttention(dModel: dModel, numHeads: numHeads, dropoutRate: dropoutAtt)
+        self._convNorm.wrappedValue = LayerNorm(dimensions: dModel)
+        self._conv.wrappedValue = ConformerConvolution(dModel: dModel, kernelSize: convKernelSize, normType: convNormType, dropoutRate: dropout)
+        self._ff2Norm.wrappedValue = LayerNorm(dimensions: dModel)
+        self._ff2.wrappedValue = ConformerFeedForward(dModel: dModel, dFF: dFF, dropoutRate: dropout)
+        self._finalNorm.wrappedValue = LayerNorm(dimensions: dModel)
+    }
+
+    func callAsFunction(_ x: MLXArray, posEmb: MLXArray, mask: MLXArray? = nil) -> MLXArray {
+        var h = x + 0.5 * ff1(ff1Norm(x))
+        h = h + attn(attnNorm(h), posEmb: posEmb, mask: mask)
+        h = h + conv(convNorm(h))
+        h = h + 0.5 * ff2(ff2Norm(h))
+        return finalNorm(h)
+    }
+}
+
+// MARK: - Conv Subsampling (2D Depthwise Separable)
+
+class ConvSubsampling: Module {
+    let subsamplingFactor: Int
+    let convChannels: Int
+    let inChannels: Int
+    let conv: [Conv2d?]
+    @ModuleInfo(key: "out") var out: Linear
+
+    init(inChannels: Int, outChannels: Int, subsamplingFactor: Int = 8, convChannels: Int = 256) {
+        self.subsamplingFactor = subsamplingFactor
+        self.convChannels = convChannels
+        self.inChannels = inChannels
+
+        self.conv = [
+            Conv2d(inputChannels: 1, outputChannels: convChannels, kernelSize: 3, stride: 2, padding: 1),
+            nil, // ReLU placeholder
+            Conv2d(inputChannels: convChannels, outputChannels: convChannels, kernelSize: 3, stride: 2, padding: 1, groups: convChannels),
+            Conv2d(inputChannels: convChannels, outputChannels: convChannels, kernelSize: 1, stride: 1, padding: 0),
+            nil, // ReLU placeholder
+            Conv2d(inputChannels: convChannels, outputChannels: convChannels, kernelSize: 3, stride: 2, padding: 1, groups: convChannels),
+            Conv2d(inputChannels: convChannels, outputChannels: convChannels, kernelSize: 1, stride: 1, padding: 0),
+        ]
+
+        self._out.wrappedValue = Linear(convChannels * (inChannels / subsamplingFactor), outChannels)
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        let (B, T, D) = (x.dim(0), x.dim(1), x.dim(2))
+
+        var h = x.expandedDimensions(axis: 3)
+
+        h = relu(conv[0]!(h))
+        h = conv[2]!(h)
+        h = relu(conv[3]!(h))
+        h = conv[5]!(h)
+        h = relu(conv[6]!(h))
+
+        let (B2, TOut, DOut, C) = (h.dim(0), h.dim(1), h.dim(2), h.dim(3))
+        h = h.transposed(0, 1, 3, 2).reshaped(B2, TOut, -1)
+        return out(h)
+    }
+}
+
+// MARK: - Conformer Encoder
+
+public class ConformerEncoder: Module {
+    let config: ConformerEncoderConfig
+
+    @ModuleInfo(key: "pre_encode") var preEncode: ConvSubsampling
+    let posEnc: RelativePositionalEncoding
+    @ModuleInfo(key: "pre_dropout") var preDropout: Dropout
+    let layers: [ConformerLayer]
+
+    public init(_ config: ConformerEncoderConfig) {
+        self.config = config
+
+        self._preEncode.wrappedValue = ConvSubsampling(
+            inChannels: config.featIn,
+            outChannels: config.dModel,
+            subsamplingFactor: config.subsamplingFactor,
+            convChannels: config.subsamplingConvChannels
+        )
+
+        self.posEnc = RelativePositionalEncoding(
+            dModel: config.dModel,
+            maxLen: config.posEmbMaxLen,
+            xscale: false
+        )
+
+        self._preDropout.wrappedValue = Dropout(p: config.dropoutPreEncoder)
+
+        self.layers = (0..<config.nLayers).map { _ in
+            ConformerLayer(
+                dModel: config.dModel,
+                numHeads: config.nHeads,
+                ffExpansionFactor: config.ffExpansionFactor,
+                convKernelSize: config.convKernelSize,
+                convNormType: config.convNormType,
+                dropout: config.dropout,
+                dropoutAtt: config.dropoutAtt
+            )
+        }
+    }
+
+    public func callAsFunction(_ x: MLXArray, lengths: MLXArray? = nil) -> (MLXArray, MLXArray) {
+        var h = preEncode(x)
+
+        let newLengths: MLXArray
+        if let lengths = lengths {
+            newLengths = lengths / config.subsamplingFactor
+        } else {
+            newLengths = MLXArray([Int32(h.shape[1])] + Array(repeating: Int32(h.shape[1]), count: h.shape[0] - 1))
+        }
+
+        let (scaledH, posEmb) = posEnc(h)
+        h = scaledH
+        h = preDropout(h)
+
+        var mask: MLXArray? = nil
+        let maxLen = h.shape[1]
+        let idx = MLXArray(0..<Int32(maxLen)).expandedDimensions(axis: 0)
+        let paddingMask = idx .>= newLengths.expandedDimensions(axis: 1)
+        mask = MLX.which(
+            paddingMask.expandedDimensions(axes: [1, 2]),
+            MLXArray(Float(-1e9)),
+            MLXArray(Float(0))
+        )
+
+        for layer in layers {
+            h = layer(h, posEmb: posEmb, mask: mask)
+        }
+
+        return (h, newLengths)
+    }
+}
+
+// MARK: - MLP Adapter
+
+public class AdapterMLP: Module {
+    @ModuleInfo var norm: LayerNorm?
+    @ModuleInfo var linears: [Linear]
+
+    public init(inChannels: Int, outChannels: Int, hiddenDims: [Int], useLayerNorm: Bool = true, dropout: Float = 0.0) {
+        let channels = [inChannels] + hiddenDims + [outChannels]
+        self._norm.wrappedValue = useLayerNorm ? LayerNorm(dimensions: channels[0]) : nil
+        var linearList: [Linear] = []
+        for i in 0..<(channels.count - 1) {
+            linearList.append(Linear(channels[i], channels[i + 1]))
+        }
+        self._linears.wrappedValue = linearList
+    }
+
+    public func callAsFunction(_ x: MLXArray) -> MLXArray {
+        var h = x
+        if let norm { h = norm(h) }
+        for (i, linear) in linears.enumerated() {
+            h = linear(h)
+            if i < linears.count - 1 {
+                h = MLXNN.gelu(h)
+            }
+        }
+        return h
+    }
+}
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/Detokenizer.swift b/Sources/MLXAudioSTS/Models/LFMAudio/Detokenizer.swift
new file mode 100644
index 0000000..d3e1ec8
--- /dev/null
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/Detokenizer.swift
@@ -0,0 +1,422 @@
+import Foundation
+import MLX
+import MLXAudioCore
+import MLXNN
+
+// MARK: - Fused Embedding
+
+class FusedEmbedding: Module {
+    let numCodebooks: Int
+    let vocabSize: Int
+    let dim: Int
+
+    @ModuleInfo(key: "emb") var emb: Embedding
+
+    init(numCodebooks: Int, vocabSize: Int, dim: Int) {
+        self.numCodebooks = numCodebooks
+        self.vocabSize = vocabSize
+        self.dim = dim
+        self._emb.wrappedValue = Embedding(embeddingCount: numCodebooks * vocabSize, dimensions: dim)
+    }
+
+    func callAsFunction(_ codes: MLXArray) -> MLXArray {
+        let K = codes.dim(1)
+        let offsets = (MLXArray(0..<Int32(K)).expandedDimensions(axes: [0, 2])) * MLXArray(Int32(vocabSize))
+        let offsetCodes = codes + offsets
+        let embeddings = emb(offsetCodes)
+        return embeddings.mean(axis: 1)
+    }
+}
+
+// MARK: - Detokenizer RMSNorm
+
+class DetokRMSNorm: Module {
+    let eps: Float
+    var weight: MLXArray
+
+    init(dim: Int, eps: Float = 1e-5) {
+        self.eps = eps
+        self.weight = MLXArray.ones([dim])
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        let rms = MLX.sqrt(MLX.mean(x * x, axis: -1, keepDims: true) + MLXArray(eps))
+        return x / rms * weight
+    }
+}
+
+// MARK: - Detokenizer Conv Layer
+
+class DetokenizerConvLayer: Module {
+    @ModuleInfo(key: "in_proj") var inProj: Linear
+    @ModuleInfo(key: "conv") var conv: Conv1d
+    @ModuleInfo(key: "out_proj") var outProj: Linear
+
+    init(dim: Int) {
+        self._inProj.wrappedValue = Linear(dim, dim * 3, bias: false)
+        self._conv.wrappedValue = Conv1d(
+            inputChannels: dim, outputChannels: dim,
+            kernelSize: 3, padding: 2, groups: dim, bias: false
+        )
+        self._outProj.wrappedValue = Linear(dim, dim, bias: false)
+    }
+
+    func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
+        let seqlen = x.dim(1)
+        let BCx = inProj(x).split(parts: 3, axis: -1)
+        let bGate = BCx[0], cGate = BCx[1], xProj = BCx[2]
+        let Bx = bGate * xProj
+        let convOut = conv(Bx)[0..., ..<seqlen, 0...]
+        return outProj(cGate * convOut)
+    }
+}
+
+// MARK: - Detokenizer Sliding Window Attention
+
+class DetokenizerSlidingWindowAttention: Module {
+    let dim: Int
+    let numHeads: Int
+    let numKvHeads: Int
+    let headDim: Int
+    let slidingWindow: Int
+    let scale: Float
+    let ropeTheta: Float
+
+    @ModuleInfo(key: "q_proj") var qProj: Linear
+    @ModuleInfo(key: "k_proj") var kProj: Linear
+    @ModuleInfo(key: "v_proj") var vProj: Linear
+    @ModuleInfo(key: "out_proj") var outProj: Linear
+    @ModuleInfo(key: "q_layernorm") var qLayernorm: DetokRMSNorm
+    @ModuleInfo(key: "k_layernorm") var kLayernorm: DetokRMSNorm
+
+    init(dim: Int, numHeads: Int, numKvHeads: Int, slidingWindow: Int, ropeTheta: Float = 1000000.0) {
+        self.dim = dim
+        self.numHeads = numHeads
+        self.numKvHeads = numKvHeads
+        self.headDim = dim / numHeads
+        self.slidingWindow = slidingWindow
+        self.scale = pow(Float(self.headDim), -0.5)
+        self.ropeTheta = ropeTheta
+
+        self._qProj.wrappedValue = Linear(dim, dim, bias: false)
+        self._kProj.wrappedValue = Linear(dim, numKvHeads * self.headDim, bias: false)
+        self._vProj.wrappedValue = Linear(dim, numKvHeads * self.headDim, bias: false)
+        self._outProj.wrappedValue = Linear(dim, dim, bias: false)
+        self._qLayernorm.wrappedValue = DetokRMSNorm(dim: self.headDim)
+        self._kLayernorm.wrappedValue = DetokRMSNorm(dim: self.headDim)
+    }
+
+    private func applyRoPE(_ x: MLXArray, offset: Int = 0) -> MLXArray {
+        let (B, H, T, D) = (x.dim(0), x.dim(1), x.dim(2), x.dim(3))
+        let invFreq = 1.0 / MLX.pow(
+            MLXArray(ropeTheta),
+            MLXArray(stride(from: 0, to: D, by: 2).map { Float($0) / Float(D) })
+        )
+        let positions = MLXArray(Int32(offset)..<Int32(offset + T)).asType(.float32)
+        let angles = MLX.outer(positions, invFreq)
+
+        let cosHalf = MLX.cos(angles)
+        let sinHalf = MLX.sin(angles)
+        let cosF = concatenated([cosHalf, cosHalf], axis: -1).expandedDimensions(axes: [0, 1])
+        let sinF = concatenated([sinHalf, sinHalf], axis: -1).expandedDimensions(axes: [0, 1])
+
+        let x1 = x[0..., 0..., 0..., ..<(D / 2)]
+        let x2 = x[0..., 0..., 0..., (D / 2)...]
+
+        let cosFirst = cosF[0..., 0..., 0..., ..<(D / 2)]
+        let sinFirst = sinF[0..., 0..., 0..., ..<(D / 2)]
+        let cosSecond = cosF[0..., 0..., 0..., (D / 2)...]
+        let sinSecond = sinF[0..., 0..., 0..., (D / 2)...]
+        let part1 = x1 * cosFirst - x2 * sinFirst
+        let part2 = x2 * cosSecond + x1 * sinSecond
+        return concatenated([part1, part2], axis: -1)
+    }
+
+    func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
+        let (B, T, _) = (x.dim(0), x.dim(1), x.dim(2))
+
+        var q = qProj(x).reshaped(B, T, numHeads, headDim).transposed(0, 2, 1, 3)
+        var k = kProj(x).reshaped(B, T, numKvHeads, headDim).transposed(0, 2, 1, 3)
+        var v = vProj(x).reshaped(B, T, numKvHeads, headDim).transposed(0, 2, 1, 3)
+
+        q = qLayernorm(q)
+        k = kLayernorm(k)
+        q = applyRoPE(q)
+        k = applyRoPE(k)
+
+        // GQA expansion
+        if numKvHeads < numHeads {
+            let nRep = numHeads / numKvHeads
+            k = MLX.repeated(k, count: nRep, axis: 1)
+            v = MLX.repeated(v, count: nRep, axis: 1)
+        }
+
+        var scores = MLX.matmul(q, k.transposed(0, 1, 3, 2)) * MLXArray(scale)
+        if let mask = mask {
+            scores = scores + mask
+        }
+        let attn = softmax(scores, axis: -1)
+        let out = MLX.matmul(attn, v).transposed(0, 2, 1, 3).reshaped(B, T, -1)
+        return outProj(out)
+    }
+}
+
+// MARK: - Detokenizer SwiGLU
+
+class DetokenizerSwiGLU: Module {
+    @ModuleInfo(key: "w1") var w1: Linear
+    @ModuleInfo(key: "w2") var w2: Linear
+    @ModuleInfo(key: "w3") var w3: Linear
+
+    init(dim: Int, hiddenDim: Int) {
+        self._w1.wrappedValue = Linear(dim, hiddenDim, bias: false)
+        self._w2.wrappedValue = Linear(hiddenDim, dim, bias: false)
+        self._w3.wrappedValue = Linear(dim, hiddenDim, bias: false)
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        w2(silu(w1(x)) * w3(x))
+    }
+}
+
+// MARK: - Detokenizer Block
+
+class DetokenizerBlock: Module {
+    let layerType: String
+
+    @ModuleInfo(key: "operator_norm") var operatorNorm: DetokRMSNorm
+    @ModuleInfo(key: "conv") var conv: DetokenizerConvLayer?
+    @ModuleInfo(key: "self_attn") var selfAttn: DetokenizerSlidingWindowAttention?
+    @ModuleInfo(key: "ffn_norm") var ffnNorm: DetokRMSNorm
+    @ModuleInfo(key: "feed_forward") var feedForward: DetokenizerSwiGLU
+
+    init(
+        dim: Int, hiddenDim: Int, layerType: String,
+        numHeads: Int = 16, numKvHeads: Int = 8,
+        slidingWindow: Int = 30, normEps: Float = 1e-5, ropeTheta: Float = 1000000.0
+    ) {
+        self.layerType = layerType
+        self._operatorNorm.wrappedValue = DetokRMSNorm(dim: dim, eps: normEps)
+
+        if layerType == "conv" {
+            self._conv.wrappedValue = DetokenizerConvLayer(dim: dim)
+        } else {
+            self._selfAttn.wrappedValue = DetokenizerSlidingWindowAttention(
+                dim: dim, numHeads: numHeads, numKvHeads: numKvHeads,
+                slidingWindow: slidingWindow, ropeTheta: ropeTheta
+            )
+        }
+
+        self._ffnNorm.wrappedValue = DetokRMSNorm(dim: dim, eps: normEps)
+        self._feedForward.wrappedValue = DetokenizerSwiGLU(dim: dim, hiddenDim: hiddenDim)
+    }
+
+    func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
+        let h = operatorNorm(x)
+        let r: MLXArray
+        if layerType == "conv" {
+            r = conv!(h, mask: mask)
+        } else {
+            r = selfAttn!(h, mask: mask)
+        }
+        var out = x + r
+        out = out + feedForward(ffnNorm(out))
+        return out
+    }
+}
+
+// MARK: - LFM Detokenizer Model
+
+class LFMDetokenizerModel: Module {
+    let config: DetokenizerConfig
+
+    @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
+    @ModuleInfo(key: "embedding_norm") var embeddingNorm: DetokRMSNorm
+    let layers: [DetokenizerBlock]
+
+    init(_ config: DetokenizerConfig) {
+        self.config = config
+
+        self._embedTokens.wrappedValue = Embedding(embeddingCount: 65536, dimensions: config.hiddenSize)
+        self._embeddingNorm.wrappedValue = DetokRMSNorm(dim: config.hiddenSize, eps: config.normEps)
+
+        self.layers = config.layerTypes.map { layerType in
+            DetokenizerBlock(
+                dim: config.hiddenSize, hiddenDim: config.intermediateSize,
+                layerType: layerType, numHeads: config.numAttentionHeads,
+                numKvHeads: config.numKeyValueHeads, slidingWindow: config.slidingWindow,
+                normEps: config.normEps, ropeTheta: config.ropeTheta
+            )
+        }
+    }
+
+    func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
+        var h = x
+        for layer in layers {
+            h = layer(h, mask: mask)
+        }
+        return embeddingNorm(h)
+    }
+}
+
+// MARK: - LFM2 Audio Detokenizer
+
+public class LFM2AudioDetokenizer: Module {
+    let config: DetokenizerConfig
+
+    @ModuleInfo(key: "emb") var emb: FusedEmbedding
+    @ModuleInfo(key: "lfm") var lfm: LFMDetokenizerModel
+    @ModuleInfo(key: "lin") var lin: Linear
+
+    let nFft: Int
+    let hopLength: Int
+    var istftWindow: MLXArray?
+
+    public init(_ config: DetokenizerConfig) {
+        self.config = config
+        self.nFft = config.nFft
+        self.hopLength = config.hopLength
+
+        self._emb.wrappedValue = FusedEmbedding(
+            numCodebooks: config.numCodebooks, vocabSize: config.vocabSize,
+            dim: config.hiddenSize
+        )
+        self._lfm.wrappedValue = LFMDetokenizerModel(config)
+        self._lin.wrappedValue = Linear(config.hiddenSize, config.outputSize, bias: true)
+    }
+
+    private var window: MLXArray {
+        if let w = istftWindow { return w }
+        let n = nFft
+        return MLXArray(
+            (0..<n).map { Float(0.5 - 0.5 * cos(2.0 * Float.pi * Float($0) / Float(n))) }
+        )
+    }
+
+    private func createSlidingWindowMask(_ T: Int) -> MLXArray {
+        let idx = MLXArray(0..<Int32(T))
+        let dIdx = idx.expandedDimensions(axis: 1) - idx.expandedDimensions(axis: 0)
+        let geZero = dIdx .>= MLXArray(Int32(0))
+        let ltWindow = MLXArray(Int32(config.slidingWindow)) .> dIdx
+        let valid = geZero .&& ltWindow
+        let mask = MLX.which(valid, MLXArray(Float(0)), MLXArray(Float(-1e9)))
+        return mask.expandedDimensions(axes: [0, 1])
+    }
+
+    public func callAsFunction(_ codes: MLXArray) -> MLXArray {
+        let (B, K, T) = (codes.dim(0), codes.dim(1), codes.dim(2))
+
+        let clampedCodes = MLX.clip(codes, min: 0, max: config.vocabSize - 1)
+        var x = emb(clampedCodes)
+        x = MLX.repeated(x, count: config.upsampleFactor, axis: 1)
+
+        let mask = createSlidingWindowMask(x.dim(1))
+        x = lfm(x, mask: mask)
+        x = lin(x)
+
+        let nBins = nFft / 2 + 1
+        let logMag = x[0..., 0..., ..<nBins]
+        let phase = x[0..., 0..., nBins...]
+        let mag = MLX.exp(logMag)
+        return performISTFT(mag: mag, phase: phase)
+    }
+
+    private func performISTFT(mag: MLXArray, phase: MLXArray) -> MLXArray {
+        let B = mag.dim(0)
+        let TFrames = mag.dim(1)
+        let win = window
+        let pad = (nFft - hopLength) / 2
+
+        let real = mag * MLX.cos(phase)
+        let imag = mag * MLX.sin(phase)
+        let stftComplex = real + MLXArray(real: Float(0), imaginary: Float(1)) * imag
+
+        var outputs: [MLXArray] = []
+        for b in 0..<B {
+            let spec = stftComplex[b].transposed(1, 0)
+            let framesFreq = MLXFFT.irfft(spec, axis: 0)
+            let framesTime = framesFreq.transposed(1, 0)
+            let windowedFrames = framesTime * win
+
+            let outputLength = (TFrames - 1) * hopLength + nFft
+            var audioSamples = [Float](repeating: 0, count: outputLength)
+            var windowSum = [Float](repeating: 0, count: outputLength)
+
+            let windowArray = win.asArray(Float.self)
+
+            for i in 0..<TFrames {
+                let start = i * hopLength
+                let frameData = windowedFrames[i].asArray(Float.self)
+                for j in 0..<min(nFft, frameData.count) {
+                    if start + j < outputLength {
+                        audioSamples[start + j] += frameData[j]
+                        windowSum[start + j] += windowArray[j] * windowArray[j]
+                    }
+                }
+            }
+
+            for i in 0..<outputLength {
+                if windowSum[i] != 0 {
+                    audioSamples[i] /= windowSum[i]
+                }
+            }
+
+            let trimmed: [Float]
+            if pad > 0 && outputLength > 2 * pad {
+                trimmed = Array(audioSamples[pad..<(outputLength - pad)])
+            } else {
+                trimmed = audioSamples
+            }
+
+            outputs.append(MLXArray(trimmed))
+        }
+
+        return MLX.stacked(outputs, axis: 0)
+    }
+
+    // MARK: - Weight Sanitization
+
+    public static func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
+        var mapped: [String: MLXArray] = [:]
+        for (key, var value) in weights {
+            if key.contains("conv.conv.weight") {
+                if value.ndim == 3 {
+                    if value.dim(2) > value.dim(1) {
+                        value = value.transposed(0, 2, 1)
+                    }
+                }
+            }
+            mapped[key] = value
+        }
+        return mapped
+    }
+
+    // MARK: - From Pretrained
+
+    public static func fromPretrained(modelPath: URL) throws -> LFM2AudioDetokenizer {
+        let configURL = modelPath.appendingPathComponent("audio_detokenizer/config.json")
+        let weightsURL = modelPath.appendingPathComponent("audio_detokenizer/model.safetensors")
+
+        let configData = try Data(contentsOf: configURL)
+        var config = try JSONDecoder().decode(DetokenizerConfig.self, from: configData)
+
+        var weights = try MLX.loadArrays(url: weightsURL)
+
+        if let ffnWeight = weights["lfm.layers.0.feed_forward.w1.weight"] {
+            config.intermediateSize = ffnWeight.dim(0)
+        }
+
+        let model = LFM2AudioDetokenizer(config)
+
+        let istftWindow = weights.removeValue(forKey: "istft.window")
+
+        let sanitized = sanitize(weights: weights)
+        try model.update(parameters: ModuleParameters.unflattened(sanitized), verify: [.noUnusedKeys])
+
+        if let w = istftWindow {
+            model.istftWindow = w
+        }
+
+        return model
+    }
+}
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioConfig.swift b/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioConfig.swift
new file mode 100644
index 0000000..7ba6e9a
--- /dev/null
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioConfig.swift
@@ -0,0 +1,407 @@
+import Foundation
+import MLXLMCommon
+
+// MARK: - Preprocessor Config
+
+public struct PreprocessorConfig: Codable, Sendable {
+    public var sampleRate: Int
+    public var normalize: String
+    public var windowSize: Float
+    public var windowStride: Float
+    public var window: String
+    public var features: Int
+    public var nFft: Int
+    public var log: Bool
+    public var frameSplicing: Int
+    public var dither: Float
+    public var padTo: Int
+    public var padValue: Float
+    public var preemph: Float
+
+    public var hopLength: Int { Int(Float(sampleRate) * windowStride) }
+    public var winLength: Int { Int(Float(sampleRate) * windowSize) }
+
+    enum CodingKeys: String, CodingKey {
+        case sampleRate = "sample_rate"
+        case normalize
+        case windowSize = "window_size"
+        case windowStride = "window_stride"
+        case window
+        case features
+        case nFft = "n_fft"
+        case log
+        case frameSplicing = "frame_splicing"
+        case dither
+        case padTo = "pad_to"
+        case padValue = "pad_value"
+        case preemph
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+        sampleRate = try c.decodeIfPresent(Int.self, forKey: .sampleRate) ?? 16000
+        normalize = try c.decodeIfPresent(String.self, forKey: .normalize) ?? "per_feature"
+        windowSize = try c.decodeIfPresent(Float.self, forKey: .windowSize) ?? 0.025
+        windowStride = try c.decodeIfPresent(Float.self, forKey: .windowStride) ?? 0.01
+        window = try c.decodeIfPresent(String.self, forKey: .window) ?? "hann"
+        features = try c.decodeIfPresent(Int.self, forKey: .features) ?? 128
+        nFft = try c.decodeIfPresent(Int.self, forKey: .nFft) ?? 512
+        log = try c.decodeIfPresent(Bool.self, forKey: .log) ?? true
+        frameSplicing = try c.decodeIfPresent(Int.self, forKey: .frameSplicing) ?? 1
+        dither = try c.decodeIfPresent(Float.self, forKey: .dither) ?? 1e-05
+        padTo = try c.decodeIfPresent(Int.self, forKey: .padTo) ?? 0
+        padValue = try c.decodeIfPresent(Float.self, forKey: .padValue) ?? 0.0
+        preemph = try c.decodeIfPresent(Float.self, forKey: .preemph) ?? 0.97
+    }
+
+    public init() {
+        sampleRate = 16000; normalize = "per_feature"; windowSize = 0.025
+        windowStride = 0.01; window = "hann"; features = 128; nFft = 512
+        log = true; frameSplicing = 1; dither = 1e-05; padTo = 0
+        padValue = 0.0; preemph = 0.97
+    }
+}
+
+// MARK: - Conformer Encoder Config
+
+public struct ConformerEncoderConfig: Codable, Sendable {
+    public var featIn: Int
+    public var featOut: Int
+    public var nLayers: Int
+    public var dModel: Int
+    public var subsampling: String
+    public var subsamplingFactor: Int
+    public var subsamplingConvChannels: Int
+    public var causalDownsampling: Bool
+    public var ffExpansionFactor: Int
+    public var selfAttentionModel: String
+    public var nHeads: Int
+    public var attContextSize: [Int]
+    public var xscaling: Bool
+    public var untieBiases: Bool
+    public var posEmbMaxLen: Int
+    public var convKernelSize: Int
+    public var convNormType: String
+    public var dropout: Float
+    public var dropoutPreEncoder: Float
+    public var dropoutEmb: Float
+    public var dropoutAtt: Float
+
+    enum CodingKeys: String, CodingKey {
+        case featIn = "feat_in"
+        case featOut = "feat_out"
+        case nLayers = "n_layers"
+        case dModel = "d_model"
+        case subsampling
+        case subsamplingFactor = "subsampling_factor"
+        case subsamplingConvChannels = "subsampling_conv_channels"
+        case causalDownsampling = "causal_downsampling"
+        case ffExpansionFactor = "ff_expansion_factor"
+        case selfAttentionModel = "self_attention_model"
+        case nHeads = "n_heads"
+        case attContextSize = "att_context_size"
+        case xscaling
+        case untieBiases = "untie_biases"
+        case posEmbMaxLen = "pos_emb_max_len"
+        case convKernelSize = "conv_kernel_size"
+        case convNormType = "conv_norm_type"
+        case dropout
+        case dropoutPreEncoder = "dropout_pre_encoder"
+        case dropoutEmb = "dropout_emb"
+        case dropoutAtt = "dropout_att"
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+        featIn = try c.decodeIfPresent(Int.self, forKey: .featIn) ?? 128
+        featOut = try c.decodeIfPresent(Int.self, forKey: .featOut) ?? -1
+        nLayers = try c.decodeIfPresent(Int.self, forKey: .nLayers) ?? 17
+        dModel = try c.decodeIfPresent(Int.self, forKey: .dModel) ?? 512
+        subsampling = try c.decodeIfPresent(String.self, forKey: .subsampling) ?? "dw_striding"
+        subsamplingFactor = try c.decodeIfPresent(Int.self, forKey: .subsamplingFactor) ?? 8
+        subsamplingConvChannels = try c.decodeIfPresent(Int.self, forKey: .subsamplingConvChannels) ?? 256
+        causalDownsampling = try c.decodeIfPresent(Bool.self, forKey: .causalDownsampling) ?? false
+        ffExpansionFactor = try c.decodeIfPresent(Int.self, forKey: .ffExpansionFactor) ?? 4
+        selfAttentionModel = try c.decodeIfPresent(String.self, forKey: .selfAttentionModel) ?? "rel_pos"
+        nHeads = try c.decodeIfPresent(Int.self, forKey: .nHeads) ?? 8
+        attContextSize = try c.decodeIfPresent([Int].self, forKey: .attContextSize) ?? [-1, -1]
+        xscaling = try c.decodeIfPresent(Bool.self, forKey: .xscaling) ?? false
+        untieBiases = try c.decodeIfPresent(Bool.self, forKey: .untieBiases) ?? true
+        posEmbMaxLen = try c.decodeIfPresent(Int.self, forKey: .posEmbMaxLen) ?? 5000
+        convKernelSize = try c.decodeIfPresent(Int.self, forKey: .convKernelSize) ?? 9
+        convNormType = try c.decodeIfPresent(String.self, forKey: .convNormType) ?? "batch_norm"
+        dropout = try c.decodeIfPresent(Float.self, forKey: .dropout) ?? 0.1
+        dropoutPreEncoder = try c.decodeIfPresent(Float.self, forKey: .dropoutPreEncoder) ?? 0.1
+        dropoutEmb = try c.decodeIfPresent(Float.self, forKey: .dropoutEmb) ?? 0.0
+        dropoutAtt = try c.decodeIfPresent(Float.self, forKey: .dropoutAtt) ?? 0.1
+    }
+
+    public init() {
+        featIn = 128; featOut = -1; nLayers = 17; dModel = 512
+        subsampling = "dw_striding"; subsamplingFactor = 8
+        subsamplingConvChannels = 256; causalDownsampling = false
+        ffExpansionFactor = 4; selfAttentionModel = "rel_pos"
+        nHeads = 8; attContextSize = [-1, -1]; xscaling = false
+        untieBiases = true; posEmbMaxLen = 5000; convKernelSize = 9
+        convNormType = "batch_norm"; dropout = 0.1
+        dropoutPreEncoder = 0.1; dropoutEmb = 0.0; dropoutAtt = 0.1
+    }
+}
+
+// MARK: - Depthformer Config
+
+public struct DepthformerConfig: Codable, Sendable {
+    public var layers: Int
+    public var dim: Int
+    public var numHeads: Int
+    public var numKvHeads: Int
+    public var tie: Bool
+
+    enum CodingKeys: String, CodingKey {
+        case layers, dim
+        case numHeads = "num_heads"
+        case numKvHeads = "num_kv_heads"
+        case tie
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+        layers = try c.decodeIfPresent(Int.self, forKey: .layers) ?? 6
+        dim = try c.decodeIfPresent(Int.self, forKey: .dim) ?? 1024
+        numHeads = try c.decodeIfPresent(Int.self, forKey: .numHeads) ?? 32
+        numKvHeads = try c.decodeIfPresent(Int.self, forKey: .numKvHeads) ?? 8
+        tie = try c.decodeIfPresent(Bool.self, forKey: .tie) ?? true
+    }
+
+    public init() {
+        layers = 6; dim = 1024; numHeads = 32; numKvHeads = 8; tie = true
+    }
+}
+
+// MARK: - Detokenizer Config
+
+public struct DetokenizerConfig: Codable, Sendable {
+    public var hiddenSize: Int
+    public var numHiddenLayers: Int
+    public var numAttentionHeads: Int
+    public var numKeyValueHeads: Int
+    public var layerTypes: [String]
+    public var slidingWindow: Int
+    public var intermediateSize: Int
+    public var normEps: Float
+    public var ropeTheta: Float
+    public var outputSize: Int
+    public var numCodebooks: Int
+    public var vocabSize: Int
+    public var nFft: Int
+    public var hopLength: Int
+    public var upsampleFactor: Int
+
+    enum CodingKeys: String, CodingKey {
+        case hiddenSize = "hidden_size"
+        case numHiddenLayers = "num_hidden_layers"
+        case numAttentionHeads = "num_attention_heads"
+        case numKeyValueHeads = "num_key_value_heads"
+        case layerTypes = "layer_types"
+        case slidingWindow = "sliding_window"
+        case intermediateSize = "intermediate_size"
+        case normEps = "norm_eps"
+        case ropeTheta = "rope_theta"
+        case outputSize = "output_size"
+        case numCodebooks = "num_codebooks"
+        case vocabSize = "vocab_size"
+        case nFft = "n_fft"
+        case hopLength = "hop_length"
+        case upsampleFactor = "upsample_factor"
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+        hiddenSize = try c.decodeIfPresent(Int.self, forKey: .hiddenSize) ?? 512
+        numHiddenLayers = try c.decodeIfPresent(Int.self, forKey: .numHiddenLayers) ?? 8
+        numAttentionHeads = try c.decodeIfPresent(Int.self, forKey: .numAttentionHeads) ?? 16
+        numKeyValueHeads = try c.decodeIfPresent(Int.self, forKey: .numKeyValueHeads) ?? 8
+        layerTypes = try c.decodeIfPresent([String].self, forKey: .layerTypes) ?? [
+            "conv", "conv", "sliding_attention", "conv",
+            "sliding_attention", "conv", "sliding_attention", "conv",
+        ]
+        slidingWindow = try c.decodeIfPresent(Int.self, forKey: .slidingWindow) ?? 30
+        intermediateSize = try c.decodeIfPresent(Int.self, forKey: .intermediateSize) ?? 2304
+        normEps = try c.decodeIfPresent(Float.self, forKey: .normEps) ?? 1e-5
+        ropeTheta = try c.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 1000000.0
+        outputSize = try c.decodeIfPresent(Int.self, forKey: .outputSize) ?? 1282
+        numCodebooks = 8
+        vocabSize = 2048
+        nFft = try c.decodeIfPresent(Int.self, forKey: .nFft) ?? 1280
+        hopLength = try c.decodeIfPresent(Int.self, forKey: .hopLength) ?? 320
+        upsampleFactor = try c.decodeIfPresent(Int.self, forKey: .upsampleFactor) ?? 6
+    }
+
+    public init() {
+        hiddenSize = 512; numHiddenLayers = 8; numAttentionHeads = 16
+        numKeyValueHeads = 8
+        layerTypes = ["conv", "conv", "sliding_attention", "conv",
+                      "sliding_attention", "conv", "sliding_attention", "conv"]
+        slidingWindow = 30; intermediateSize = 2304; normEps = 1e-5
+        ropeTheta = 1000000.0; outputSize = 1282; numCodebooks = 8
+        vocabSize = 2048; nFft = 1280; hopLength = 320; upsampleFactor = 6
+    }
+}
+
+// MARK: - LFM2 Backbone Config
+
+public struct LFM2BackboneConfig: Codable, Sendable {
+    public var vocabSize: Int
+    public var hiddenSize: Int
+    public var numHiddenLayers: Int
+    public var numAttentionHeads: Int
+    public var numKeyValueHeads: Int
+    public var maxPositionEmbeddings: Int?
+    public var normEps: Float
+    public var convBias: Bool
+    public var convLCache: Int
+    public var blockDim: Int?
+    public var blockFFDim: Int?
+    public var blockMultipleOf: Int
+    public var blockFFNDimMultiplier: Float
+    public var blockAutoAdjustFFDim: Bool
+    public var fullAttnIdxs: [Int]?
+    public var layerTypes: [String]?
+    public var ropeTheta: Float
+
+    public var effectiveBlockDim: Int { blockDim ?? hiddenSize }
+    public var effectiveBlockFFDim: Int { blockFFDim ?? hiddenSize }
+
+    public var resolvedFullAttnIdxs: [Int] {
+        if let idxs = fullAttnIdxs { return idxs }
+        if let types = layerTypes {
+            return types.enumerated().compactMap { i, t in t == "full_attention" ? i : nil }
+        }
+        return Array(0..<numHiddenLayers)
+    }
+
+    public var headDimensions: Int { hiddenSize / numAttentionHeads }
+
+    enum CodingKeys: String, CodingKey {
+        case vocabSize = "vocab_size"
+        case hiddenSize = "hidden_size"
+        case numHiddenLayers = "num_hidden_layers"
+        case numAttentionHeads = "num_attention_heads"
+        case numKeyValueHeads = "num_key_value_heads"
+        case maxPositionEmbeddings = "max_position_embeddings"
+        case normEps = "norm_eps"
+        case convBias = "conv_bias"
+        case convLCache = "conv_L_cache"
+        case blockDim = "block_dim"
+        case blockFFDim = "block_ff_dim"
+        case blockMultipleOf = "block_multiple_of"
+        case blockFFNDimMultiplier = "block_ffn_dim_multiplier"
+        case blockAutoAdjustFFDim = "block_auto_adjust_ff_dim"
+        case fullAttnIdxs = "full_attn_idxs"
+        case layerTypes = "layer_types"
+        case ropeTheta = "rope_theta"
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+        vocabSize = try c.decodeIfPresent(Int.self, forKey: .vocabSize) ?? 65536
+        hiddenSize = try c.decode(Int.self, forKey: .hiddenSize)
+        numHiddenLayers = try c.decode(Int.self, forKey: .numHiddenLayers)
+        numAttentionHeads = try c.decode(Int.self, forKey: .numAttentionHeads)
+        numKeyValueHeads = try c.decode(Int.self, forKey: .numKeyValueHeads)
+        maxPositionEmbeddings = try c.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings)
+        normEps = try c.decodeIfPresent(Float.self, forKey: .normEps) ?? 1e-5
+        convBias = try c.decodeIfPresent(Bool.self, forKey: .convBias) ?? false
+        convLCache = try c.decodeIfPresent(Int.self, forKey: .convLCache) ?? 3
+        blockDim = try c.decodeIfPresent(Int.self, forKey: .blockDim)
+        blockFFDim = try c.decodeIfPresent(Int.self, forKey: .blockFFDim)
+        blockMultipleOf = try c.decodeIfPresent(Int.self, forKey: .blockMultipleOf) ?? 256
+        blockFFNDimMultiplier = try c.decodeIfPresent(Float.self, forKey: .blockFFNDimMultiplier) ?? 1.0
+        blockAutoAdjustFFDim = try c.decodeIfPresent(Bool.self, forKey: .blockAutoAdjustFFDim) ?? true
+        fullAttnIdxs = try c.decodeIfPresent([Int].self, forKey: .fullAttnIdxs)
+        layerTypes = try c.decodeIfPresent([String].self, forKey: .layerTypes)
+        ropeTheta = try c.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 1000000.0
+    }
+}
+
+// MARK: - Top-Level LFM2 Audio Config
+
+public struct LFM2AudioConfig: Codable, Sendable {
+    public var modelType: String
+    public var sampleRate: Int
+    public var codebooks: Int
+    public var tieAudioEmbeddings: Bool
+    public var semanticCodebookFactor: Int
+    public var codebookWeight: String
+    public var audioVocabSize: Int
+    public var interleavedNText: Int
+    public var interleavedNAudio: Int
+    public var adapterHiddenDims: [Int]
+    public var adapterDropout: Float
+    public var adapterUseLayerNorm: Bool
+    public var preprocessor: PreprocessorConfig
+    public var encoder: ConformerEncoderConfig
+    public var lfm: LFM2BackboneConfig
+    public var depthformer: DepthformerConfig
+
+    public var perLayerQuantization: BaseConfiguration.PerLayerQuantization?
+
+    enum CodingKeys: String, CodingKey {
+        case modelType = "model_type"
+        case sampleRate = "sample_rate"
+        case codebooks
+        case tieAudioEmbeddings = "tie_audio_embeddings"
+        case semanticCodebookFactor = "semantic_codebook_factor"
+        case codebookWeight = "codebook_weight"
+        case audioVocabSize = "audio_vocab_size"
+        case interleavedNText = "interleaved_n_text"
+        case interleavedNAudio = "interleaved_n_audio"
+        case adapterHiddenDims = "adapter_hidden_dims"
+        case adapterDropout = "adapter_dropout"
+        case adapterUseLayerNorm = "adapter_use_layer_norm"
+        case preprocessor, encoder, lfm, depthformer
+    }
+
+    public init(from decoder: Decoder) throws {
+        let c = try decoder.container(keyedBy: CodingKeys.self)
+        modelType = try c.decodeIfPresent(String.self, forKey: .modelType) ?? "lfm_audio"
+        sampleRate = try c.decodeIfPresent(Int.self, forKey: .sampleRate) ?? 24000
+        codebooks = try c.decodeIfPresent(Int.self, forKey: .codebooks) ?? 8
+        tieAudioEmbeddings = try c.decodeIfPresent(Bool.self, forKey: .tieAudioEmbeddings) ?? false
+        semanticCodebookFactor = try c.decodeIfPresent(Int.self, forKey: .semanticCodebookFactor) ?? 100
+        codebookWeight = try c.decodeIfPresent(String.self, forKey: .codebookWeight) ?? "log"
+        audioVocabSize = try c.decodeIfPresent(Int.self, forKey: .audioVocabSize) ?? 2049
+        interleavedNText = try c.decodeIfPresent(Int.self, forKey: .interleavedNText) ?? 6
+        interleavedNAudio = try c.decodeIfPresent(Int.self, forKey: .interleavedNAudio) ?? 12
+        adapterHiddenDims = try c.decodeIfPresent([Int].self, forKey: .adapterHiddenDims) ?? [2048]
+        adapterDropout = try c.decodeIfPresent(Float.self, forKey: .adapterDropout) ?? 0.0
+        adapterUseLayerNorm = try c.decodeIfPresent(Bool.self, forKey: .adapterUseLayerNorm) ?? true
+        preprocessor = try c.decodeIfPresent(PreprocessorConfig.self, forKey: .preprocessor) ?? PreprocessorConfig()
+        encoder = try c.decodeIfPresent(ConformerEncoderConfig.self, forKey: .encoder) ?? ConformerEncoderConfig()
+        lfm = try c.decode(LFM2BackboneConfig.self, forKey: .lfm)
+        depthformer = try c.decodeIfPresent(DepthformerConfig.self, forKey: .depthformer) ?? DepthformerConfig()
+
+        let baseConfig = try? BaseConfiguration(from: decoder)
+        perLayerQuantization = baseConfig?.perLayerQuantization
+    }
+
+    public func encode(to coder: Swift.Encoder) throws {
+        var container = coder.container(keyedBy: CodingKeys.self)
+        try container.encode(modelType, forKey: .modelType)
+        try container.encode(sampleRate, forKey: .sampleRate)
+        try container.encode(codebooks, forKey: .codebooks)
+        try container.encode(tieAudioEmbeddings, forKey: .tieAudioEmbeddings)
+        try container.encode(semanticCodebookFactor, forKey: .semanticCodebookFactor)
+        try container.encode(codebookWeight, forKey: .codebookWeight)
+        try container.encode(audioVocabSize, forKey: .audioVocabSize)
+        try container.encode(interleavedNText, forKey: .interleavedNText)
+        try container.encode(interleavedNAudio, forKey: .interleavedNAudio)
+        try container.encode(adapterHiddenDims, forKey: .adapterHiddenDims)
+        try container.encode(adapterDropout, forKey: .adapterDropout)
+        try container.encode(adapterUseLayerNorm, forKey: .adapterUseLayerNorm)
+        try container.encode(preprocessor, forKey: .preprocessor)
+        try container.encode(encoder, forKey: .encoder)
+        try container.encode(lfm, forKey: .lfm)
+        try container.encode(depthformer, forKey: .depthformer)
+    }
+}
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift b/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift
new file mode 100644
index 0000000..3476517
--- /dev/null
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/LFMAudioModel.swift
@@ -0,0 +1,782 @@
+import Foundation
+import Hub
+import HuggingFace
+@preconcurrency import MLX
+import MLXAudioCore
+import MLXLMCommon
+import MLXNN
+
+// MARK: - Constants
+
+public enum LFMModality: Int, Sendable {
+    case text = 1
+    case audioIn = 2
+    case audioOut = 3
+}
+
+public let lfmAudioStartToken: Int = 128
+public let lfmImEndToken: Int = 7
+public let lfmTextEndToken: Int = 130
+public let lfmAudioEOSToken: Int = 2048
+
+// MARK: - Generation Config
+
+public struct LFMGenerationConfig: Sendable {
+    public var maxNewTokens: Int
+    public var temperature: Float
+    public var topK: Int
+    public var topP: Float
+    public var audioTemperature: Float
+    public var audioTopK: Int
+
+    public init(
+        maxNewTokens: Int = 512, temperature: Float = 1.0,
+        topK: Int = 50, topP: Float = 1.0,
+        audioTemperature: Float = 1.0, audioTopK: Int = 4
+    ) {
+        self.maxNewTokens = maxNewTokens
+        self.temperature = temperature
+        self.topK = topK
+        self.topP = topP
+        self.audioTemperature = audioTemperature
+        self.audioTopK = audioTopK
+    }
+}
+
+// MARK: - Generation Output
+
+public enum LFMGenerationOutput: @unchecked Sendable {
+    case text(MLXArray)
+    case audio(MLXArray)
+}
+
+// MARK: - Audio Embedding
+
+class AudioEmbedding: Module {
+    let vocabSize: Int
+    let dim: Int
+    let numCodebooks: Int
+
+    @ModuleInfo(key: "embedding") var embedding: Embedding
+    @ModuleInfo(key: "embedding_norm") var embeddingNorm: RMSNorm
+    @ModuleInfo(key: "to_logits") var toLogits: Linear
+
+    let codebookOffsets: MLXArray
+
+    init(vocabSize: Int, dim: Int, numCodebooks: Int = 8, tie: Bool = false) {
+        self.vocabSize = vocabSize
+        self.dim = dim
+        self.numCodebooks = numCodebooks
+
+        let totalVocab = vocabSize * numCodebooks
+        self._embedding.wrappedValue = Embedding(embeddingCount: totalVocab, dimensions: dim)
+        self._embeddingNorm.wrappedValue = RMSNorm(dimensions: dim)
+        self._toLogits.wrappedValue = Linear(dim, totalVocab, bias: false)
+
+        self.codebookOffsets = MLXArray(
+            (0..<numCodebooks).map { Int32($0 * vocabSize) }
+        )
+    }
+
+    func callAsFunction(_ codes: MLXArray) -> MLXArray {
+        var c = codes
+        if c.ndim == 1 { c = c.expandedDimensions(axis: 0) }
+        let K = c.dim(1)
+        let offsetCodes = c + codebookOffsets[..<K]
+        let embedded = embedding(offsetCodes).sum(axis: 1)
+        return c.ndim == 1 ? embedded.squeezed(axis: 0) : embedded
+    }
+}
+
+// MARK: - Audio Embedding With Norm
+
+class AudioEmbeddingWithNorm: Module {
+    @ModuleInfo(key: "embedding") var embedding: Embedding
+    @ModuleInfo(key: "embedding_norm") var embeddingNorm: RMSNorm
+    @ModuleInfo(key: "to_logits") var toLogits: Linear
+
+    init(vocabSize: Int, dim: Int) {
+        self._embedding.wrappedValue = Embedding(embeddingCount: vocabSize, dimensions: dim)
+        self._embeddingNorm.wrappedValue = RMSNorm(dimensions: dim)
+        self._toLogits.wrappedValue = Linear(dim, vocabSize, bias: false)
+    }
+
+    func embed(_ x: MLXArray) -> MLXArray {
+        embeddingNorm(embedding(x))
+    }
+
+    func embedRaw(_ x: MLXArray) -> MLXArray {
+        embedding(x)
+    }
+
+    func logits(_ x: MLXArray) -> MLXArray {
+        toLogits(x)
+    }
+}
+
+// MARK: - Audio Head
+
+class AudioHead: Module {
+    let numCodebooks: Int
+    let depthformerDim: Int
+
+    @ModuleInfo(key: "depthformer") var depthformer: Depthformer
+
+    init(inputDim: Int, config: DepthformerConfig, numCodebooks: Int = 8) {
+        self.numCodebooks = numCodebooks
+        self.depthformerDim = config.dim
+
+        self._depthformer.wrappedValue = Depthformer(
+            layers: config.layers, dim: config.dim,
+            numHeads: config.numHeads, numKvHeads: config.numKvHeads,
+            tie: config.tie
+        )
+    }
+
+    func callAsFunction(
+        _ x: MLXArray, cache: [(MLXArray, MLXArray)?]? = nil, useCache: Bool = false
+    ) -> (MLXArray, [(MLXArray, MLXArray)]?) {
+        let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))
+        var h = x.reshaped(B, L, numCodebooks, depthformerDim)
+        h = h.transposed(0, 2, 1, 3)
+        h = h.reshaped(B * numCodebooks, L, depthformerDim)
+
+        let (out, newCache) = depthformer(h, cache: cache, useCache: useCache)
+
+        var result = out.reshaped(B, numCodebooks, L, depthformerDim)
+        result = result.transposed(0, 2, 1, 3)
+        return (result, newCache)
+    }
+}
+
+// MARK: - LFM2 Audio Model
+
+public class LFM2AudioModel: Module {
+    public let config: LFM2AudioConfig
+    public var processor: LFM2AudioProcessor?
+    public var modelDirectory: URL?
+
+    @ModuleInfo(key: "audio_encoder") var audioEncoder: ConformerEncoder
+    @ModuleInfo(key: "audio_adapter") var audioAdapter: AdapterMLP
+    @ModuleInfo(key: "lfm") var lfm: Lfm2Model
+    @ModuleInfo(key: "audio_embedding") var audioEmbedding: AudioEmbedding
+    @ModuleInfo(key: "depth_embeddings") var depthEmbeddings: [AudioEmbeddingWithNorm]
+    @ModuleInfo(key: "depth_linear") var depthLinear: Linear
+    @ModuleInfo(key: "audio_head") var audioHead: AudioHead
+
+    public init(_ config: LFM2AudioConfig) {
+        self.config = config
+
+        self._audioEncoder.wrappedValue = ConformerEncoder(config.encoder)
+        self._audioAdapter.wrappedValue = AdapterMLP(
+            inChannels: config.encoder.dModel,
+            outChannels: config.lfm.hiddenSize,
+            hiddenDims: config.adapterHiddenDims,
+            useLayerNorm: config.adapterUseLayerNorm,
+            dropout: config.adapterDropout
+        )
+        self._lfm.wrappedValue = Lfm2Model(config.lfm)
+        self._audioEmbedding.wrappedValue = AudioEmbedding(
+            vocabSize: config.audioVocabSize,
+            dim: config.lfm.hiddenSize,
+            numCodebooks: config.codebooks,
+            tie: config.tieAudioEmbeddings
+        )
+
+        self._depthEmbeddings.wrappedValue = (0..<config.codebooks).map { _ in
+            AudioEmbeddingWithNorm(vocabSize: config.audioVocabSize, dim: config.depthformer.dim)
+        }
+
+        self._depthLinear.wrappedValue = Linear(
+            config.lfm.hiddenSize, config.codebooks * config.depthformer.dim
+        )
+        self._audioHead.wrappedValue = AudioHead(
+            inputDim: config.lfm.hiddenSize, config: config.depthformer,
+            numCodebooks: config.codebooks
+        )
+    }
+
+    // MARK: - Encoding
+
+    func encodeAudio(_ melFeatures: MLXArray, lengths: MLXArray? = nil) -> (MLXArray, MLXArray) {
+        let (encoded, newLengths) = audioEncoder(melFeatures, lengths: lengths)
+        let adapted = audioAdapter(encoded)
+        return (adapted, newLengths)
+    }
+
+    func embedText(_ inputIds: MLXArray) -> MLXArray {
+        lfm.embedTokens(inputIds)
+    }
+
+    func embedAudioIn(_ audioCodes: MLXArray) -> MLXArray {
+        audioEmbedding(audioCodes)
+    }
+
+    func embedAudioOut(_ audioCodes: MLXArray) -> MLXArray {
+        audioEmbedding(audioCodes)
+    }
+
+    // MARK: - Prefill
+
+    func prefill(
+        textTokens: MLXArray? = nil,
+        audioFeatures: MLXArray? = nil,
+        audioCodes: MLXArray? = nil,
+        modalities: MLXArray? = nil,
+        cache: [KVCache]? = nil
+    ) -> (MLXArray, [KVCache]) {
+        let inputEmbeddings: MLXArray
+
+        if let modalities = modalities {
+            inputEmbeddings = buildInterleavedEmbeddings(
+                textTokens: textTokens, audioFeatures: audioFeatures,
+                audioCodes: audioCodes, modalities: modalities
+            )
+        } else {
+            var embeddings: [MLXArray] = []
+            if let t = textTokens { embeddings.append(embedText(t)) }
+            if let a = audioFeatures { embeddings.append(encodeAudio(a).0) }
+            if let ac = audioCodes {
+                let (B, T, _) = (ac.dim(0), ac.dim(1), ac.dim(2))
+                var audioOutEmb = MLXArray.zeros([B, T, config.lfm.hiddenSize])
+                for t in 0..<T {
+                    audioOutEmb = audioOutEmb.at[0..., t..<(t+1), 0...].add(
+                        embedAudioOut(ac[0..., t, 0...]).expandedDimensions(axis: 1)
+                    )
+                }
+                embeddings.append(audioOutEmb)
+            }
+            inputEmbeddings = embeddings.count > 1
+                ? concatenated(embeddings, axis: 1)
+                : embeddings[0]
+        }
+
+        let effectiveCache = cache ?? lfm.makeCache()
+        let hiddenStates = lfm(inputEmbeddings: inputEmbeddings, cache: effectiveCache)
+        return (hiddenStates, effectiveCache)
+    }
+
+    // MARK: - Interleaved Embeddings
+
+    private func buildInterleavedEmbeddings(
+        textTokens: MLXArray?, audioFeatures: MLXArray?,
+        audioCodes: MLXArray?, modalities: MLXArray
+    ) -> MLXArray {
+        let B = modalities.dim(0)
+        let TTotal = modalities.dim(1)
+        let D = config.lfm.hiddenSize
+
+        let modsFlat: [Int] = modalities[0].asArray(Int.self)
+        let uniqueMods = Set(modsFlat)
+
+        if uniqueMods == [LFMModality.text.rawValue], let t = textTokens {
+            return embedText(t)
+        }
+        if uniqueMods == [LFMModality.audioIn.rawValue], let a = audioFeatures {
+            return encodeAudio(a).0
+        }
+
+        let textEmb = textTokens.map { embedText($0) }
+        let audioEmb = audioFeatures.map { encodeAudio($0).0 }
+        var audioOutEmb: MLXArray? = nil
+        if let ac = audioCodes {
+            let (_, TAudio, _) = (ac.dim(0), ac.dim(1), ac.dim(2))
+            var parts: [MLXArray] = []
+            for t in 0..<TAudio {
+                parts.append(embedAudioOut(ac[0..., t, 0...]))
+            }
+            audioOutEmb = MLX.stacked(parts, axis: 1)
+        }
+
+        var textPos: [Int] = [], audioInPos: [Int] = [], audioOutPos: [Int] = []
+        for (pos, mod) in modsFlat.enumerated() {
+            switch mod {
+            case LFMModality.text.rawValue: textPos.append(pos)
+            case LFMModality.audioIn.rawValue: audioInPos.append(pos)
+            case LFMModality.audioOut.rawValue: audioOutPos.append(pos)
+            default: break
+            }
+        }
+
+        var embeddings = MLXArray.zeros([B, TTotal, D])
+
+        if let te = textEmb, !textPos.isEmpty {
+            let n = min(textPos.count, te.dim(1))
+            for i in 0..<n {
+                let pos = textPos[i]
+                embeddings = embeddings.at[0..., pos..<(pos+1), 0...].add(te[0..., i..<(i+1), 0...])
+            }
+        }
+
+        if let ae = audioEmb, !audioInPos.isEmpty {
+            let n = min(audioInPos.count, ae.dim(1))
+            for i in 0..<n {
+                let pos = audioInPos[i]
+                embeddings = embeddings.at[0..., pos..<(pos+1), 0...].add(ae[0..., i..<(i+1), 0...])
+            }
+        }
+
+        if let aoe = audioOutEmb, !audioOutPos.isEmpty {
+            let n = min(audioOutPos.count, aoe.dim(1))
+            for i in 0..<n {
+                let pos = audioOutPos[i]
+                embeddings = embeddings.at[0..., pos..<(pos+1), 0...].add(aoe[0..., i..<(i+1), 0...])
+            }
+        }
+
+        return embeddings
+    }
+
+    // MARK: - Sampling
+
+    func sampleTextToken(logits: MLXArray, temperature: Float = 1.0, topK: Int = 50) -> MLXArray {
+        if temperature == 0 { return MLX.argMax(logits, axis: -1) }
+
+        var l = logits / MLXArray(temperature)
+
+        if topK > 0 && topK < l.dim(-1) {
+            let sortedIndices = MLX.argSort(-l, axis: -1)
+            let kthPos = sortedIndices[0, topK - 1].item(Int.self)
+            let kthValue = l[0, kthPos]
+            l = MLX.which(l .>= kthValue, l, MLXArray(-Float.infinity))
+        }
+
+        return MLXRandom.categorical(l)
+    }
+
+    func sampleAudioFrame(
+        hiddenState: MLXArray, audioCache: [(MLXArray, MLXArray)?]? = nil,
+        temperature: Float = 1.0, topK: Int = 4
+    ) -> (MLXArray, [(MLXArray, MLXArray)]?) {
+        let B = hiddenState.dim(0)
+        let depthformerIn = depthLinear(hiddenState)
+            .reshaped(B, 1, config.codebooks, audioHead.depthformerDim)
+
+        var depthformerToken = MLXArray.zeros([B, audioHead.depthformerDim])
+        var cache = audioCache ?? Array(repeating: nil, count: audioHead.depthformer.layersCount)
+        var codes: [MLXArray] = []
+
+        let greedy = temperature <= 0 || topK == 1
+
+        for i in 0..<config.codebooks {
+            var curInput = depthformerIn[0..., 0..., i, 0...]
+            curInput = curInput + depthformerToken.expandedDimensions(axis: 1)
+
+            let (out, newCache) = audioHead.depthformer(curInput, cache: cache, useCache: true)
+            cache = newCache?.map { Optional($0) } ?? cache
+
+            let logits = depthEmbeddings[i].logits(out[0..., (-1)..., 0...].squeezed(axis: 1))
+
+            let code: MLXArray
+            if greedy {
+                code = MLX.argMax(logits, axis: -1, keepDims: true)
+            } else {
+                var l = logits / MLXArray(temperature)
+                if topK > 0 && topK < l.dim(-1) {
+                    let sortedIndices = MLX.argSort(-l, axis: -1)
+                    let kthPos = sortedIndices[0, topK - 1].item(Int.self)
+                    let kthValue = l[0, kthPos]
+                    l = MLX.which(l .>= kthValue, l, MLXArray(-Float.infinity))
+                }
+                code = MLXRandom.categorical(l).expandedDimensions(axis: -1)
+            }
+
+            codes.append(code.squeezed(axis: -1))
+            depthformerToken = depthEmbeddings[i].embedRaw(code.squeezed(axis: -1))
+        }
+
+        return (MLX.stacked(codes, axis: -1), cache.compactMap { $0 })
+    }
+
+    // MARK: - Interleaved Generation
+
+    public func generateInterleaved(
+        textTokens: MLXArray? = nil,
+        audioFeatures: MLXArray? = nil,
+        audioCodes: MLXArray? = nil,
+        modalities: MLXArray? = nil,
+        config genConfig: LFMGenerationConfig = LFMGenerationConfig()
+    ) -> AsyncThrowingStream<(MLXArray, LFMModality), Error> {
+        AsyncThrowingStream { continuation in
+            let nText = config.interleavedNText
+            let nAudio = config.interleavedNAudio
+
+            let (hiddenStates, cache) = prefill(
+                textTokens: textTokens, audioFeatures: audioFeatures,
+                audioCodes: audioCodes, modalities: modalities
+            )
+
+            var lastHidden = hiddenStates[0..., (-1)..., 0...]
+            var generated = 0
+            var modalityLeft = nText
+            var textDone = false
+            var currentModality = LFMModality.text
+
+            while generated < genConfig.maxNewTokens {
+                if currentModality == .text {
+                    let textLogits = lfm.embedTokens.asLinear(lastHidden)[0..., -1, 0...]
+                    let textToken = sampleTextToken(
+                        logits: textLogits, temperature: genConfig.temperature,
+                        topK: genConfig.topK
+                    )
+                    let tokenId = textToken.item(Int.self)
+
+                    if tokenId == lfmImEndToken { break }
+
+                    continuation.yield((textToken, .text))
+
+                    if tokenId == lfmTextEndToken { textDone = true }
+
+                    let nextEmb = embedText(MLXArray([Int32(tokenId)]).reshaped(1, 1))
+                    lastHidden = lfm(inputEmbeddings: nextEmb, cache: cache)
+
+                    modalityLeft -= 1
+                    generated += 1
+
+                    if modalityLeft <= 0 || textDone {
+                        modalityLeft = nAudio
+                        currentModality = .audioOut
+                    }
+                } else {
+                    let (audioFrame, _) = sampleAudioFrame(
+                        hiddenState: lastHidden, audioCache: nil,
+                        temperature: genConfig.audioTemperature,
+                        topK: genConfig.audioTopK
+                    )
+
+                    if audioFrame[0, 0].item(Int.self) == lfmAudioEOSToken {
+                        let eosFrame = MLX.full(audioFrame.shape, values: MLXArray(Int32(lfmAudioEOSToken)), type: Int32.self)
+                        continuation.yield((eosFrame.squeezed(axis: 0), .audioOut))
+
+                        generated += 1
+                        currentModality = .text
+                        if textDone { break }
+                        modalityLeft = nText
+                        continue
+                    }
+
+                    continuation.yield((audioFrame.squeezed(axis: 0), .audioOut))
+
+                    let nextEmb = embedAudioOut(audioFrame).expandedDimensions(axis: 1)
+                    lastHidden = lfm(inputEmbeddings: nextEmb, cache: cache)
+
+                    modalityLeft -= 1
+                    generated += 1
+
+                    if modalityLeft <= 0 && !textDone {
+                        modalityLeft = nText
+                        currentModality = .text
+                    }
+                }
+            }
+
+            continuation.finish()
+        }
+    }
+
+    // MARK: - Sequential Generation
+
+    public func generateSequential(
+        textTokens: MLXArray? = nil,
+        audioFeatures: MLXArray? = nil,
+        audioCodes: MLXArray? = nil,
+        modalities: MLXArray? = nil,
+        config genConfig: LFMGenerationConfig = LFMGenerationConfig()
+    ) -> AsyncThrowingStream<(MLXArray, LFMModality), Error> {
+        AsyncThrowingStream { continuation in
+            let (hiddenStates, cache) = prefill(
+                textTokens: textTokens, audioFeatures: audioFeatures,
+                audioCodes: audioCodes, modalities: modalities
+            )
+
+            var lastHidden = hiddenStates[0..., (-1)..., 0...]
+
+            var currentModality: LFMModality = .text
+            if let t = textTokens, t[0, -1].item(Int.self) == lfmAudioStartToken {
+                currentModality = .audioOut
+            }
+
+            var generated = 0
+
+            while generated < genConfig.maxNewTokens {
+                if currentModality == .text {
+                    let textLogits = lfm.embedTokens.asLinear(lastHidden)[0..., -1, 0...]
+                    let textToken = sampleTextToken(
+                        logits: textLogits, temperature: genConfig.temperature,
+                        topK: genConfig.topK
+                    )
+                    let tokenId = textToken.item(Int.self)
+
+                    if tokenId == lfmImEndToken {
+                        continuation.yield((textToken, .text))
+                        break
+                    }
+
+                    if tokenId == lfmAudioStartToken {
+                        currentModality = .audioOut
+                        let nextEmb = embedText(MLXArray([Int32(tokenId)]).reshaped(1, 1))
+                        lastHidden = lfm(inputEmbeddings: nextEmb, cache: cache)
+                        continue
+                    }
+
+                    continuation.yield((textToken, .text))
+
+                    let nextEmb = embedText(MLXArray([Int32(tokenId)]).reshaped(1, 1))
+                    lastHidden = lfm(inputEmbeddings: nextEmb, cache: cache)
+
+                } else {
+                    let (audioFrame, _) = sampleAudioFrame(
+                        hiddenState: lastHidden, audioCache: nil,
+                        temperature: genConfig.audioTemperature,
+                        topK: genConfig.audioTopK
+                    )
+
+                    if audioFrame[0, 0].item(Int.self) == lfmAudioEOSToken {
+                        let eosFrame = MLX.full(audioFrame.shape, values: MLXArray(Int32(lfmAudioEOSToken)), type: Int32.self)
+                        currentModality = .text
+                        continuation.yield((eosFrame.squeezed(axis: 0), .audioOut))
+                        let nextEmb = embedAudioOut(eosFrame).expandedDimensions(axis: 1)
+                        lastHidden = lfm(inputEmbeddings: nextEmb, cache: cache)
+                        generated += 1
+                        continue
+                    }
+
+                    continuation.yield((audioFrame.squeezed(axis: 0), .audioOut))
+
+                    let nextEmb = embedAudioOut(audioFrame).expandedDimensions(axis: 1)
+                    lastHidden = lfm(inputEmbeddings: nextEmb, cache: cache)
+                }
+
+                generated += 1
+            }
+
+            continuation.finish()
+        }
+    }
+
+    // MARK: - Weight Sanitization
+
+    public static func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
+        var sanitized: [String: MLXArray] = [:]
+
+        let skipKeys = [
+            "audio_loss_weights", "codebook_offsets", "downsample.", "upsample.",
+            ".num_batches_tracked", "pos_enc.pe", ".freqs",
+        ]
+
+        for (key, value) in weights {
+            if skipKeys.contains(where: { key.contains($0) }) { continue }
+
+            var newKey = key
+
+            if key.hasPrefix("conformer.") {
+                newKey = key.replacingOccurrences(of: "conformer.", with: "audio_encoder.")
+                newKey = newKey.replacingOccurrences(of: ".norm_feed_forward1.", with: ".ff1_norm.")
+                newKey = newKey.replacingOccurrences(of: ".norm_feed_forward2.", with: ".ff2_norm.")
+                newKey = newKey.replacingOccurrences(of: ".norm_self_att.", with: ".attn_norm.")
+                newKey = newKey.replacingOccurrences(of: ".norm_conv.", with: ".conv_norm.")
+                newKey = newKey.replacingOccurrences(of: ".norm_out.", with: ".final_norm.")
+                newKey = newKey.replacingOccurrences(of: ".feed_forward1.", with: ".ff1.")
+                newKey = newKey.replacingOccurrences(of: ".feed_forward2.", with: ".ff2.")
+                newKey = newKey.replacingOccurrences(of: ".self_attn.linear_q.", with: ".attn.q_proj.")
+                newKey = newKey.replacingOccurrences(of: ".self_attn.linear_k.", with: ".attn.k_proj.")
+                newKey = newKey.replacingOccurrences(of: ".self_attn.linear_v.", with: ".attn.v_proj.")
+                newKey = newKey.replacingOccurrences(of: ".self_attn.linear_out.", with: ".attn.out_proj.")
+                newKey = newKey.replacingOccurrences(of: ".self_attn.linear_pos.", with: ".attn.pos_proj.")
+                newKey = newKey.replacingOccurrences(of: ".self_attn.pos_bias_u", with: ".attn.pos_bias_u")
+                newKey = newKey.replacingOccurrences(of: ".self_attn.pos_bias_v", with: ".attn.pos_bias_v")
+                newKey = newKey.replacingOccurrences(of: ".conv.batch_norm.", with: ".conv.norm.")
+            }
+            else if key.hasPrefix("audio_adapter.model.") {
+                newKey = key.replacingOccurrences(of: "audio_adapter.model.", with: "audio_adapter.layers.")
+            }
+            else if key.hasPrefix("lfm.") {
+                newKey = newKey.replacingOccurrences(of: ".feed_forward.linear1.", with: ".feed_forward.w1.")
+                newKey = newKey.replacingOccurrences(of: ".feed_forward.linear2.", with: ".feed_forward.w2.")
+                newKey = newKey.replacingOccurrences(of: ".feed_forward.linear3.", with: ".feed_forward.w3.")
+            }
+            else if key.hasPrefix("depthformer.") {
+                if let range = key.range(of: #"depthformer\.layers\.(\d+)\.(.*)"#, options: .regularExpression) {
+                    let matched = String(key[range])
+                    let components = matched.split(separator: ".")
+                    if components.count >= 4 {
+                        let layerIdx = components[2]
+                        let rest = components[3...].joined(separator: ".")
+
+                        if rest == "operator.qkv_proj.weight" {
+                            newKey = "audio_head.depthformer.blocks.\(layerIdx).attn.qkv_weight"
+                        } else if rest == "operator.out_proj.weight" {
+                            newKey = "audio_head.depthformer.blocks.\(layerIdx).attn.o_proj.weight"
+                        } else if rest == "operator.bounded_attention.q_layernorm.weight" {
+                            newKey = "audio_head.depthformer.blocks.\(layerIdx).attn.q_norm.weight"
+                        } else if rest == "operator.bounded_attention.k_layernorm.weight" {
+                            newKey = "audio_head.depthformer.blocks.\(layerIdx).attn.k_norm.weight"
+                        } else if rest.hasPrefix("operator_norm.") {
+                            let suffix = rest.split(separator: ".", maxSplits: 1).last.map(String.init) ?? ""
+                            newKey = "audio_head.depthformer.blocks.\(layerIdx).attn_norm.\(suffix)"
+                        } else if rest.hasPrefix("feed_forward.") {
+                            let suffix = rest.split(separator: ".", maxSplits: 1).last.map(String.init) ?? ""
+                            newKey = "audio_head.depthformer.blocks.\(layerIdx).ffn.\(suffix)"
+                        } else if rest.hasPrefix("ffn_norm.") {
+                            let suffix = rest.split(separator: ".", maxSplits: 1).last.map(String.init) ?? ""
+                            newKey = "audio_head.depthformer.blocks.\(layerIdx).ffn_norm.\(suffix)"
+                        } else {
+                            newKey = "audio_head.depthformer.blocks.\(layerIdx).\(rest)"
+                        }
+                    }
+                }
+            }
+
+            sanitized[newKey] = value
+        }
+
+        var keysToRemove: [String] = []
+        var keysToAdd: [String: MLXArray] = [:]
+
+        for (key, value) in sanitized {
+            if key.contains(".attn.qkv_weight") {
+                let qDim = 1024
+                let kvDim = 256
+                let qWeight = value[..<qDim]
+                let kWeight = value[qDim..<(qDim + kvDim)]
+                let vWeight = value[(qDim + kvDim)...]
+
+                let baseKey = key.replacingOccurrences(of: ".qkv_weight", with: "")
+                keysToAdd["\(baseKey).q_proj.weight"] = qWeight
+                keysToAdd["\(baseKey).k_proj.weight"] = kWeight
+                keysToAdd["\(baseKey).v_proj.weight"] = vWeight
+                keysToRemove.append(key)
+            }
+        }
+
+        for key in keysToRemove { sanitized.removeValue(forKey: key) }
+        sanitized.merge(keysToAdd) { _, new in new }
+
+        for (key, value) in sanitized {
+            if key.contains("pointwise_conv") && key.contains("weight") && value.ndim == 3 {
+                sanitized[key] = value.ndim == 2 ? value : value.squeezed(axis: -1)
+            } else if (key.contains("depthwise_conv") || key.hasSuffix(".conv.weight"))
+                        && value.ndim == 3 {
+                if value.dim(2) > value.dim(1) {
+                    sanitized[key] = value.transposed(0, 2, 1)
+                }
+            } else if key.contains("pre_encode.conv") && value.ndim == 4 {
+            }
+        }
+
+        for (key, value) in sanitized {
+            if key.hasPrefix("lfm.") && key.contains("conv.conv.weight") && value.ndim == 3 {
+                if value.dim(value.ndim - 1) > value.dim(1) {
+                    sanitized[key] = value.transposed(0, 2, 1)
+                }
+            }
+        }
+
+
+        var adapterRemap: [String: MLXArray] = [:]
+        var adapterRemove: [String] = []
+        var linearIdx = 0
+        let adapterKeys = sanitized.keys.filter { $0.hasPrefix("audio_adapter.layers.") }
+        let indices = Set(adapterKeys.compactMap { key -> Int? in
+            let parts = key.dropFirst("audio_adapter.layers.".count).split(separator: ".", maxSplits: 1)
+            return Int(parts.first ?? "")
+        }).sorted()
+
+        for idx in indices {
+            let prefix = "audio_adapter.layers.\(idx)"
+            let matchingKeys = adapterKeys.filter { $0.hasPrefix(prefix + ".") }
+            if matchingKeys.isEmpty { continue }
+
+            let isNorm = matchingKeys.contains { $0.hasSuffix(".weight") } &&
+                         !matchingKeys.contains { $0.hasSuffix(".scales") } &&
+                         sanitized["\(prefix).weight"]?.ndim == 1
+            if isNorm {
+                for key in matchingKeys {
+                    let suffix = String(key.dropFirst(prefix.count + 1))
+                    adapterRemap["audio_adapter.norm.\(suffix)"] = sanitized[key]
+                    adapterRemove.append(key)
+                }
+            } else {
+                for key in matchingKeys {
+                    let suffix = String(key.dropFirst(prefix.count + 1))
+                    adapterRemap["audio_adapter.linears.\(linearIdx).\(suffix)"] = sanitized[key]
+                    adapterRemove.append(key)
+                }
+                linearIdx += 1
+            }
+        }
+        for key in adapterRemove { sanitized.removeValue(forKey: key) }
+        sanitized.merge(adapterRemap) { _, new in new }
+
+        return sanitized
+    }
+
+    // MARK: - From Pretrained
+
+    public static func fromPretrained(_ modelNameOrPath: String) async throws -> LFM2AudioModel {
+        guard let repoID = Repo.ID(rawValue: modelNameOrPath) else {
+            throw LFMAudioError.modelNotFound(modelNameOrPath)
+        }
+        let modelDir = try await ModelUtils.resolveOrDownloadModel(
+            repoID: repoID,
+            requiredExtension: "safetensors"
+        )
+
+        let configURL = modelDir.appendingPathComponent("config.json")
+        let configData = try Data(contentsOf: configURL)
+        let config = try JSONDecoder().decode(LFM2AudioConfig.self, from: configData)
+
+        let model = LFM2AudioModel(config)
+
+        // Load weights
+        let files = try FileManager.default.contentsOfDirectory(
+            at: modelDir, includingPropertiesForKeys: nil
+        )
+        let safetensorFiles = files.filter {
+            $0.pathExtension == "safetensors" && !$0.lastPathComponent.contains("tokenizer")
+        }
+
+        var weights: [String: MLXArray] = [:]
+        for file in safetensorFiles {
+            let fileWeights = try MLX.loadArrays(url: file)
+            weights.merge(fileWeights) { _, new in new }
+        }
+
+        let sanitizedWeights = sanitize(weights: weights)
+
+        // Quantize if needed (follows Python model_quant_predicate: skip norm/conv)
+        let perLayerQuantization = config.perLayerQuantization
+        if perLayerQuantization != nil {
+            quantize(model: model) { path, module in
+                if path.contains("norm") || path.contains("conv") {
+                    return nil
+                }
+                if sanitizedWeights["\(path).scales"] != nil {
+                    return perLayerQuantization?.quantization(layer: path)?.asTuple
+                }
+                return nil
+            }
+        }
+
+        // Cast float32 to float16 (except conv/norm) for non-quantized models
+        var finalWeights = sanitizedWeights
+        if perLayerQuantization == nil {
+            for (key, value) in finalWeights {
+                if value.dtype == .float32 {
+                    if key.contains("conv") || key.contains("norm") { continue }
+                    finalWeights[key] = value.asType(.float16)
+                }
+            }
+        }
+
+        let _ = try model.update(parameters: ModuleParameters.unflattened(finalWeights), verify: .noUnusedKeys)
+        eval(model.parameters())
+
+        model.modelDirectory = modelDir
+        model.processor = try await LFM2AudioProcessor.fromPretrained(modelDir, config: config)
+
+        return model
+    }
+
+    public var sampleRate: Int { config.sampleRate }
+}
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/Processor.swift b/Sources/MLXAudioSTS/Models/LFMAudio/Processor.swift
new file mode 100644
index 0000000..1b00a96
--- /dev/null
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/Processor.swift
@@ -0,0 +1,266 @@
+import Foundation
+import MLX
+import MLXAudioCore
+import MLXNN
+import Tokenizers
+
+// MARK: - Audio Preprocessor
+
+public class AudioPreprocessor {
+    let config: PreprocessorConfig
+    let melFilterbank: MLXArray
+
+    public init(_ config: PreprocessorConfig) {
+        self.config = config
+        self.melFilterbank = melFilters(
+            sampleRate: config.sampleRate,
+            nFft: config.nFft,
+            nMels: config.features,
+            fMin: 0.0,
+            fMax: Float(config.sampleRate / 2),
+            norm: "slaney",
+            melScale: .slaney
+        )
+    }
+
+    public func callAsFunction(_ audio: MLXArray) -> MLXArray {
+        let singleInput = audio.ndim == 1
+        let input = singleInput ? audio.expandedDimensions(axis: 0) : audio
+        let B = input.dim(0)
+        var featuresList: [MLXArray] = []
+
+        for i in 0..<B {
+            var waveform = input[i]
+
+            if config.dither > 0 {
+                waveform = waveform + MLXArray(config.dither) * MLXRandom.normal(waveform.shape)
+            }
+
+            if config.preemph > 0 {
+                let first = waveform[..<1]
+                let rest = waveform[1...] - MLXArray(config.preemph) * waveform[..<(waveform.dim(0) - 1)]
+                waveform = concatenated([first, rest])
+            }
+
+            let spec = stftConstantPad(
+                waveform, nFft: config.nFft,
+                hopLength: config.hopLength,
+                winLength: config.winLength
+            )
+
+            let powerSpec = MLX.abs(spec).square()
+            var melSpec = MLX.matmul(powerSpec, melFilterbank)
+
+            if config.log {
+                let logGuard: Float = 5.96e-8
+                melSpec = MLX.log(melSpec + MLXArray(logGuard))
+            }
+
+            if config.normalize == "per_feature" {
+                let validFrames = waveform.dim(0) / config.hopLength
+                let n = min(validFrames, melSpec.dim(0))
+                let validMel = melSpec[..<n]
+                let mean = MLX.mean(validMel, axis: 0, keepDims: true)
+                let variance = MLX.sum((validMel - mean).square(), axis: 0, keepDims: true) / MLXArray(Float(n - 1))
+                let std = MLX.sqrt(variance) + MLXArray(Float(1e-5))
+                melSpec = (melSpec - mean) / std
+            }
+
+            featuresList.append(melSpec)
+        }
+
+        let features = MLX.stacked(featuresList, axis: 0)
+        return singleInput ? features.squeezed(axis: 0) : features
+    }
+
+    private func stftConstantPad(
+        _ audio: MLXArray, nFft: Int, hopLength: Int, winLength: Int
+    ) -> MLXArray {
+        let padding = nFft / 2
+        let prefix = MLXArray.zeros([padding])
+        let suffix = MLXArray.zeros([padding])
+        let padded = concatenated([prefix, audio, suffix])
+
+        let paddedLen = padded.dim(0)
+        let numFrames = 1 + (paddedLen - nFft) / hopLength
+
+        let framesStacked = asStrided(padded, [numFrames, nFft], strides: [hopLength, 1], offset: 0)
+
+        let window = hanningWindow(size: winLength)
+        let effectiveWindow: MLXArray
+        if winLength < nFft {
+            let padLeft = (nFft - winLength) / 2
+            let padRight = nFft - winLength - padLeft
+            effectiveWindow = concatenated([MLXArray.zeros([padLeft]), window, MLXArray.zeros([padRight])])
+        } else {
+            effectiveWindow = window
+        }
+
+        let windowed = framesStacked * effectiveWindow
+        return MLXFFT.rfft(windowed, axis: 1)
+    }
+}
+
+// MARK: - Chat State
+
+public class ChatState {
+    public let processor: LFM2AudioProcessor
+    public var textTokens: [Int]
+    public var audioFeatures: MLXArray?
+    public var audioOutCodes: [MLXArray]
+    public var modalities: [LFMModality]
+    public var currentTurn: String?
+
+    public init(processor: LFM2AudioProcessor, addBos: Bool = true) {
+        self.processor = processor
+        self.textTokens = []
+        self.audioFeatures = nil
+        self.audioOutCodes = []
+        self.modalities = []
+        self.currentTurn = nil
+
+        if addBos {
+            textTokens.append(1) // BOS token
+            modalities.append(.text)
+        }
+    }
+
+    public func newTurn(role: String) {
+        currentTurn = role
+        let turnPrefix = "<|im_start|>\(role)\n"
+        let tokens = processor.tokenize(turnPrefix)
+        textTokens.append(contentsOf: tokens)
+        for _ in tokens { modalities.append(.text) }
+    }
+
+    public func endTurn() {
+        let tokens = processor.tokenize("<|im_end|>\n")
+        textTokens.append(contentsOf: tokens)
+        for _ in tokens { modalities.append(.text) }
+        currentTurn = nil
+    }
+
+    public func addText(_ text: String) {
+        let tokens = processor.tokenize(text)
+        textTokens.append(contentsOf: tokens)
+        for _ in tokens { modalities.append(.text) }
+    }
+
+    public func addAudioStartToken() {
+        textTokens.append(lfmAudioStartToken)
+        modalities.append(.text)
+    }
+
+    public func addAudio(_ audio: MLXArray, sampleRate: Int = 16000) {
+        let features = processor.preprocessAudio(audio, sampleRate: sampleRate)
+        if audioFeatures == nil {
+            audioFeatures = features
+        } else {
+            audioFeatures = concatenated([audioFeatures!, features], axis: 0)
+        }
+
+        func convOutput(_ inputLen: Int, kernel: Int = 3, stride: Int = 2, padding: Int = 1) -> Int {
+            (inputLen + 2 * padding - kernel) / stride + 1
+        }
+        let melFrames = features.dim(0)
+        var t = convOutput(melFrames)
+        t = convOutput(t)
+        t = convOutput(t)
+
+        for _ in 0..<t { modalities.append(.audioIn) }
+    }
+
+    public func append(token: MLXArray, modality: LFMModality) {
+        if modality == .text {
+            textTokens.append(token.item(Int.self))
+        } else if modality == .audioOut {
+            audioOutCodes.append(token)
+        }
+        modalities.append(modality)
+    }
+
+    public func getTextTokens() -> MLXArray {
+        MLXArray(textTokens.map { Int32($0) }).expandedDimensions(axis: 0)
+    }
+
+    public func getAudioFeatures() -> MLXArray? {
+        guard let af = audioFeatures else { return nil }
+        return af.ndim == 2 ? af.expandedDimensions(axis: 0) : af
+    }
+
+    public func getModalities() -> MLXArray {
+        MLXArray(modalities.map { Int32($0.rawValue) }).expandedDimensions(axis: 0)
+    }
+}
+
+// MARK: - LFM2 Audio Processor
+
+public class LFM2AudioProcessor {
+    public let config: LFM2AudioConfig
+    let audioPreprocessor: AudioPreprocessor
+    private var _tokenizer: Tokenizer?
+    public var modelPath: URL?
+
+    public init(_ config: LFM2AudioConfig) {
+        self.config = config
+        self.audioPreprocessor = AudioPreprocessor(config.preprocessor)
+    }
+
+    public var tokenizer: Tokenizer {
+        get throws {
+            if let t = _tokenizer { return t }
+            throw LFMAudioError.tokenizerNotLoaded
+        }
+    }
+
+    public func loadTokenizer() async throws {
+        guard let path = modelPath else {
+            throw LFMAudioError.tokenizerNotLoaded
+        }
+        _tokenizer = try await AutoTokenizer.from(modelFolder: path)
+    }
+
+    public func tokenize(_ text: String) -> [Int] {
+        do {
+            let tok = try tokenizer
+            return tok.encode(text: text, addSpecialTokens: false)
+        } catch {
+            return []
+        }
+    }
+
+    public func preprocessAudio(_ audio: MLXArray, sampleRate: Int = 16000) -> MLXArray {
+        audioPreprocessor(audio)
+    }
+
+    public func decodeText(_ tokens: [Int]) -> String {
+        do {
+            return try tokenizer.decode(tokens: tokens)
+        } catch {
+            return ""
+        }
+    }
+
+    public static func fromPretrained(_ modelPath: URL, config: LFM2AudioConfig) async throws -> LFM2AudioProcessor {
+        let processor = LFM2AudioProcessor(config)
+        processor.modelPath = modelPath
+        try await processor.loadTokenizer()
+        return processor
+    }
+}
+
+// MARK: - Errors
+
+public enum LFMAudioError: Error, LocalizedError {
+    case tokenizerNotLoaded
+    case modelNotFound(String)
+    case weightLoadingFailed(String)
+
+    public var errorDescription: String? {
+        switch self {
+        case .tokenizerNotLoaded: return "Tokenizer not loaded. Set modelPath first."
+        case .modelNotFound(let path): return "Model not found at: \(path)"
+        case .weightLoadingFailed(let msg): return "Weight loading failed: \(msg)"
+        }
+    }
+}
diff --git a/Sources/MLXAudioSTS/Models/LFMAudio/Transformer.swift b/Sources/MLXAudioSTS/Models/LFMAudio/Transformer.swift
new file mode 100644
index 0000000..ce98bc0
--- /dev/null
+++ b/Sources/MLXAudioSTS/Models/LFMAudio/Transformer.swift
@@ -0,0 +1,454 @@
+import Foundation
+import MLX
+import MLXLMCommon
+import MLXNN
+
+// MARK: - RoPE Utilities
+
+func precomputeFreqsCis(dim: Int, maxSeqLen: Int, theta: Float = 10000.0) -> MLXArray {
+    let freqs = 1.0 / MLX.pow(
+        MLXArray(theta),
+        MLXArray(stride(from: 0, to: dim, by: 2).map { Float($0) / Float(dim) })
+    )
+    let t = MLXArray(0..<Int32(maxSeqLen)).asType(.float32)
+    return MLX.outer(t, freqs)
+}
+
+func applyRotaryEmb(
+    xq: MLXArray, xk: MLXArray, freqs: MLXArray, offset: Int = 0
+) -> (MLXArray, MLXArray) {
+    let seqLen = xq.dim(1)
+    let f = freqs[offset..<(offset + seqLen)]
+    let fExpanded = f.expandedDimensions(axes: [0, 2])
+
+    let shape = xq.shape
+    let lastDim = shape[shape.count - 1]
+    let halfDim = lastDim / 2
+
+    let xqR = xq[0..., 0..., 0..., .stride(by: 2)]
+    let xqI = xq[0..., 0..., 0..., .stride(from: 1, by: 2)]
+    let xkR = xk[0..., 0..., 0..., .stride(by: 2)]
+    let xkI = xk[0..., 0..., 0..., .stride(from: 1, by: 2)]
+
+    let cosF = MLX.cos(fExpanded)
+    let sinF = MLX.sin(fExpanded)
+
+    let xqOutR = xqR * cosF - xqI * sinF
+    let xqOutI = xqR * sinF + xqI * cosF
+    let xkOutR = xkR * cosF - xkI * sinF
+    let xkOutI = xkR * sinF + xkI * cosF
+
+    let xqOut = MLX.stacked([xqOutR, xqOutI], axis: -1).reshaped(xq.shape)
+    let xkOut = MLX.stacked([xkOutR, xkOutI], axis: -1).reshaped(xk.shape)
+
+    return (xqOut, xkOut)
+}
+
+// MARK: - SwiGLU (for Depthformer)
+
+class DepthformerSwiGLU: Module {
+    @ModuleInfo(key: "w1") var w1: Linear
+    @ModuleInfo(key: "w2") var w2: Linear
+    @ModuleInfo(key: "w3") var w3: Linear
+
+    init(dim: Int, hiddenDim: Int, multipleOf: Int = 256) {
+        var adjDim = Int(2.0 * Float(hiddenDim) / 3.0)
+        adjDim = multipleOf * ((adjDim + multipleOf - 1) / multipleOf)
+        self._w1.wrappedValue = Linear(dim, adjDim, bias: false)
+        self._w2.wrappedValue = Linear(adjDim, dim, bias: false)
+        self._w3.wrappedValue = Linear(dim, adjDim, bias: false)
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        w2(silu(w1(x)) * w3(x))
+    }
+}
+
+// MARK: - Depthformer Attention
+
+class DepthformerAttention: Module {
+    let numHeads: Int
+    let numKvHeads: Int
+    let headDim: Int
+    let scale: Float
+    let useQkNorm: Bool
+
+    @ModuleInfo(key: "q_proj") var qProj: Linear
+    @ModuleInfo(key: "k_proj") var kProj: Linear
+    @ModuleInfo(key: "v_proj") var vProj: Linear
+    @ModuleInfo(key: "o_proj") var oProj: Linear
+    @ModuleInfo(key: "q_norm") var qNorm: RMSNorm?
+    @ModuleInfo(key: "k_norm") var kNorm: RMSNorm?
+
+    let freqs: MLXArray
+
+    init(
+        dim: Int, numHeads: Int, numKvHeads: Int,
+        maxSeqLen: Int = 4096, ropeTheta: Float = 10000.0,
+        useQkNorm: Bool = true
+    ) {
+        self.numHeads = numHeads
+        self.numKvHeads = numKvHeads
+        self.headDim = dim / numHeads
+        self.scale = pow(Float(self.headDim), -0.5)
+        self.useQkNorm = useQkNorm
+
+        self._qProj.wrappedValue = Linear(dim, numHeads * self.headDim, bias: false)
+        self._kProj.wrappedValue = Linear(dim, numKvHeads * self.headDim, bias: false)
+        self._vProj.wrappedValue = Linear(dim, numKvHeads * self.headDim, bias: false)
+        self._oProj.wrappedValue = Linear(numHeads * self.headDim, dim, bias: false)
+
+        if useQkNorm {
+            self._qNorm.wrappedValue = RMSNorm(dimensions: self.headDim)
+            self._kNorm.wrappedValue = RMSNorm(dimensions: self.headDim)
+        }
+
+        self.freqs = precomputeFreqsCis(dim: self.headDim, maxSeqLen: maxSeqLen, theta: ropeTheta)
+    }
+
+    func callAsFunction(
+        _ x: MLXArray, mask: MLXArray? = nil,
+        cache: (MLXArray, MLXArray)? = nil
+    ) -> (MLXArray, (MLXArray, MLXArray)) {
+        let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))
+
+        var q = qProj(x).reshaped(B, L, numHeads, headDim)
+        var k = kProj(x).reshaped(B, L, numKvHeads, headDim)
+        var v = vProj(x).reshaped(B, L, numKvHeads, headDim)
+
+        if useQkNorm, let qN = qNorm, let kN = kNorm {
+            q = qN(q)
+            k = kN(k)
+        }
+
+        let offset = cache?.0.dim(1) ?? 0
+        let (qRot, kRot) = applyRotaryEmb(xq: q, xk: k, freqs: freqs, offset: offset)
+        q = qRot
+        k = kRot
+
+        if let (kCache, vCache) = cache {
+            k = concatenated([kCache, k], axis: 1)
+            v = concatenated([vCache, v], axis: 1)
+        }
+
+        let newCache = (k, v)
+
+        var qT = q.transposed(0, 2, 1, 3)
+        var kT = k.transposed(0, 2, 1, 3)
+        var vT = v.transposed(0, 2, 1, 3)
+
+        if numKvHeads < numHeads {
+            let nRep = numHeads / numKvHeads
+            kT = MLX.repeated(kT, count: nRep, axis: 1)
+            vT = MLX.repeated(vT, count: nRep, axis: 1)
+        }
+
+        var scores = MLX.matmul(qT, kT.transposed(0, 1, 3, 2)) * MLXArray(scale)
+        if let mask = mask {
+            scores = scores + mask
+        }
+
+        let attn = softmax(scores, axis: -1)
+        let out = MLX.matmul(attn, vT).transposed(0, 2, 1, 3).reshaped(B, L, -1)
+        return (oProj(out), newCache)
+    }
+}
+
+// MARK: - Depthformer Block
+
+class DepthformerBlock: Module {
+    @ModuleInfo(key: "attn_norm") var attnNorm: RMSNorm
+    @ModuleInfo(key: "attn") var attn: DepthformerAttention
+    @ModuleInfo(key: "ffn_norm") var ffnNorm: RMSNorm
+    @ModuleInfo(key: "ffn") var ffn: DepthformerSwiGLU
+
+    init(
+        dim: Int, numHeads: Int, numKvHeads: Int, ffDim: Int,
+        maxSeqLen: Int = 4096, ropeTheta: Float = 10000.0,
+        normEps: Float = 1e-5, multipleOf: Int = 256, useQkNorm: Bool = true
+    ) {
+        self._attnNorm.wrappedValue = RMSNorm(dimensions: dim, eps: normEps)
+        self._attn.wrappedValue = DepthformerAttention(
+            dim: dim, numHeads: numHeads, numKvHeads: numKvHeads,
+            maxSeqLen: maxSeqLen, ropeTheta: ropeTheta, useQkNorm: useQkNorm
+        )
+        self._ffnNorm.wrappedValue = RMSNorm(dimensions: dim, eps: normEps)
+        self._ffn.wrappedValue = DepthformerSwiGLU(dim: dim, hiddenDim: ffDim, multipleOf: multipleOf)
+    }
+
+    func callAsFunction(
+        _ x: MLXArray, mask: MLXArray? = nil,
+        cache: (MLXArray, MLXArray)? = nil
+    ) -> (MLXArray, (MLXArray, MLXArray)) {
+        let (h, newCache) = attn(attnNorm(x), mask: mask, cache: cache)
+        var out = x + h
+        out = out + ffn(ffnNorm(out))
+        return (out, newCache)
+    }
+}
+
+// MARK: - Depthformer
+
+public class Depthformer: Module {
+    public let layersCount: Int
+    let dim: Int
+    let blocks: [DepthformerBlock]
+
+    public init(layers: Int, dim: Int, numHeads: Int = 32, numKvHeads: Int = 8,
+                ffDim: Int? = nil, tie: Bool = true) {
+        self.layersCount = layers
+        self.dim = dim
+        let effectiveFFDim = ffDim ?? (dim * 4)
+
+        self.blocks = (0..<layers).map { _ in
+            DepthformerBlock(
+                dim: dim, numHeads: numHeads, numKvHeads: numKvHeads,
+                ffDim: effectiveFFDim, maxSeqLen: 4096, ropeTheta: 10000.0,
+                useQkNorm: true
+            )
+        }
+    }
+
+    public func callAsFunction(
+        _ x: MLXArray, cache: [(MLXArray, MLXArray)?]? = nil,
+        useCache: Bool = false
+    ) -> (MLXArray, [(MLXArray, MLXArray)]?) {
+        var h = x
+        var newCache: [(MLXArray, MLXArray)]? = useCache ? [] : nil
+
+        for i in 0..<layersCount {
+            let layerCache = cache?[i]
+            let (out, lc) = blocks[i](h, cache: layerCache)
+            h = out
+            newCache?.append(lc)
+        }
+
+        return (h, newCache)
+    }
+}
+
+// MARK: - LFM2 Backbone
+
+class Lfm2Attention: Module {
+    let args: LFM2BackboneConfig
+    let scale: Float
+    let headDim: Int
+
+    @ModuleInfo(key: "q_proj") var qProj: Linear
+    @ModuleInfo(key: "k_proj") var kProj: Linear
+    @ModuleInfo(key: "v_proj") var vProj: Linear
+    @ModuleInfo(key: "out_proj") var outProj: Linear
+    @ModuleInfo(key: "q_layernorm") var qLayernorm: RMSNorm
+    @ModuleInfo(key: "k_layernorm") var kLayernorm: RMSNorm
+    let rope: RoPE
+
+    init(_ args: LFM2BackboneConfig) {
+        self.args = args
+        self.headDim = args.headDimensions
+        self.scale = pow(Float(headDim), -0.5)
+
+        let dim = args.hiddenSize
+        let heads = args.numAttentionHeads
+        let kvHeads = args.numKeyValueHeads
+
+        self._qProj.wrappedValue = Linear(dim, heads * headDim, bias: false)
+        self._kProj.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
+        self._vProj.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)
+        self._outProj.wrappedValue = Linear(heads * headDim, dim, bias: false)
+        self._qLayernorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.normEps)
+        self._kLayernorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.normEps)
+        self.rope = RoPE(dimensions: headDim, traditional: false, base: args.ropeTheta)
+    }
+
+    func callAsFunction(
+        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
+    ) -> MLXArray {
+        let (B, L) = (x.dim(0), x.dim(1))
+
+        var queries = qProj(x)
+        var keys = kProj(x)
+        var values = vProj(x)
+
+        queries = queries.reshaped(B, L, args.numAttentionHeads, headDim).transposed(0, 2, 1, 3)
+        keys = keys.reshaped(B, L, args.numKeyValueHeads, headDim).transposed(0, 2, 1, 3)
+        values = values.reshaped(B, L, args.numKeyValueHeads, headDim).transposed(0, 2, 1, 3)
+
+        queries = qLayernorm(queries)
+        keys = kLayernorm(keys)
+
+        if let cache {
+            queries = rope(queries, offset: cache.offset)
+            keys = rope(keys, offset: cache.offset)
+            (keys, values) = cache.update(keys: keys, values: values)
+        } else {
+            queries = rope(queries)
+            keys = rope(keys)
+        }
+
+        let output = MLXFast.scaledDotProductAttention(
+            queries: queries, keys: keys, values: values,
+            scale: scale, mask: mask
+        ).transposed(0, 2, 1, 3).reshaped(B, L, -1)
+
+        return outProj(output)
+    }
+}
+
+class Lfm2ShortConv: Module {
+    let lCache: Int
+    let hiddenSize: Int
+    let bias: Bool
+
+    @ModuleInfo(key: "conv") var conv: Conv1d
+    @ModuleInfo(key: "in_proj") var inProj: Linear
+    @ModuleInfo(key: "out_proj") var outProj: Linear
+
+    init(_ args: LFM2BackboneConfig, layerIdx: Int) {
+        self.lCache = args.convLCache
+        self.hiddenSize = args.hiddenSize
+        self.bias = args.convBias
+
+        self._conv.wrappedValue = Conv1d(
+            inputChannels: args.hiddenSize, outputChannels: args.hiddenSize,
+            kernelSize: lCache, groups: args.hiddenSize, bias: args.convBias
+        )
+        self._inProj.wrappedValue = Linear(args.hiddenSize, 3 * args.hiddenSize, bias: args.convBias)
+        self._outProj.wrappedValue = Linear(args.hiddenSize, args.hiddenSize, bias: args.convBias)
+    }
+
+    func callAsFunction(_ x: MLXArray, cache: MambaCache?) -> MLXArray {
+        let projected = inProj(x).split(parts: 3, axis: -1)
+        let b = projected[0], c = projected[1], xIn = projected[2]
+        let bx = b * xIn
+
+        var state: MLXArray? = cache?[0]
+        if state == nil {
+            state = MLXArray.zeros([bx.dim(0), lCache - 1, hiddenSize], dtype: bx.dtype)
+        }
+
+        let xConv = concatenated([state!, bx], axis: -2)
+        if let cache {
+            cache[0] = xConv[0..., (xConv.dim(1) - (lCache - 1))..., 0...]
+        }
+
+        let convOut = conv(xConv)
+        let y = c * convOut
+        return outProj(y)
+    }
+}
+
+class Lfm2MLP: Module {
+    @ModuleInfo(key: "w1") var w1: Linear
+    @ModuleInfo(key: "w2") var w2: Linear
+    @ModuleInfo(key: "w3") var w3: Linear
+
+    init(dim: Int, ffDim: Int, multipleOf: Int, autoAdjust: Bool, multiplier: Float?) {
+        var adjDim = ffDim
+        if autoAdjust {
+            adjDim = Int(Float(2 * ffDim) / 3.0)
+            if let m = multiplier { adjDim = Int(m * Float(adjDim)) }
+            adjDim = multipleOf * ((adjDim + multipleOf - 1) / multipleOf)
+        }
+        self._w1.wrappedValue = Linear(dim, adjDim, bias: false)
+        self._w2.wrappedValue = Linear(adjDim, dim, bias: false)
+        self._w3.wrappedValue = Linear(dim, adjDim, bias: false)
+    }
+
+    func callAsFunction(_ x: MLXArray) -> MLXArray {
+        w2(silu(w1(x)) * w3(x))
+    }
+}
+
+class Lfm2DecoderLayer: Module {
+    let isAttentionLayer: Bool
+
+    @ModuleInfo(key: "self_attn") var attention: Lfm2Attention?
+    @ModuleInfo(key: "conv") var conv: Lfm2ShortConv?
+    @ModuleInfo(key: "feed_forward") var feedForward: Lfm2MLP
+    @ModuleInfo(key: "operator_norm") var operatorNorm: RMSNorm
+    @ModuleInfo(key: "ffn_norm") var ffnNorm: RMSNorm
+
+    init(_ args: LFM2BackboneConfig, layerIdx: Int) {
+        self.isAttentionLayer = args.resolvedFullAttnIdxs.contains(layerIdx)
+
+        if isAttentionLayer {
+            self._attention.wrappedValue = Lfm2Attention(args)
+        } else {
+            self._conv.wrappedValue = Lfm2ShortConv(args, layerIdx: layerIdx)
+        }
+
+        self._feedForward.wrappedValue = Lfm2MLP(
+            dim: args.effectiveBlockDim, ffDim: args.effectiveBlockFFDim,
+            multipleOf: args.blockMultipleOf, autoAdjust: args.blockAutoAdjustFFDim,
+            multiplier: args.blockFFNDimMultiplier
+        )
+        self._operatorNorm.wrappedValue = RMSNorm(dimensions: args.hiddenSize, eps: args.normEps)
+        self._ffnNorm.wrappedValue = RMSNorm(dimensions: args.hiddenSize, eps: args.normEps)
+    }
+
+    func callAsFunction(
+        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?
+    ) -> MLXArray {
+        let r: MLXArray
+        if isAttentionLayer {
+            r = attention!(operatorNorm(x), mask: mask, cache: cache)
+        } else {
+            r = conv!(operatorNorm(x), cache: cache as? MambaCache)
+        }
+        let h = x + r
+        return h + feedForward(ffnNorm(h))
+    }
+}
+
+public class Lfm2Model: Module {
+    let args: LFM2BackboneConfig
+    let layers: [Lfm2DecoderLayer]
+
+    @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
+    @ModuleInfo(key: "embedding_norm") var embeddingNorm: RMSNorm
+
+    public init(_ args: LFM2BackboneConfig) {
+        self.args = args
+
+        self._embedTokens.wrappedValue = Embedding(
+            embeddingCount: args.vocabSize, dimensions: args.hiddenSize
+        )
+        self.layers = (0..<args.numHiddenLayers).map { Lfm2DecoderLayer(args, layerIdx: $0) }
+        self._embeddingNorm.wrappedValue = RMSNorm(dimensions: args.hiddenSize, eps: args.normEps)
+    }
+
+    public func callAsFunction(
+        _ inputs: MLXArray? = nil,
+        inputEmbeddings: MLXArray? = nil,
+        cache: [KVCache]? = nil
+    ) -> MLXArray {
+        var h: MLXArray
+        if let emb = inputEmbeddings {
+            h = emb
+        } else {
+            h = embedTokens(inputs!)
+        }
+
+        let mask: MLXFast.ScaledDotProductAttentionMaskMode = {
+            let firstAttnIdx = args.resolvedFullAttnIdxs.first ?? 0
+            let c = (cache != nil && firstAttnIdx < cache!.count) ? cache![firstAttnIdx] : nil
+            return createAttentionMask(h: h, cache: c)
+        }()
+
+        for (i, layer) in layers.enumerated() {
+            h = layer(h, mask: mask, cache: cache?[i])
+        }
+
+        return embeddingNorm(h)
+    }
+
+    public func makeCache() -> [KVCache] {
+        (0..<args.numHiddenLayers).map { layerIdx in
+            if args.resolvedFullAttnIdxs.contains(layerIdx) {
+                KVCacheSimple()
+            } else {
+                MambaCache()
+            }
+        }
+    }
+}
diff --git a/Sources/Tools/mlx-audio-swift-sts/App.swift b/Sources/Tools/mlx-audio-swift-sts/App.swift
index 16cfa2a..201ad24 100644
--- a/Sources/Tools/mlx-audio-swift-sts/App.swift
+++ b/Sources/Tools/mlx-audio-swift-sts/App.swift
@@ -9,6 +9,9 @@ enum AppError: Error, LocalizedError, CustomStringConvertible {
     case anchorsUnsupportedForMode(SeparationMode)
     case failedToCreateAudioBuffer
     case failedToAccessAudioBufferData
+    case unsupportedModelRepo(String)
+    case lfmRequiresText
+    case lfmRequiresAudioForMode(LFMMode)
 
     var errorDescription: String? { description }
 
@@ -22,6 +25,12 @@ enum AppError: Error, LocalizedError, CustomStringConvertible {
             "Failed to create audio buffer"
         case .failedToAccessAudioBufferData:
             "Failed to access audio buffer data"
+        case .unsupportedModelRepo(let repo):
+            "Unsupported STS model repo: \(repo). Expected SAMAudio or LFM model."
+        case .lfmRequiresText:
+            "--text is required for LFM text-to-text and text-to-speech modes."
+        case .lfmRequiresAudioForMode(let mode):
+            "--audio is required for LFM \(mode.rawValue) mode."
         }
     }
 }
@@ -32,28 +41,24 @@ enum SeparationMode: String {
     case stream
 }
 
+enum LFMMode: String {
+    case t2t
+    case tts
+    case stt
+    case sts
+}
+
 @main
 enum App {
     static func main() async {
         do {
             let args = try CLI.parse()
-            try await run(
-                modelRepo: args.model,
-                audioPath: args.audioPath,
-                description: args.description,
-                mode: args.mode,
-                outputTargetPath: args.outputTargetPath,
-                outputResidualPath: args.outputResidualPath,
-                writeResidual: args.writeResidual,
-                chunkSeconds: args.chunkSeconds,
-                overlapSeconds: args.overlapSeconds,
-                odeMethod: args.odeMethod,
-                stepSize: args.stepSize,
-                odeDecodeChunkSize: args.odeDecodeChunkSize,
-                anchors: args.anchors,
-                strict: args.strict,
-                hfToken: args.hfToken
-            )
+
+            if isLFMModel(args.model) {
+                try await runLFM(args: args)
+            } else {
+                try await runSAMAudio(args: args)
+            }
         } catch {
             fputs("Error: \(error)\n", stderr)
             CLI.printUsage()
@@ -61,69 +66,252 @@ enum App {
         }
     }
 
-    private static func run(
-        modelRepo: String,
-        audioPath: String,
-        description: String,
-        mode: SeparationMode,
-        outputTargetPath: String?,
-        outputResidualPath: String?,
-        writeResidual: Bool,
-        chunkSeconds: Float,
-        overlapSeconds: Float,
-        odeMethod: SAMAudioODEMethod,
-        stepSize: Float,
-        odeDecodeChunkSize: Int?,
-        anchors: [SAMAudioAnchor],
-        strict: Bool,
-        hfToken: String?
-    ) async throws {
+    private static func isLFMModel(_ model: String) -> Bool {
+        let lower = model.lowercased()
+        return lower.contains("lfm") || lower.contains("lfm2")
+    }
+
+    // MARK: - LFM2.5-Audio
+
+    private static func runLFM(args: CLI) async throws {
+        let lfmMode = args.lfmMode ?? .sts
+
+        switch lfmMode {
+        case .t2t, .tts:
+            guard let text = args.text, !text.isEmpty else {
+                throw AppError.lfmRequiresText
+            }
+        case .stt, .sts:
+            guard args.audioPath != nil else {
+                throw AppError.lfmRequiresAudioForMode(lfmMode)
+            }
+        }
+
+        print("Loading LFM2.5-Audio model (\(args.model))")
+        let model = try await LFM2AudioModel.fromPretrained(args.model)
+        let processor = model.processor!
+
+        let chat = ChatState(processor: processor)
+
+        let defaultSystemPrompts: [LFMMode: String] = [
+            .t2t: "You are a helpful assistant.",
+            .tts: "Perform TTS. Use a UK male voice.",
+            .stt: "You are a helpful assistant that transcribes audio.",
+            .sts: "Respond to the user with interleaved text and speech audio. Use a UK male voice.",
+        ]
+        let systemPrompt = args.systemPrompt ?? defaultSystemPrompts[lfmMode]!
+
+        switch lfmMode {
+        case .t2t:
+            chat.newTurn(role: "system")
+            chat.addText(systemPrompt)
+            chat.endTurn()
+            chat.newTurn(role: "user")
+            chat.addText(args.text!)
+            chat.endTurn()
+            chat.newTurn(role: "assistant")
+
+        case .tts:
+            chat.newTurn(role: "system")
+            chat.addText(systemPrompt)
+            chat.endTurn()
+            chat.newTurn(role: "user")
+            chat.addText(args.text!)
+            chat.endTurn()
+            chat.newTurn(role: "assistant")
+            chat.addAudioStartToken()
+
+        case .stt:
+            let inputURL = resolveURL(path: args.audioPath!)
+            guard FileManager.default.fileExists(atPath: inputURL.path) else {
+                throw AppError.inputFileNotFound(inputURL.path)
+            }
+            let (sampleRate, audioData) = try loadAudioArray(from: inputURL)
+            chat.newTurn(role: "system")
+            chat.addText(systemPrompt)
+            chat.endTurn()
+            chat.newTurn(role: "user")
+            chat.addAudio(audioData, sampleRate: sampleRate)
+            chat.addText(args.text ?? "Transcribe the audio.")
+            chat.endTurn()
+            chat.newTurn(role: "assistant")
+
+        case .sts:
+            let inputURL = resolveURL(path: args.audioPath!)
+            guard FileManager.default.fileExists(atPath: inputURL.path) else {
+                throw AppError.inputFileNotFound(inputURL.path)
+            }
+            let (sampleRate, audioData) = try loadAudioArray(from: inputURL)
+            chat.newTurn(role: "system")
+            chat.addText(systemPrompt)
+            chat.endTurn()
+            chat.newTurn(role: "user")
+            chat.addAudio(audioData, sampleRate: sampleRate)
+            if let text = args.text {
+                chat.addText(text)
+            }
+            chat.endTurn()
+            chat.newTurn(role: "assistant")
+        }
+
+        let genConfig = LFMGenerationConfig(
+            maxNewTokens: args.maxNewTokens,
+            temperature: args.temperature,
+            topK: args.topK,
+            audioTemperature: args.audioTemperature,
+            audioTopK: args.audioTopK
+        )
+
+        let started = CFAbsoluteTimeGetCurrent()
+        print("Generating (mode=\(lfmMode.rawValue))")
+
+        var textTokens: [Int] = []
+        var audioCodes: [MLXArray] = []
+
+        let useSequential = (lfmMode == .tts)
+        let collectText = (lfmMode == .t2t || lfmMode == .stt || lfmMode == .sts)
+        let collectAudio = (lfmMode == .tts || lfmMode == .sts)
+
+        let stream: AsyncThrowingStream<(MLXArray, LFMModality), Error>
+
+        if useSequential {
+            stream = model.generateSequential(
+                textTokens: chat.getTextTokens(),
+                audioFeatures: chat.getAudioFeatures(),
+                modalities: chat.getModalities(),
+                config: genConfig
+            )
+        } else {
+            stream = model.generateInterleaved(
+                textTokens: chat.getTextTokens(),
+                audioFeatures: chat.getAudioFeatures(),
+                modalities: chat.getModalities(),
+                config: genConfig
+            )
+        }
+
+        for try await (token, modality) in stream {
+            eval(token)
+            if modality == .text && collectText {
+                let tokenId = token.item(Int.self)
+                textTokens.append(tokenId)
+                if args.stream {
+                    print(processor.decodeText([tokenId]), terminator: "")
+                    fflush(stdout)
+                }
+            } else if modality == .audioOut && collectAudio {
+                if useSequential {
+                    if token[0].item(Int.self) == lfmAudioEOSToken { break }
+                    audioCodes.append(token)
+                } else {
+                    if token[0].item(Int.self) != lfmAudioEOSToken {
+                        audioCodes.append(token)
+                    }
+                }
+            }
+        }
+
+        if args.stream && !textTokens.isEmpty { print() }
+
+        let elapsed = CFAbsoluteTimeGetCurrent() - started
+
+        if collectText && !textTokens.isEmpty {
+            let decodedText = processor.decodeText(textTokens)
+            if !args.stream {
+                print("Text: \(decodedText)")
+            }
+            print("Generated \(textTokens.count) text tokens")
+
+            if let outputPath = args.outputTextPath {
+                let url = resolveURL(path: outputPath)
+                try decodedText.write(to: url, atomically: true, encoding: .utf8)
+                print("Wrote text to \(url.path)")
+            }
+        }
+
+        if collectAudio && !audioCodes.isEmpty {
+            print("Generated \(audioCodes.count) audio frames")
+            let stacked = MLX.stacked(audioCodes, axis: 0)
+            let codesInput = stacked.transposed(1, 0).expandedDimensions(axis: 0)
+            eval(codesInput)
+
+            let detokenizer = try LFM2AudioDetokenizer.fromPretrained(modelPath: model.modelDirectory!)
+            let waveform = detokenizer(codesInput)
+            eval(waveform)
+            let samples = waveform[0].asArray(Float.self)
+
+            let duration = Double(samples.count) / 24000.0
+            print(String(format: "Decoded %d audio samples (%.1fs at 24kHz)", samples.count, duration))
+
+            let outputURL: URL
+            if let path = args.outputTargetPath {
+                outputURL = resolveURL(path: path)
+            } else {
+                outputURL = resolveURL(path: "lfm_output.wav")
+            }
+
+            try AudioUtils.writeWavFile(samples: samples, sampleRate: 24000, fileURL: outputURL)
+            print("Wrote WAV to \(outputURL.path)")
+        }
+
+        print(String(format: "Done. Elapsed: %.2fs", elapsed))
+    }
+
+    // MARK: - SAM Audio
+
+    private static func runSAMAudio(args: CLI) async throws {
+        let mode = args.mode
+
+        guard let audioPath = args.audioPath else {
+            throw AppError.inputFileNotFound("(none)")
+        }
+
         let inputURL = resolveURL(path: audioPath)
         guard FileManager.default.fileExists(atPath: inputURL.path) else {
             throw AppError.inputFileNotFound(inputURL.path)
         }
 
-        if !anchors.isEmpty, mode != .short {
+        if !args.anchors.isEmpty, mode != .short {
             throw AppError.anchorsUnsupportedForMode(mode)
         }
 
-        let resolvedHFToken = hfToken
+        let resolvedHFToken = args.hfToken
             ?? ProcessInfo.processInfo.environment["HF_TOKEN"]
             ?? Bundle.main.object(forInfoDictionaryKey: "HF_TOKEN") as? String
 
-        print("Loading SAM Audio model (\(modelRepo))")
+        print("Loading SAM Audio model (\(args.model))")
         let model = try await SAMAudio.fromPretrained(
-            modelRepo,
+            args.model,
             hfToken: resolvedHFToken,
-            strict: strict
+            strict: args.strict
         )
 
         let targetOutputURL = makeOutputURL(
-            outputPath: outputTargetPath,
+            outputPath: args.outputTargetPath,
             inputURL: inputURL,
             defaultSuffix: "target.wav"
         )
 
         let residualOutputURL = makeOutputURL(
-            outputPath: outputResidualPath,
+            outputPath: args.outputResidualPath,
             inputURL: inputURL,
             defaultSuffix: "residual.wav"
         )
 
-        let ode = SAMAudioODEOptions(method: odeMethod, stepSize: stepSize)
+        let ode = SAMAudioODEOptions(method: args.odeMethod, stepSize: args.stepSize)
 
-        print("Running SAM Audio (mode=\(mode.rawValue), description=\"\(description)\")")
+        print("Running SAM Audio (mode=\(mode.rawValue), description=\"\(args.description)\")")
         let started = CFAbsoluteTimeGetCurrent()
 
         switch mode {
         case .short:
             let result = try await model.separate(
                 audioPaths: [inputURL.path],
-                descriptions: [description],
-                anchors: anchors.isEmpty ? nil : [anchors],
+                descriptions: [args.description],
+                anchors: args.anchors.isEmpty ? nil : [args.anchors],
                 noise: nil,
                 ode: ode,
-                odeDecodeChunkSize: odeDecodeChunkSize
+                odeDecodeChunkSize: args.odeDecodeChunkSize
             )
 
             try writeWavArray(
@@ -133,7 +321,7 @@ enum App {
             )
             print("Wrote target WAV to \(targetOutputURL.path)")
 
-            if writeResidual {
+            if args.writeResidual {
                 try writeWavArray(
                     result.residual[0],
                     sampleRate: Double(model.sampleRate),
@@ -145,11 +333,11 @@ enum App {
         case .long:
             let result = try await model.separateLong(
                 audioPaths: [inputURL.path],
-                descriptions: [description],
-                chunkSeconds: chunkSeconds,
-                overlapSeconds: overlapSeconds,
+                descriptions: [args.description],
+                chunkSeconds: args.chunkSeconds,
+                overlapSeconds: args.overlapSeconds,
                 ode: ode,
-                odeDecodeChunkSize: odeDecodeChunkSize
+                odeDecodeChunkSize: args.odeDecodeChunkSize
             )
 
             try writeWavArray(
@@ -159,7 +347,7 @@ enum App {
             )
             print("Wrote target WAV to \(targetOutputURL.path)")
 
-            if writeResidual {
+            if args.writeResidual {
                 try writeWavArray(
                     result.residual[0],
                     sampleRate: Double(model.sampleRate),
@@ -172,13 +360,13 @@ enum App {
             try await separateStreamingToDisk(
                 model: model,
                 audioPath: inputURL.path,
-                description: description,
-                chunkSeconds: chunkSeconds,
-                overlapSeconds: overlapSeconds,
+                description: args.description,
+                chunkSeconds: args.chunkSeconds,
+                overlapSeconds: args.overlapSeconds,
                 ode: ode,
                 targetOutputURL: targetOutputURL,
                 residualOutputURL: residualOutputURL,
-                writeResidual: writeResidual
+                writeResidual: args.writeResidual
             )
         }
 
@@ -233,6 +421,8 @@ enum App {
         print("Streamed \(chunks) chunk(s)")
     }
 
+    // MARK: - Helpers
+
     private static func resolveURL(path: String) -> URL {
         if path.hasPrefix("/") {
             return URL(fileURLWithPath: path)
@@ -277,6 +467,8 @@ enum App {
     }
 }
 
+// MARK: - CLI
+
 enum CLIError: Error, CustomStringConvertible {
     case missingValue(String)
     case unknownOption(String)
@@ -295,11 +487,16 @@ enum CLIError: Error, CustomStringConvertible {
 }
 
 struct CLI {
-    let audioPath: String
     let model: String
+    let audioPath: String?
+    let text: String?
+    let outputTargetPath: String?
+    let outputTextPath: String?
+    let hfToken: String?
+    let stream: Bool
+
     let description: String
     let mode: SeparationMode
-    let outputTargetPath: String?
     let outputResidualPath: String?
     let writeResidual: Bool
     let chunkSeconds: Float
@@ -309,15 +506,24 @@ struct CLI {
     let odeDecodeChunkSize: Int?
     let anchors: [SAMAudioAnchor]
     let strict: Bool
-    let hfToken: String?
+
+    let lfmMode: LFMMode?
+    let systemPrompt: String?
+    let maxNewTokens: Int
+    let temperature: Float
+    let topK: Int
+    let audioTemperature: Float
+    let audioTopK: Int
 
     static func parse() throws -> CLI {
         var audioPath: String?
         var model = SAMAudio.defaultRepo
+        var text: String?
         var description = "speech"
         var mode: SeparationMode = .short
         var outputTargetPath: String?
         var outputResidualPath: String?
+        var outputTextPath: String?
         var writeResidual = true
         var chunkSeconds: Float = 10.0
         var overlapSeconds: Float = 3.0
@@ -327,6 +533,15 @@ struct CLI {
         var anchors: [SAMAudioAnchor] = []
         var strict = false
         var hfToken: String?
+        var stream = false
+
+        var lfmMode: LFMMode?
+        var systemPrompt: String?
+        var maxNewTokens = 512
+        var temperature: Float = 0.8
+        var topK = 50
+        var audioTemperature: Float = 0.7
+        var audioTopK = 30
 
         var iterator = CommandLine.arguments.dropFirst().makeIterator()
         while let arg = iterator.next() {
@@ -337,18 +552,52 @@ struct CLI {
             case "--model":
                 guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
                 model = value
+            case "--text", "-t":
+                guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
+                text = value
             case "--description", "--prompt", "-d":
                 guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
                 description = value
             case "--mode":
                 guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
-                guard let parsed = SeparationMode(rawValue: value.lowercased()) else {
+                if let parsed = LFMMode(rawValue: value.lowercased()) {
+                    lfmMode = parsed
+                } else if let parsed = SeparationMode(rawValue: value.lowercased()) {
+                    mode = parsed
+                } else {
                     throw CLIError.invalidValue(arg, value)
                 }
-                mode = parsed
+            case "--system":
+                guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
+                systemPrompt = value
+            case "--max-new-tokens":
+                guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
+                guard let parsed = Int(value) else { throw CLIError.invalidValue(arg, value) }
+                maxNewTokens = parsed
+            case "--temperature":
+                guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
+                guard let parsed = Float(value) else { throw CLIError.invalidValue(arg, value) }
+                temperature = parsed
+            case "--top-k":
+                guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
+                guard let parsed = Int(value) else { throw CLIError.invalidValue(arg, value) }
+                topK = parsed
+            case "--audio-temperature":
+                guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
+                guard let parsed = Float(value) else { throw CLIError.invalidValue(arg, value) }
+                audioTemperature = parsed
+            case "--audio-top-k":
+                guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
+                guard let parsed = Int(value) else { throw CLIError.invalidValue(arg, value) }
+                audioTopK = parsed
+            case "--stream":
+                stream = true
             case "--output-target", "-o":
                 guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
                 outputTargetPath = value
+            case "--output-text":
+                guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
+                outputTextPath = value
             case "--output-residual":
                 guard let value = iterator.next() else { throw CLIError.missingValue(arg) }
                 outputResidualPath = value
@@ -396,16 +645,16 @@ struct CLI {
             }
         }
 
-        guard let finalAudioPath = audioPath, !finalAudioPath.isEmpty else {
-            throw CLIError.missingValue("--audio")
-        }
-
         return CLI(
-            audioPath: finalAudioPath,
             model: model,
+            audioPath: audioPath,
+            text: text,
+            outputTargetPath: outputTargetPath,
+            outputTextPath: outputTextPath,
+            hfToken: hfToken,
+            stream: stream,
             description: description,
             mode: mode,
-            outputTargetPath: outputTargetPath,
             outputResidualPath: outputResidualPath,
             writeResidual: writeResidual,
             chunkSeconds: chunkSeconds,
@@ -415,7 +664,13 @@ struct CLI {
             odeDecodeChunkSize: odeDecodeChunkSize,
             anchors: anchors,
             strict: strict,
-            hfToken: hfToken
+            lfmMode: lfmMode,
+            systemPrompt: systemPrompt,
+            maxNewTokens: maxNewTokens,
+            temperature: temperature,
+            topK: topK,
+            audioTemperature: audioTemperature,
+            audioTopK: audioTopK
         )
     }
 
@@ -442,41 +697,53 @@ struct CLI {
         print(
             """
             Usage:
-              \(executable) --audio <path> [--description <text>] [--mode short|long|stream] [options]
+              \(executable) [--model <repo>] [--mode <mode>] [options]
 
             Description:
-              Runs SAM Audio source separation and writes target/residual WAV output.
-
-            Options:
-              -i, --audio <path>           Input audio file path (required if not passed as trailing arg)
-                  --model <repo-or-path>   SAM Audio model repo or local folder.
-                                           Default: \(SAMAudio.defaultRepo)
-              -d, --description <text>     Text prompt describing target sound.
-                                           Default: speech
-                  --mode <mode>            Separation mode: short, long, stream.
-                                           Default: short
-              -o, --output-target <path>   Target WAV output path.
-                                           Default: <input_stem>.target.wav
-                  --output-residual <path> Residual WAV output path.
-                                           Default: <input_stem>.residual.wav
-                  --no-residual            Skip writing residual output file.
-
-                  --chunk-seconds <float>  Chunk duration for long/stream modes.
-                                           Default: 10.0
-                  --overlap-seconds <float> Overlap duration for long/stream modes.
-                                           Default: 3.0
-                  --ode-method <method>    ODE method: midpoint or euler.
-                                           Default: midpoint
-                  --step-size <float>      ODE step size (0 < value < 1).
-                                           Default: 0.0625
-                  --decode-chunk-size <n>  Optional decoder chunk size.
-
-                  --anchor <tok:start:end> Temporal anchor (repeatable). tok is + or -.
-                                           Example: --anchor +:1.5:3.0
-                                           Note: anchors are only supported in --mode short.
-
-                  --strict                 Enable strict model weight loading.
-                  --hf-token <token>       Hugging Face token (or set HF_TOKEN env var)
+              Runs STS (Speech-to-Speech) models. Supports SAM Audio source separation
+              and LFM2.5-Audio multimodal generation (text-to-text, text-to-speech,
+              speech-to-text, speech-to-speech).
+
+            Model Selection:
+              --model <repo>               Model repo or local path.
+                                           SAM Audio default: \(SAMAudio.defaultRepo)
+                                           LFM example: mlx-community/LFM2.5-Audio-1.5B-6bit
+
+            LFM2.5-Audio Options:
+              --mode <t2t|tts|stt|sts>     LFM generation mode.
+                                             t2t: text-to-text
+                                             tts: text-to-speech
+                                             stt: speech-to-text
+                                             sts: speech-to-speech (default)
+              -t, --text <string>          Input text (required for t2t/tts, optional prompt for stt)
+              -i, --audio <path>           Input audio file (required for stt/sts)
+              --system <string>            System prompt (overrides per-mode default)
+              --max-new-tokens <int>       Max tokens to generate. Default: 512
+              --temperature <float>        Text sampling temperature. Default: 0.8
+              --top-k <int>                Text top-K. Default: 50
+              --audio-temperature <float>  Audio sampling temperature. Default: 0.7
+              --audio-top-k <int>          Audio top-K. Default: 30
+              --stream                     Stream text output to stdout
+              -o, --output-target <path>   Audio WAV output path. Default: lfm_output.wav
+              --output-text <path>         Text output path (optional)
+
+            SAM Audio Options:
+              --mode <short|long|stream>   Separation mode. Default: short
+              -i, --audio <path>           Input audio file (required)
+              -d, --description <text>     Target description. Default: speech
+              -o, --output-target <path>   Target WAV output. Default: <input>.target.wav
+              --output-residual <path>     Residual WAV output. Default: <input>.residual.wav
+              --no-residual                Skip residual write
+              --chunk-seconds <float>      Chunk duration for long/stream. Default: 10.0
+              --overlap-seconds <float>    Overlap for long/stream. Default: 3.0
+              --ode-method <method>        midpoint or euler. Default: midpoint
+              --step-size <float>          ODE step size. Default: 0.0625
+              --decode-chunk-size <n>      Optional decoder chunk size
+              --anchor <tok:start:end>     Anchor (short mode only, repeatable)
+              --strict                     Strict weight loading
+
+            Common:
+              --hf-token <token>           Hugging Face token (or set HF_TOKEN env var)
               -h, --help                   Show this help
             """
         )
diff --git a/Sources/Tools/mlx-audio-swift-sts/README.md b/Sources/Tools/mlx-audio-swift-sts/README.md
index 885db9d..885ffa3 100644
--- a/Sources/Tools/mlx-audio-swift-sts/README.md
+++ b/Sources/Tools/mlx-audio-swift-sts/README.md
@@ -1,27 +1,69 @@
 # mlx-audio-swift-sts
 
-Command-line tool for speech/source separation using SAM Audio (`MLXAudioSTS`).
+Command-line tool for Speech-to-Speech tasks using models from the `MLXAudioSTS` module.
+
+Supports two model families:
+- **LFM2.5-Audio**: Multimodal generation (text-to-text, text-to-speech, speech-to-text, speech-to-speech)
+- **SAM Audio**: Source separation
 
 ## Build and Run
 
 ```bash
-swift run mlx-audio-swift-sts --audio /path/to/input.wav
+swift run mlx-audio-swift-sts --help
 ```
 
-## Example
+## LFM2.5-Audio Examples
+
+### Text-to-Text
+```bash
+swift run mlx-audio-swift-sts \
+  --model mlx-community/LFM2.5-Audio-1.5B-6bit \
+  --mode t2t \
+  --text "What is 2 + 2?" \
+  --system "Answer briefly." \
+  --stream
+```
 
+### Text-to-Speech
+```bash
+swift run mlx-audio-swift-sts \
+  --model mlx-community/LFM2.5-Audio-1.5B-6bit \
+  --mode tts \
+  --text "Hello, welcome to MLX Audio!" \
+  --system "Perform TTS. Use a UK male voice." \
+  -o /tmp/lfm_tts.wav
+```
+
+### Speech-to-Text
+```bash
+swift run mlx-audio-swift-sts \
+  --model mlx-community/LFM2.5-Audio-1.5B-6bit \
+  --mode stt \
+  --audio /path/to/audio.wav \
+  --stream
+```
+
+### Speech-to-Speech
+```bash
+swift run mlx-audio-swift-sts \
+  --model mlx-community/LFM2.5-Audio-1.5B-6bit \
+  --mode sts \
+  --audio /path/to/audio.wav \
+  -o /tmp/lfm_sts.wav
+```
+
+## SAM Audio Examples
+
+### Short Mode (default)
 ```bash
 swift run mlx-audio-swift-sts \
-  --model mlx-community/SAM-48k2-v1.5 \
   --audio /path/to/mix.wav \
   --description speech \
   --mode short \
-  --output-target /tmp/target.wav \
-  --output-residual /tmp/residual.wav
+  --output-target /tmp/target.wav
 ```
 
-## Streaming Mode Example
-
+### Streaming Mode
 ```bash
 swift run mlx-audio-swift-sts \
   --audio /path/to/mix.wav \
@@ -32,8 +74,24 @@ swift run mlx-audio-swift-sts \
 
 ## Options
 
+### LFM2.5-Audio
+- `--model`: Model repo (must contain "lfm" to auto-detect)
+- `--mode`: `t2t | tts | stt | sts`
+- `--text`, `-t`: Input text
+- `--audio`, `-i`: Input audio path
+- `--system`: System prompt (overrides per-mode defaults: t2t="You are a helpful assistant.", tts="Perform TTS.", stt="You are a helpful assistant that transcribes audio.", sts="Respond to the user with interleaved text and speech audio.")
+- `--max-new-tokens`: Max generation tokens (default: 512)
+- `--temperature`: Text temperature (default: 0.8)
+- `--top-k`: Text top-K (default: 50)
+- `--audio-temperature`: Audio temperature (default: 0.7)
+- `--audio-top-k`: Audio top-K (default: 30)
+- `--stream`: Stream text to stdout
+- `--output-target`, `-o`: Audio WAV output
+- `--output-text`: Text output file
+
+### SAM Audio
 - `--audio`, `-i`: Input audio path (required)
-- `--model`: Model repo id or local path
+- `--model`: Model repo or local path
 - `--description`, `--prompt`, `-d`: Target description text
 - `--mode`: `short | long | stream`
 - `--output-target`, `-o`: Target WAV output path
@@ -43,8 +101,9 @@ swift run mlx-audio-swift-sts \
 - `--overlap-seconds`: Chunk overlap for `long`/`stream`
 - `--ode-method`: `midpoint | euler`
 - `--step-size`: ODE step size
-- `--decode-chunk-size`: Optional decoder chunk size
-- `--anchor`: Anchor rule (`+| - :start:end`), repeatable, `short` mode only
+- `--anchor`: Anchor rule (`+|-:start:end`), repeatable, `short` mode only
 - `--strict`: Strict weight loading
+
+### Common
 - `--hf-token`: Hugging Face token (or use `HF_TOKEN` env var)
 - `--help`, `-h`: Show help
diff --git a/Tests/MLXAudioSTSTests.swift b/Tests/MLXAudioSTSTests.swift
index 7c10b6c..f141c39 100644
--- a/Tests/MLXAudioSTSTests.swift
+++ b/Tests/MLXAudioSTSTests.swift
@@ -1,3 +1,10 @@
+//
+//  MLXAudioSTSTests.swift
+//  MLXAudioTests
+//
+//  Created by Claude on 17/02/2026.
+//
+
 import Foundation
 import Testing
 import MLX
@@ -1152,3 +1159,256 @@ struct SAMAudioWeightsTests {
         #expect(model.sampleRate == model.config.audioCodec.sampleRate)
     }
 }
+
+
+// MARK: - LFMAudio Config Tests
+
+struct LFMAudioConfigTests {
+
+    // MARK: - PreprocessorConfig
+
+    @Test func preprocessorConfigDefaults() {
+        let config = PreprocessorConfig()
+
+        #expect(config.sampleRate == 16000)
+        #expect(config.normalize == "per_feature")
+        #expect(config.windowSize == 0.025)
+        #expect(config.windowStride == 0.01)
+        #expect(config.window == "hann")
+        #expect(config.features == 128)
+        #expect(config.nFft == 512)
+        #expect(config.log == true)
+        #expect(config.frameSplicing == 1)
+        #expect(config.dither == 1e-05)
+        #expect(config.padTo == 0)
+        #expect(config.padValue == 0.0)
+        #expect(config.preemph == 0.97)
+    }
+
+    @Test func preprocessorConfigComputedProperties() {
+        let config = PreprocessorConfig()
+
+        // hopLength = Int(16000 * 0.01) = 160
+        #expect(config.hopLength == 160)
+        // winLength = Int(16000 * 0.025) = 400
+        #expect(config.winLength == 400)
+    }
+
+    @Test func preprocessorConfigDecoding() throws {
+        let json = "{}"
+        let data = json.data(using: .utf8)!
+        let config = try JSONDecoder().decode(PreprocessorConfig.self, from: data)
+
+        #expect(config.sampleRate == 16000)
+        #expect(config.features == 128)
+        #expect(config.nFft == 512)
+        #expect(config.preemph == 0.97)
+    }
+
+    // MARK: - ConformerEncoderConfig
+
+    @Test func conformerEncoderConfigDefaults() {
+        let config = ConformerEncoderConfig()
+
+        #expect(config.featIn == 128)
+        #expect(config.featOut == -1)
+        #expect(config.nLayers == 17)
+        #expect(config.dModel == 512)
+        #expect(config.subsampling == "dw_striding")
+        #expect(config.subsamplingFactor == 8)
+        #expect(config.subsamplingConvChannels == 256)
+        #expect(config.causalDownsampling == false)
+        #expect(config.ffExpansionFactor == 4)
+        #expect(config.selfAttentionModel == "rel_pos")
+        #expect(config.nHeads == 8)
+        #expect(config.attContextSize == [-1, -1])
+        #expect(config.xscaling == false)
+        #expect(config.untieBiases == true)
+        #expect(config.posEmbMaxLen == 5000)
+        #expect(config.convKernelSize == 9)
+        #expect(config.convNormType == "batch_norm")
+        #expect(config.dropout == 0.1)
+    }
+
+    @Test func conformerEncoderConfigDecoding() throws {
+        let json = """
+        {
+            "feat_in": 80,
+            "n_layers": 12,
+            "d_model": 256,
+            "n_heads": 4
+        }
+        """
+        let data = json.data(using: .utf8)!
+        let config = try JSONDecoder().decode(ConformerEncoderConfig.self, from: data)
+
+        #expect(config.featIn == 80)
+        #expect(config.nLayers == 12)
+        #expect(config.dModel == 256)
+        #expect(config.nHeads == 4)
+        // Defaults for unspecified fields
+        #expect(config.subsamplingFactor == 8)
+        #expect(config.convKernelSize == 9)
+    }
+
+    // MARK: - DepthformerConfig
+
+    @Test func depthformerConfigDefaults() {
+        let config = DepthformerConfig()
+
+        #expect(config.layers == 6)
+        #expect(config.dim == 1024)
+        #expect(config.numHeads == 32)
+        #expect(config.numKvHeads == 8)
+        #expect(config.tie == true)
+    }
+
+    @Test func depthformerConfigDecoding() throws {
+        let json = """
+        {
+            "layers": 4,
+            "dim": 512,
+            "num_heads": 16,
+            "num_kv_heads": 4,
+            "tie": false
+        }
+        """
+        let data = json.data(using: .utf8)!
+        let config = try JSONDecoder().decode(DepthformerConfig.self, from: data)
+
+        #expect(config.layers == 4)
+        #expect(config.dim == 512)
+        #expect(config.numHeads == 16)
+        #expect(config.numKvHeads == 4)
+        #expect(config.tie == false)
+    }
+
+    // MARK: - DetokenizerConfig
+
+    @Test func detokenizerConfigDefaults() {
+        let config = DetokenizerConfig()
+
+        #expect(config.hiddenSize == 512)
+        #expect(config.numHiddenLayers == 8)
+        #expect(config.numAttentionHeads == 16)
+        #expect(config.numKeyValueHeads == 8)
+        #expect(config.slidingWindow == 30)
+        #expect(config.intermediateSize == 2304)
+        #expect(config.normEps == 1e-5)
+        #expect(config.ropeTheta == 1000000.0)
+        #expect(config.outputSize == 1282)
+        #expect(config.numCodebooks == 8)
+        #expect(config.vocabSize == 2048)
+        #expect(config.nFft == 1280)
+        #expect(config.hopLength == 320)
+        #expect(config.upsampleFactor == 6)
+    }
+
+    @Test func detokenizerConfigLayerTypes() {
+        let config = DetokenizerConfig()
+
+        #expect(config.layerTypes.count == 8)
+        // Pattern: conv, conv, sliding_attention, conv, sliding_attention, conv, sliding_attention, conv
+        #expect(config.layerTypes[0] == "conv")
+        #expect(config.layerTypes[2] == "sliding_attention")
+        #expect(config.layerTypes[7] == "conv")
+    }
+
+    // MARK: - LFMGenerationConfig
+
+    @Test func generationConfigDefaults() {
+        let config = LFMGenerationConfig()
+
+        #expect(config.maxNewTokens == 512)
+        #expect(config.temperature == 1.0)
+        #expect(config.topK == 50)
+        #expect(config.topP == 1.0)
+        #expect(config.audioTemperature == 1.0)
+        #expect(config.audioTopK == 4)
+    }
+
+    @Test func generationConfigCustom() {
+        let config = LFMGenerationConfig(
+            maxNewTokens: 2048,
+            temperature: 0.8,
+            topK: 30,
+            audioTemperature: 0.7,
+            audioTopK: 10
+        )
+
+        #expect(config.maxNewTokens == 2048)
+        #expect(config.temperature == 0.8)
+        #expect(config.topK == 30)
+        #expect(config.audioTemperature == 0.7)
+        #expect(config.audioTopK == 10)
+    }
+}
+
+
+// MARK: - Module Setup Tests
+
+struct LFMAudioModuleSetupTests {
+
+    @Test func modalityConstants() {
+        #expect(LFMModality.text.rawValue == 1)
+        #expect(LFMModality.audioIn.rawValue == 2)
+        #expect(LFMModality.audioOut.rawValue == 3)
+    }
+
+    @Test func specialTokenConstants() {
+        #expect(lfmAudioStartToken == 128)
+        #expect(lfmImEndToken == 7)
+        #expect(lfmTextEndToken == 130)
+        #expect(lfmAudioEOSToken == 2048)
+    }
+
+    @Test func audioEmbeddingShape() {
+        let vocabSize = 2049
+        let dim = 64
+        let numCodebooks = 8
+        let emb = AudioEmbedding(vocabSize: vocabSize, dim: dim, numCodebooks: numCodebooks)
+
+        // Input: (B, K) where K = numCodebooks, values in [0, vocabSize)
+        let input = MLXArray([0, 1, 2, 3, 4, 5, 6, 7]).expandedDimensions(axis: 0) // (1, 8)
+        let output = emb(input)
+
+        // Output should be (1, dim) after summing over codebooks
+        #expect(output.shape == [1, dim])
+    }
+
+    @Test func audioEmbeddingWithNormShape() {
+        let vocabSize = 2049
+        let dim = 64
+        let emb = AudioEmbeddingWithNorm(vocabSize: vocabSize, dim: dim)
+
+        // embed: (B,) -> (B, dim)
+        let input = MLXArray([Int32(42)]).expandedDimensions(axis: 0) // (1, 1)
+        let embedded = emb.embed(input.squeezed(axis: 1))
+        #expect(embedded.shape == [1, dim])
+
+        // logits: (B, dim) -> (B, vocabSize)
+        let hidden = MLXArray.zeros([1, dim])
+        let logits = emb.logits(hidden)
+        #expect(logits.shape == [1, vocabSize])
+    }
+
+    @Test func conformerEncoderConstruction() {
+        let config = ConformerEncoderConfig()
+        let encoder = ConformerEncoder(config)
+
+        // Verify the encoder was constructed (it has layers)
+        #expect(encoder.layers.count == config.nLayers)
+    }
+
+    @Test func depthformerConstruction() {
+        let config = DepthformerConfig()
+        let depthformer = Depthformer(
+            layers: config.layers, dim: config.dim,
+            numHeads: config.numHeads, numKvHeads: config.numKvHeads
+        )
+
+        #expect(depthformer.layersCount == config.layers)
+    }
+}
+
+
diff --git a/Tests/MLXAudioSmokeTests.swift b/Tests/MLXAudioSmokeTests.swift
index 8e643e0..7dc7538 100644
--- a/Tests/MLXAudioSmokeTests.swift
+++ b/Tests/MLXAudioSmokeTests.swift
@@ -17,6 +17,7 @@
 //    -only-testing:'MLXAudioTests/Smoke/TTSSmokeTests'
 //    -only-testing:'MLXAudioTests/Smoke/STTSmokeTests'
 //    -only-testing:'MLXAudioTests/Smoke/VADSmokeTests'
+//    -only-testing:'MLXAudioTests/Smoke/STSSmokeTests'
 //
 //  Run a single test (note the trailing parentheses for Swift Testing):
 //    -only-testing:'MLXAudioTests/Smoke/STTSmokeTests/qwen3ASRTranscribe()'
@@ -34,6 +35,7 @@ import Foundation
 @testable import MLXAudioTTS
 @testable import MLXAudioSTT
 @testable import MLXAudioVAD
+@testable import MLXAudioSTS
 
 
 // MARK: - Helpers
@@ -734,4 +736,254 @@ struct VADSmokeTests {
     }
 }
 
+// MARK: - STS Smoke Tests
+
+@Suite("STS Smoke Tests", .serialized)
+struct STSSmokeTests {
+
+    static let modelName = "mlx-community/LFM2.5-Audio-1.5B-6bit"
+
+    @Test func lfm2TextToText() async throws {
+        testHeader("lfm2TextToText")
+        defer { testCleanup("lfm2TextToText") }
+
+        print("\u{001B}[33mLoading LFM2.5-Audio model...\u{001B}[0m")
+        let model = try await LFM2AudioModel.fromPretrained(Self.modelName)
+        let processor = model.processor!
+        print("\u{001B}[32mModel loaded!\u{001B}[0m")
+
+        let chat = ChatState(processor: processor)
+        chat.newTurn(role: "system")
+        chat.addText("Answer briefly in one sentence.")
+        chat.endTurn()
+        chat.newTurn(role: "user")
+        chat.addText("What is 2 + 2?")
+        chat.endTurn()
+        chat.newTurn(role: "assistant")
+
+        let genConfig = LFMGenerationConfig(
+            maxNewTokens: 64,
+            temperature: 0.8,
+            topK: 50
+        )
+
+        print("\u{001B}[33mGenerating text-to-text response...\u{001B}[0m")
+
+        var textTokens: [Int] = []
+        for try await (token, modality) in model.generateInterleaved(
+            textTokens: chat.getTextTokens(),
+            audioFeatures: chat.getAudioFeatures(),
+            modalities: chat.getModalities(),
+            config: genConfig
+        ) {
+            eval(token)
+            if modality == .text {
+                textTokens.append(token.item(Int.self))
+            }
+        }
+
+        let decodedText = processor.decodeText(textTokens)
+        print("\u{001B}[32mText-to-Text output: \(decodedText)\u{001B}[0m")
+        print("\u{001B}[32mGenerated \(textTokens.count) text tokens\u{001B}[0m")
+
+        #expect(textTokens.count > 0, "Should generate at least one text token")
+        #expect(!decodedText.isEmpty, "Decoded text should not be empty")
+    }
+
+    @Test func lfm2TextToSpeech() async throws {
+        testHeader("lfm2TextToSpeech")
+        defer { testCleanup("lfm2TextToSpeech") }
+
+        print("\u{001B}[33mLoading LFM2.5-Audio model...\u{001B}[0m")
+        let model = try await LFM2AudioModel.fromPretrained(Self.modelName)
+        let processor = model.processor!
+        print("\u{001B}[32mModel loaded!\u{001B}[0m")
+
+        let chat = ChatState(processor: processor)
+        chat.newTurn(role: "system")
+        chat.addText("Perform TTS. Use a UK male voice.")
+        chat.endTurn()
+        chat.newTurn(role: "user")
+        chat.addText("Hello, welcome to MLX Audio!")
+        chat.endTurn()
+        chat.newTurn(role: "assistant")
+        chat.addAudioStartToken()
+
+        let genConfig = LFMGenerationConfig(
+            maxNewTokens: 256,
+            temperature: 0.8,
+            topK: 50,
+            audioTemperature: 0.7,
+            audioTopK: 30
+        )
+
+        print("\u{001B}[33mGenerating text-to-speech response...\u{001B}[0m")
+
+        var audioCodes: [MLXArray] = []
+        for try await (token, modality) in model.generateSequential(
+            textTokens: chat.getTextTokens(),
+            audioFeatures: chat.getAudioFeatures(),
+            modalities: chat.getModalities(),
+            config: genConfig
+        ) {
+            eval(token)
+            if modality == .audioOut {
+                if token[0].item(Int.self) == lfmAudioEOSToken {
+                    break
+                }
+                audioCodes.append(token)
+            }
+        }
+
+        print("\u{001B}[32mText-to-Speech: generated \(audioCodes.count) audio frames\u{001B}[0m")
+
+        #expect(audioCodes.count > 0, "Should generate at least one audio frame")
+
+        if let firstFrame = audioCodes.first {
+            #expect(firstFrame.shape == [8], "Audio frame should have 8 codebook values")
+        }
+
+        let stacked = MLX.stacked(audioCodes, axis: 0)
+        let codesInput = stacked.transposed(1, 0).expandedDimensions(axis: 0)
+        eval(codesInput)
+
+        let detokenizer = try LFM2AudioDetokenizer.fromPretrained(modelPath: model.modelDirectory!)
+        let waveform = detokenizer(codesInput)
+        eval(waveform)
+        let samples = waveform[0].asArray(Float.self)
+        print("\u{001B}[32mDecoded \(samples.count) audio samples (\(String(format: "%.1f", Double(samples.count) / 24000.0))s at 24kHz)\u{001B}[0m")
+
+        let outputURL = URL(fileURLWithPath: NSHomeDirectory())
+            .appendingPathComponent("Desktop/lfm_tts_output.wav")
+        try AudioUtils.writeWavFile(samples: samples, sampleRate: 24000, fileURL: outputURL)
+        print("\u{001B}[32mSaved WAV to: \(outputURL.path)\u{001B}[0m")
+    }
+
+    @Test func lfm2SpeechToText() async throws {
+        testHeader("lfm2SpeechToText")
+        defer { testCleanup("lfm2SpeechToText") }
+
+        let audioURL = Bundle.module.url(forResource: "conversational_a", withExtension: "wav", subdirectory: "media")!
+        let (sampleRate, audioData) = try loadAudioArray(from: audioURL)
+        print("\u{001B}[33mLoaded audio: \(audioData.shape), sample rate: \(sampleRate)\u{001B}[0m")
+
+        print("\u{001B}[33mLoading LFM2.5-Audio model...\u{001B}[0m")
+        let model = try await LFM2AudioModel.fromPretrained(Self.modelName)
+        let processor = model.processor!
+        print("\u{001B}[32mModel loaded!\u{001B}[0m")
+
+        let chat = ChatState(processor: processor)
+        chat.newTurn(role: "user")
+        chat.addAudio(audioData, sampleRate: sampleRate)
+        chat.addText("Transcribe the audio.")
+        chat.endTurn()
+        chat.newTurn(role: "assistant")
+
+        let genConfig = LFMGenerationConfig(
+            maxNewTokens: 256,
+            temperature: 0.8,
+            topK: 50
+        )
+
+        print("\u{001B}[33mGenerating speech-to-text response...\u{001B}[0m")
+
+        var textTokens: [Int] = []
+        for try await (token, modality) in model.generateInterleaved(
+            textTokens: chat.getTextTokens(),
+            audioFeatures: chat.getAudioFeatures(),
+            modalities: chat.getModalities(),
+            config: genConfig
+        ) {
+            eval(token)
+            if modality == .text {
+                textTokens.append(token.item(Int.self))
+                print(processor.decodeText([token.item(Int.self)]), terminator: "")
+            }
+        }
+
+        let decodedText = processor.decodeText(textTokens)
+        print("\n\u{001B}[32mSpeech-to-Text transcription: \(decodedText)\u{001B}[0m")
+        print("\u{001B}[32mGenerated \(textTokens.count) text tokens\u{001B}[0m")
+
+        #expect(textTokens.count > 0, "Should generate at least one text token")
+        #expect(!decodedText.isEmpty, "Transcription should not be empty")
+    }
+
+    @Test func lfm2SpeechToSpeech() async throws {
+        testHeader("lfm2SpeechToSpeech")
+        defer { testCleanup("lfm2SpeechToSpeech") }
+
+        let audioURL = Bundle.module.url(forResource: "conversational_a", withExtension: "wav", subdirectory: "media")!
+        let (sampleRate, audioData) = try loadAudioArray(from: audioURL)
+        print("\u{001B}[33mLoaded audio: \(audioData.shape), sample rate: \(sampleRate)\u{001B}[0m")
+
+        print("\u{001B}[33mLoading LFM2.5-Audio model...\u{001B}[0m")
+        let model = try await LFM2AudioModel.fromPretrained(Self.modelName)
+        let processor = model.processor!
+        print("\u{001B}[32mModel loaded!\u{001B}[0m")
+
+        let chat = ChatState(processor: processor)
+        chat.newTurn(role: "system")
+        chat.addText("Respond with interleaved text and audio.")
+        chat.endTurn()
+        chat.newTurn(role: "user")
+        chat.addAudio(audioData, sampleRate: sampleRate)
+        chat.endTurn()
+        chat.newTurn(role: "assistant")
+
+        let genConfig = LFMGenerationConfig(
+            maxNewTokens: 512,
+            temperature: 0.8,
+            topK: 50,
+            audioTemperature: 0.7,
+            audioTopK: 30
+        )
+
+        print("\u{001B}[33mGenerating speech-to-speech response...\u{001B}[0m")
+
+        var textTokens: [Int] = []
+        var audioCodes: [MLXArray] = []
+        for try await (token, modality) in model.generateInterleaved(
+            textTokens: chat.getTextTokens(),
+            audioFeatures: chat.getAudioFeatures(),
+            modalities: chat.getModalities(),
+            config: genConfig
+        ) {
+            eval(token)
+            if modality == .text {
+                textTokens.append(token.item(Int.self))
+            } else if modality == .audioOut {
+                // Filter EOS frames (code 2048) — they're out-of-range for the detokenizer
+                if token[0].item(Int.self) != lfmAudioEOSToken {
+                    audioCodes.append(token)
+                }
+            }
+        }
+
+        let decodedText = processor.decodeText(textTokens)
+        print("\u{001B}[32mSpeech-to-Speech text: \(decodedText)\u{001B}[0m")
+        print("\u{001B}[32mGenerated \(textTokens.count) text tokens, \(audioCodes.count) audio frames\u{001B}[0m")
+
+        let totalTokens = textTokens.count + audioCodes.count
+        #expect(totalTokens > 0, "Should generate at least one token (text or audio)")
+
+        if !audioCodes.isEmpty {
+            let stacked = MLX.stacked(audioCodes, axis: 0)
+            let codesInput = stacked.transposed(1, 0).expandedDimensions(axis: 0)
+            eval(codesInput)
+
+            let detokenizer = try LFM2AudioDetokenizer.fromPretrained(modelPath: model.modelDirectory!)
+            let waveform = detokenizer(codesInput)
+            eval(waveform)
+            let samples = waveform[0].asArray(Float.self)
+            print("\u{001B}[32mDecoded \(samples.count) audio samples (\(String(format: "%.1f", Double(samples.count) / 24000.0))s at 24kHz)\u{001B}[0m")
+
+            let outputURL = URL(fileURLWithPath: NSHomeDirectory())
+                .appendingPathComponent("Desktop/lfm_sts_output.wav")
+            try AudioUtils.writeWavFile(samples: samples, sampleRate: 24000, fileURL: outputURL)
+            print("\u{001B}[32mSaved WAV to: \(outputURL.path)\u{001B}[0m")
+        }
+    }
+}
+
 } // end Smoke
